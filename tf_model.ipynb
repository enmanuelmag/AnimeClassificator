{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import numba\n",
    "import faiss\n",
    "import hashlib\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from numba import vectorize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk_metrics\n",
    "from alive_progress import alive_bar\n",
    "from notifier import Notifier, notify\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import visualkeras\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import pickle as pkl\n",
    "import transformer\n",
    "import tf_functions\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "GeneratePatch = tf_functions.GeneratePatch\n",
    "AnimeClassifier = tf_functions.AnimeClassifier\n",
    "ReleaseMemory = tf_functions.ReleaseMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "SIZE_IMG = 224 #224#224#416\n",
    "UNITS = 1024 #2048 1024 128 256 512-seq\n",
    "MAX_CLASS = 32 #1024 32 16 8\n",
    "\n",
    "TRANSFORMER_LAYERS = 2 #2-\n",
    "PATCH_SIZE = 4 #4- 10 20 #64 more big ==> less pieces\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_HEADS = 2\n",
    "MLP_DIM = 128 #1024 128-\n",
    "\n",
    "DATASET_PATH = './data/animes'\n",
    "DATASET_FACES_PATH = './data/faces'\n",
    "DATASET_FACE_FOLDER = './data/moeimouto-faces'\n",
    "CLASS_ARRAY_PATH = f'./data/class_array_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_FACES_PATH = f'./data/class_array_faces_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_VEC_PATH = f'./data/class_array_vec_{MAX_CLASS}.pkl'\n",
    "\n",
    "DATASET_JSON_PATH = './data/anime_data.json'\n",
    "\n",
    "AMOUNT_TABLE_PATH = './data/anime_amount.pkl'\n",
    "AMOUNT_FACES_TABLE_PATH = './data/faces_amount.pkl'\n",
    "\n",
    "DATASET_JSON_RANK = './data/anime_rank.json'\n",
    "\n",
    "TFRECORD_PATH = f'./data/anime_data_{MAX_CLASS}.tfrecord'\n",
    "TFRECORD_FACES_PATH = f'./data/anime_faces_data_{MAX_CLASS}.tfrecord'\n",
    "\n",
    "TG_ID = \"293701727\"\n",
    "TG_TOKEN = \"1878628343:AAEFVRsqDz63ycmaLOFS7gvsG969wdAsJ0w\"\n",
    "WEBHOOK_URL = \"https://discord.com/api/webhooks/796406472459288616/PAkiGGwqe0_PwtBxXYQvOzbk78B4RQP6VWRkvpBtw6Av0sc_mDa3saaIlwVPFjOIeIbt\"\n",
    "\n",
    "#seed random seed to 42 for reproducibility\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if False:\n",
    "  #DATASET_PATH = DATASET_FACES_PATH\n",
    "  CLASS_ARRAY_PATH = CLASS_ARRAY_FACES_PATH\n",
    "  TFRECORD_PATH = TFRECORD_FACES_PATH\n",
    "  AMOUNT_TABLE_PATH = AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_image(url, anime_name, idx):\n",
    "  #download image from url\n",
    "  file_path = f'./data/animes/{anime_name}____{idx}.jpg' \n",
    "  if os.path.exists(file_path):\n",
    "    return\n",
    "\n",
    "  img_data = requests.get(url).content\n",
    "  with open(file_path, 'wb') as handler:\n",
    "    handler.write(img_data)\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Anime images',\n",
    "  msg='Finished downloading anime images'\n",
    ")\n",
    "def get_images(data):\n",
    "  #with alive_bar(len(data)) as bar:\n",
    "  for idx_a, anime_name in enumerate(data):\n",
    "    urls = data[anime_name]\n",
    "    for idx, url in enumerate(urls):\n",
    "      if idx >= 400:\n",
    "        break\n",
    "      name_clean = re.sub(r'_+', r'_', re.sub(r'[\\W\\s]', r'_', anime_name))\n",
    "      try:\n",
    "        dowload_image(url['image'], name_clean, idx)\n",
    "      except Exception as e:\n",
    "        print(f'Error on download image {idx + 1} of {anime_name}')\n",
    "        pass\n",
    "    #bar()\n",
    "    print(f'Progress: {idx_a + 1}/{len(data)} - {round((idx_a + 1)/len(data)*100, 2)}%')\n",
    "\n",
    "def get_classes_anime(path):\n",
    "  classes = set()\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    classes.add(class_name)\n",
    "  return list(classes)\n",
    "\n",
    "def wait_for_it(driver, xpath, timeout=3):\n",
    "  try:\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "def iter_post(driver):\n",
    "  anime_data = []\n",
    "\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = True\n",
    "\n",
    "  while next_button is not None:\n",
    "    if len(anime_data) > 400:\n",
    "      break\n",
    "    ul_element = wait_for_it(driver, '//ul[@id=\"post-list-posts\"]')\n",
    "    if ul_element is None:\n",
    "      next_button = wait_for_it(driver, xpath_next)\n",
    "      if next_button is not None:\n",
    "        next_button.click()\n",
    "        time.sleep(1)\n",
    "      continue\n",
    "    for i, li_element in enumerate(ul_element.find_elements(By.TAG_NAME, 'li')):\n",
    "      a_video = li_element.find_element(By.XPATH, './a').get_attribute('href')\n",
    "      a_image = li_element.find_element(By.XPATH, './div/a/img').get_attribute('src')\n",
    "      anime_data.append({\n",
    "        'video': a_video,\n",
    "        'image': a_image\n",
    "      })\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "    if next_button is not None:\n",
    "      next_button.click()\n",
    "      time.sleep(rd.randint(1, 2))\n",
    "  return anime_data\n",
    "\n",
    "def get_images_links(url, driver, anime_name):\n",
    "  url_search = url + anime_name\n",
    "  driver.get(url_search)\n",
    "  return iter_post(driver)\n",
    "\n",
    "def get_names(driver):\n",
    "  names = []\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = wait_for_it(driver, xpath_next)\n",
    "  \n",
    "  while next_button is not None:\n",
    "    for tr_element in driver.find_elements(By.XPATH, '//table[@class=\"highlightable\"]/tbody/tr'):\n",
    "      try:\n",
    "        amount_post = tr_element.find_element(By.XPATH, './td[1]').text\n",
    "        amount_post = int(amount_post)\n",
    "        if amount_post >= 10:\n",
    "          a_name = tr_element.find_element(By.XPATH, './td[2]/a[2]' ).text\n",
    "          names.append(a_name)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    next_button.click()\n",
    "    time.sleep(rd.randint(1, 2))\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "  return names\n",
    "\n",
    "def get_score(anime_name, driver):\n",
    "  url_search = f'https://myanimelist.net/anime.php?cat=anime&q={anime_name}'\n",
    "  driver.get(url_search)\n",
    "  score = 0\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    score += 1\n",
    "  return score\n",
    "\n",
    "def relevant_anime(anime_name, df_anime, amount_table, threshold=350, rank=True):\n",
    "  \n",
    "  if amount_table.get(anime_name, 0) <= threshold:\n",
    "    return False\n",
    "\n",
    "  if not rank:\n",
    "    return True\n",
    "\n",
    "  anime_name = re.sub(r'_', r' ', anime_name)\n",
    "  df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "\n",
    "  if df_result.empty:\n",
    "    anime_name = ' '.join(anime_name.split(' ')[:3])\n",
    "    df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "  return not df_result.empty\n",
    "\n",
    "def amount_anime_table(datapath):\n",
    "  dic = {}\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    dic[class_name] = dic.get(class_name, 0) + 1\n",
    "  return dic\n",
    "\n",
    "def detect(filename, cascade_file):\n",
    "  if not os.path.isfile(cascade_file):\n",
    "    raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "  cascade = cv2.CascadeClassifier(cascade_file)\n",
    "  image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "  #src = cv2.cuda_GpuMat()\n",
    "  #src.upload(image)\n",
    "\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray = cv2.equalizeHist(gray)\n",
    "\n",
    "  faces = cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor = 1.1,\n",
    "    minNeighbors = 5,\n",
    "    minSize = (24, 24)\n",
    "  )\n",
    "\n",
    "  new_images = []\n",
    "  for (x, y, w, h) in faces:\n",
    "    new_images.append(image[y:y+h, x:x+w])\n",
    "  #clahe = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "  #dst = clahe.apply(src, cv2.cuda_Stream.Null())\n",
    "  #result = dst.download()\n",
    "  return new_images\n",
    "\n",
    "def extract_faces(datapath):\n",
    "  faces_amount = 0\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    new_images = []\n",
    "    try:\n",
    "      new_images = detect(datapath + '/' + filename, './data/haar/lbpcascade_animeface.xml')\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      pass\n",
    "    if len(new_images) > 0:\n",
    "      for idx, img in enumerate(new_images):\n",
    "        new_face_name = f'./data/faces/{class_name}____{idx}.jpg'\n",
    "        try:\n",
    "          if not os.path.exists(new_face_name):\n",
    "            cv2.imwrite(new_face_name, img)\n",
    "            faces_amount += 1\n",
    "        except:\n",
    "          pass\n",
    "  print(f'Faces amount: {faces_amount}')\n",
    "\n",
    "def parse_face_dataset(face_path, out_path):\n",
    "  for char_name in os.listdir(face_path):\n",
    "    if char_name == '.DS_Store':\n",
    "      continue\n",
    "    for idx, filename in enumerate(os.listdir(face_path + '/' + char_name)):\n",
    "      if filename == '.DS_Store':\n",
    "        continue\n",
    "      ext = os.path.splitext(filename)[1]\n",
    "      if ext == '.csv':\n",
    "        continue\n",
    "      clean_char_name = char_name.split('_')[1]\n",
    "      new_filename = f'{clean_char_name}____{idx}.{ext}'\n",
    "      shutil.copy(face_path + '/' + char_name + '/' + filename, out_path + '/' + new_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anime images\n",
    "anime_data = json.load(open(DATASET_JSON_PATH))\n",
    "get_images(anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only faces with OpenCV\n",
    "extract_faces(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse dataset faces\n",
    "parse_face_dataset(DATASET_FACE_FOLDER, DATASET_FACES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FACES Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_FACES_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_anime_rank.pkl')\n",
    "\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "\n",
    "class_array = set()\n",
    "for anime_name in all_class_array:\n",
    "  if relevant_anime(anime_name, df, amount_table, threshold=100, rank=True):\n",
    "    class_array.add((anime_name, amount_table[anime_name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "class_array = [x[0] for x in class_array]\n",
    "\n",
    "mean_keyframes = np.mean([amount_table[x] for x in class_array])\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)} - Mean keyframes: {int(mean_keyframes)}')\n",
    "#pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))\n",
    "del all_class_array\n",
    "del mean_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for faces\n",
    "all_class_array = get_classes_anime(DATASET_FACES_PATH)\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "\n",
    "class_array = set()\n",
    "for name in all_class_array:\n",
    "  class_array.add((name, amount_table[name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "class_array = [x[0] for x in class_array]\n",
    "\n",
    "mean_keyframes = np.mean([amount_table[x] for x in class_array])\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)} - Mean keyframes: {int(mean_keyframes)}')\n",
    "#pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))\n",
    "del all_class_array\n",
    "del mean_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLASS_ARRAY_PATH)\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "class_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(class_name):\n",
    "  return class_array.index(class_name)\n",
    "\n",
    "def build_example(path_file, class_name):\n",
    "  img_array = open(path_file, 'rb').read()\n",
    "  \n",
    "  #img = load_img(path_file, target_size=(SIZE_IMG, SIZE_IMG))\n",
    "  #img_array = np.array(img)\n",
    "  #img_array = preprocess_input(img_array, mode='tf')\n",
    "  #key = hashlib.sha256(img_array).hexdigest()\n",
    "  example = tf.train.Example(\n",
    "    features=tf.train.Features(feature={\n",
    "    #'key': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf-8')])),\n",
    "    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array])),\n",
    "    #'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array.tobytes()])),\n",
    "    'class_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[get_class_id(class_name)])),\n",
    "    'class_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[class_name.encode('utf-8')])),\n",
    "    'filepath': tf.train.Feature(bytes_list=tf.train.BytesList(value=[path_file.encode('utf-8')]))\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "def create_tfrecord(data_path, withe_list, path_tfrecord):\n",
    "  files = os.listdir(data_path)\n",
    "  writer = tf.io.TFRecordWriter(path_tfrecord)\n",
    "  \n",
    "  print('Started creating tfrecord')\n",
    "  for idx, filename in enumerate(files):\n",
    "    class_name, _ = filename.split('____')\n",
    "  \n",
    "    if class_name in withe_list:\n",
    "      path_file = os.path.join(data_path, filename)\n",
    "      tf_example = build_example(path_file, class_name)\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "  print('Finished creating tfrecord')\n",
    "  writer.close()\n",
    "\n",
    "def parse_tfrecord(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  #class_id = tf.sparse.to_dense(x['class_id'], default_value=-1)\n",
    "  class_id = x['class_id']\n",
    "  if class_id is None:\n",
    "    class_id = -1\n",
    "  labels = tf.cast(class_id, tf.int64)\n",
    "  \n",
    "  #labels = []\n",
    "  #for i in range(len(class_array)):\n",
    "  #  labels.append(1 if i == class_id else 0)\n",
    "\n",
    "  y_train = labels\n",
    "  #y_train = tf.stack([ labels ], axis=1)\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, size, use_cache=False, use_file=True):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  dataset = dataset.map(lambda x: parse_tfrecord(x, size))\n",
    "  if use_file and use_cache:\n",
    "    dataset = dataset.cache(f'./cache/tf_record.cache')\n",
    "  elif use_cache:\n",
    "    dataset = dataset.cache()\n",
    "  return dataset\n",
    "\n",
    "def create_model(num_classes, input_shape, units, type_extractor = 'vgg') -> tf.keras.Model:\n",
    "  if type_extractor == 'vgg':\n",
    "    feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'inception':\n",
    "    feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'resnet':\n",
    "    feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  else:\n",
    "    raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(tf.keras.layers.Input(input_shape, name='input'))\n",
    "  model.add(feature_extractor)\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  #new\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "  return model\n",
    "\n",
    "def render_image_and_patches(image, patches, patch_size):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.imshow(tf.cast(image[0], tf.uint8))\n",
    "  #plt.xlabel(class_types [np.argmax(train_iter_7label)], fontsize=13)\n",
    "  n = int(np.sqrt(patches.shape[1]))\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  #plt.suptitle(f\"Image Patches\", size=13)\n",
    "  for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i+1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    ax.axis('off') \n",
    "\n",
    "def build_model(extractor_model='vgg', vanilla_model=False, input_shape=(None, SIZE_IMG, SIZE_IMG, 3)):\n",
    "  model = None\n",
    "  if vanilla_model:\n",
    "    model = create_model(\n",
    "      num_classes=len(class_array),\n",
    "      input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "      type_extractor='vgg',\n",
    "      units=UNITS\n",
    "    )\n",
    "  else:\n",
    "    if extractor_model in ['vgg', 'resnet']:\n",
    "      model = AnimeClassifier(\n",
    "        num_classes=len(class_array),\n",
    "        input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "        type_extractor=EXTRACTOR_MODEL,\n",
    "        units=UNITS,\n",
    "        inner_layers=INNER_LY\n",
    "      )\n",
    "      model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "    else:\n",
    "      model = VitModel(\n",
    "        mlp_dim=MLP_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        class_types=len(class_array),\n",
    "        transformer_layers=TRANSFORMER_LAYERS,\n",
    "      )\n",
    "  model.build(input_shape)\n",
    "  return model\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Train model',\n",
    "  msg='Training has finished'\n",
    ")\n",
    "def train(model, train_ds, val_ds, units, epochs=15, mode='fit', type_model='vgg', save_weights_only=False, inner_ly=1):\n",
    "  check_name =  f'checkpoints/{type_model}_{MAX_CLASS}class_{units}_units_{inner_ly}_checkpoint.h5'\n",
    "  if type_model == 'transformer':\n",
    "    check_name = f'checkpoints/{type_model}_{MAX_CLASS}class_{MLP_DIM}_units_{TRANSFORMER_LAYERS}_trans_layers_{NUM_HEADS}_heads_{HIDDEN_SIZE}_hs_{PATCH_SIZE}checkpoint.h5'\n",
    "  logdir = \"logs/scalars/\"+ type_model + \"/\" + time.strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "  if mode == 'eager_tf':\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "      for batch, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(images, training=True)\n",
    "          regularization_loss = tf.reduce_sum(model.losses)\n",
    "          pred_loss = []\n",
    "          for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "          total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"{}_train_{}, {}, {}\".format(\n",
    "          epoch, batch, total_loss.numpy(),\n",
    "          list(map(lambda x: np.sum(x.numpy()), pred_loss))\n",
    "        ))\n",
    "        avg_loss.update_state(total_loss)\n",
    "  elif mode == 'fit':\n",
    "    callbacks = [\n",
    "      ReleaseMemory(),\n",
    "      ReduceLROnPlateau(verbose=1),\n",
    "      #EarlyStopping(patience=10, verbose=1),\n",
    "      ModelCheckpoint(\n",
    "        check_name, \n",
    "        verbose=1,\n",
    "        monitor='accuracy',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=save_weights_only,\n",
    "      ),\n",
    "      TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_data=val_ds\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Total Training Time: {end_time} seconds')\n",
    "\n",
    "\n",
    "IMAGE_FEATURE_MAP = {\n",
    "  'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  'class_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "if False:\n",
    "  class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "  if os.path.exists(TFRECORD_PATH):\n",
    "    os.remove(TFRECORD_PATH)\n",
    "  #DATASET_PATH DATASET_FACES_PATH\n",
    "  create_tfrecord(DATASET_PATH, class_array, TFRECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 11753\n"
     ]
    }
   ],
   "source": [
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "tf_record = load_tfrecord_dataset(TFRECORD_PATH, SIZE_IMG, False, False) #TFRECORD_PATH\n",
    "all_ds_len = sum(1 for _ in tf_record)\n",
    "print(f'Total number of images: {all_ds_len}')\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_valid = int(all_ds_len * 0.1)\n",
    "n_test = all_ds_len - n_train - n_valid\n",
    "\n",
    "tf_record = tf_record.shuffle(n_train + n_valid + n_test, seed=SEED)\n",
    "train_ds = tf_record.take(n_train)\n",
    "valid_ds = tf_record.skip(n_train).take(n_valid)\n",
    "test_ds = tf_record.skip(n_train + n_valid).take(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001000, clipnorm=1.0) #0.000050 0.000025 0.00001\n",
    "\"\"\"\n",
    "1. transformer_32class_128_units_2_trans_layers_2_heads_64_hs_4checkpoint\n",
    "\"\"\"\n",
    "\n",
    "model = None\n",
    "vanilla_model = False\n",
    "\n",
    "INNER_LY = 3\n",
    "EXTRACTOR_MODEL = 'transformer' #transformer vgg inception resnet\n",
    "input_shape = (None, SIZE_IMG, SIZE_IMG, 3)\n",
    "use_my_implementation = False\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tf_functions)\n",
    "importlib.reload(transformer)\n",
    "\n",
    "if use_my_implementation:\n",
    "  VitModel = transformer.ViTModel\n",
    "else:\n",
    "  VitModel = tf_functions.ViTModel\n",
    "\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "\n",
    "model = build_model(EXTRACTOR_MODEL, vanilla_model)\n",
    "model.built = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "random_array = np.random.rand(1, SIZE_IMG, SIZE_IMG, 3)\n",
    "print(random_array[0].shape)\n",
    "\n",
    "restult = model(random_array)[0].numpy()\n",
    "print(restult.shape)\n",
    "loss([5], restult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expand(input_shape).summary()\n",
    "tf.keras.utils.plot_model(model.expand(input_shape), rankdir='TB', show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Layer AddPositionEmbs has arguments ['self', 'posemb_init']\n",
      "in `__init__` and therefore must override `get_config()`.\n",
      "\n",
      "Example:\n",
      "\n",
      "class CustomLayer(keras.layers.Layer):\n",
      "    def __init__(self, arg1, arg2):\n",
      "        super().__init__()\n",
      "        self.arg1 = arg1\n",
      "        self.arg2 = arg2\n",
      "\n",
      "    def get_config(self):\n",
      "        config = super().get_config()\n",
      "        config.update({\n",
      "            \"arg1\": self.arg1,\n",
      "            \"arg2\": self.arg2,\n",
      "        })\n",
      "        return config\n",
      "Epoch 1/200\n",
      "    367/Unknown - 70s 157ms/step - loss: 3.4648 - accuracy: 0.0402"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\AI\\AnimeClass\\tf_model.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], run_eagerly\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=2'>3</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=3'>4</a>\u001b[0m   model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=4'>5</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=5'>6</a>\u001b[0m   units\u001b[39m=\u001b[39;49mUNITS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=6'>7</a>\u001b[0m   val_ds\u001b[39m=\u001b[39;49mvalid_ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m8\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=7'>8</a>\u001b[0m   train_ds\u001b[39m=\u001b[39;49mtrain_ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m8\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=8'>9</a>\u001b[0m   save_weights_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m vanilla_model \u001b[39melse\u001b[39;49;00m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=9'>10</a>\u001b[0m   mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, type_model\u001b[39m=\u001b[39;49mEXTRACTOR_MODEL, inner_ly\u001b[39m=\u001b[39;49mINNER_LY\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000020?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\notifier\\__init__.py:73\u001b[0m, in \u001b[0;36mnotify.<locals>.wrapper_decorator.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/notifier/__init__.py?line=70'>71</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/notifier/__init__.py?line=71'>72</a>\u001b[0m   start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m---> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/notifier/__init__.py?line=72'>73</a>\u001b[0m   result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/notifier/__init__.py?line=73'>74</a>\u001b[0m   end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/notifier/__init__.py?line=74'>75</a>\u001b[0m   extra \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSUCCESFULLY - \u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32me:\\AI\\AnimeClass\\tf_model.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_ds, val_ds, units, epochs, mode, type_model, save_weights_only, inner_ly)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=187'>188</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=188'>189</a>\u001b[0m   ReleaseMemory(),\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=189'>190</a>\u001b[0m   ReduceLROnPlateau(verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=199'>200</a>\u001b[0m   TensorBoard(log_dir\u001b[39m=\u001b[39mlogdir, histogram_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=200'>201</a>\u001b[0m ]\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=202'>203</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=203'>204</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=204'>205</a>\u001b[0m   train_ds,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=205'>206</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=206'>207</a>\u001b[0m   callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=207'>208</a>\u001b[0m   validation_data\u001b[39m=\u001b[39;49mval_ds\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=208'>209</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=209'>210</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=210'>211</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal Training Time: \u001b[39m\u001b[39m{\u001b[39;00mend_time\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:1021\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1018'>1019</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_function\u001b[39m(iterator):\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1019'>1020</a>\u001b[0m   \u001b[39m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1020'>1021</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m step_function(\u001b[39mself\u001b[39;49m, iterator)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:1010\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1006'>1007</a>\u001b[0m   run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1007'>1008</a>\u001b[0m       run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, experimental_relax_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1008'>1009</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1009'>1010</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1010'>1011</a>\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1011'>1012</a>\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1012'>1013</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1312\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1306'>1307</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1307'>1308</a>\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1308'>1309</a>\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1309'>1310</a>\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1310'>1311</a>\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=1311'>1312</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2888\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=2885'>2886</a>\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=2886'>2887</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=2887'>2888</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3689\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=3686'>3687</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=3687'>3688</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/distribute/distribute_lib.py?line=3688'>3689</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/autograph/impl/api.py?line=592'>593</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/autograph/impl/api.py?line=593'>594</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/autograph/impl/api.py?line=594'>595</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:1000\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=998'>999</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=999'>1000</a>\u001b[0m   outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1000'>1001</a>\u001b[0m   \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1001'>1002</a>\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:863\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=860'>861</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=861'>862</a>\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=862'>863</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=863'>864</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:530\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[1;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=498'>499</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=499'>500</a>\u001b[0m   \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=500'>501</a>\u001b[0m \n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=501'>502</a>\u001b[0m \u001b[39m  This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=527'>528</a>\u001b[0m \n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=528'>529</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=529'>530</a>\u001b[0m   grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_gradients(\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=530'>531</a>\u001b[0m       loss, var_list\u001b[39m=\u001b[39;49mvar_list, grad_loss\u001b[39m=\u001b[39;49mgrad_loss, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=531'>532</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py:573\u001b[0m, in \u001b[0;36mLossScaleOptimizer._compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=570'>571</a>\u001b[0m \u001b[39mwith\u001b[39;00m tape:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=571'>572</a>\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_scaled_loss(loss)\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=572'>573</a>\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer\u001b[39m.\u001b[39;49m_compute_gradients(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=573'>574</a>\u001b[0m     loss,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=574'>575</a>\u001b[0m     var_list,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=575'>576</a>\u001b[0m     grad_loss,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=576'>577</a>\u001b[0m     tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=577'>578</a>\u001b[0m grads \u001b[39m=\u001b[39m [g \u001b[39mfor\u001b[39;00m g, _ \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/mixed_precision/loss_scale_optimizer.py?line=578'>579</a>\u001b[0m weights \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars]\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:583\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=580'>581</a>\u001b[0m var_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(var_list)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=581'>582</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gradients\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=582'>583</a>\u001b[0m   grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_gradients(tape, loss, var_list, grad_loss)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=584'>585</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assert_valid_dtypes([\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=585'>586</a>\u001b[0m     v \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=586'>587</a>\u001b[0m     \u001b[39mif\u001b[39;00m g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m v\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m tf\u001b[39m.\u001b[39mresource\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=587'>588</a>\u001b[0m ])\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=589'>590</a>\u001b[0m \u001b[39mreturn\u001b[39;00m grads_and_vars\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:464\u001b[0m, in \u001b[0;36mOptimizerV2._get_gradients\u001b[1;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=461'>462</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_gradients\u001b[39m(\u001b[39mself\u001b[39m, tape, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=462'>463</a>\u001b[0m   \u001b[39m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=463'>464</a>\u001b[0m   grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list, grad_loss)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/optimizer_v2/optimizer_v2.py?line=464'>465</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1076'>1077</a>\u001b[0m \u001b[39mif\u001b[39;00m output_gradients \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1077'>1078</a>\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1078'>1079</a>\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(output_gradients)]\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1080'>1081</a>\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1081'>1082</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1082'>1083</a>\u001b[0m     flat_targets,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1083'>1084</a>\u001b[0m     flat_sources,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1084'>1085</a>\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1085'>1086</a>\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1086'>1087</a>\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1088'>1089</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1089'>1090</a>\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=1090'>1091</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=62'>63</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=63'>64</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=64'>65</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=67'>68</a>\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=68'>69</a>\u001b[0m     target,\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=69'>70</a>\u001b[0m     sources,\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=70'>71</a>\u001b[0m     output_gradients,\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=71'>72</a>\u001b[0m     sources_raw,\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/imperative_grad.py?line=72'>73</a>\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=153'>154</a>\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=154'>155</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=155'>156</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=156'>157</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/backprop.py?line=157'>158</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_grad.py:340\u001b[0m, in \u001b[0;36m_EinsumGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=337'>338</a>\u001b[0m y_shape \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape(y)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=338'>339</a>\u001b[0m grad_x \u001b[39m=\u001b[39m _GetGradWrt(grad, y, x_shape, x_subs, y_subs, output_subs)\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=339'>340</a>\u001b[0m grad_y \u001b[39m=\u001b[39m _GetGradWrt(grad, x, y_shape, y_subs, x_subs, output_subs)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=341'>342</a>\u001b[0m \u001b[39mif\u001b[39;00m ellipsis \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m output_subs:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=342'>343</a>\u001b[0m   \u001b[39m# If no ellipsis in the output; then no need to unbroadcast.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=343'>344</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_x, grad_y\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_grad.py:280\u001b[0m, in \u001b[0;36m_EinsumGrad.<locals>._GetGradWrt\u001b[1;34m(output_grad, other_operand, input_shape, input_subs, other_subs, output_subs)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=275'>276</a>\u001b[0m left_subs \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m input_subs \u001b[39mif\u001b[39;00m s \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m reduced_label_set)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=277'>278</a>\u001b[0m \u001b[39m# Compute the gradient wrt the input, without accounting for the operation\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=278'>279</a>\u001b[0m \u001b[39m# \"abc->ac\". So, now we have the VJP of the operation \"ac,cd->ad\".\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=279'>280</a>\u001b[0m grad_reduced \u001b[39m=\u001b[39m gen_linalg_ops\u001b[39m.\u001b[39;49meinsum([output_grad, other_operand],\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=280'>281</a>\u001b[0m                                      \u001b[39m\"\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m,\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m->\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=281'>282</a>\u001b[0m                                          output_subs, other_subs,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=282'>283</a>\u001b[0m                                          left_subs))\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=283'>284</a>\u001b[0m \u001b[39m# If the reduced_label_set is empty, then we already have the gradient\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=284'>285</a>\u001b[0m \u001b[39m# wrt the input.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/linalg_grad.py?line=285'>286</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reduced_label_set:\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py:1082\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(inputs, equation, name)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1079'>1080</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1080'>1081</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1081'>1082</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1082'>1083</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mEinsum\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, inputs, \u001b[39m\"\u001b[39;49m\u001b[39mequation\u001b[39;49m\u001b[39m\"\u001b[39;49m, equation)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1083'>1084</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/ops/gen_linalg_ops.py?line=1084'>1085</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'], run_eagerly=True)\n",
    "train(\n",
    "  model=model,\n",
    "  epochs=200,\n",
    "  units=UNITS,\n",
    "  val_ds=valid_ds.batch(8),\n",
    "  train_ds=train_ds.batch(8),\n",
    "  save_weights_only=False if vanilla_model else True,\n",
    "  mode='fit', type_model=EXTRACTOR_MODEL, inner_ly=INNER_LY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVA_INNER = 3\n",
    "EVA_CLASS = 32\n",
    "EVA_UNITS = 1024 #2048 1024\n",
    "EVA_TYPE  = 'resnet'\n",
    "EVA_MODEL_CLASS = 32\n",
    "MAX_VEC_LEN = 1024 * 8\n",
    "#Shape of vector result: (2048,)\n",
    "USE_FACES = True\n",
    "SUFIX = '_faces' if USE_FACES else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_eva_path = f'./data/class_array{\"_faces\" if USE_FACES else \"\"}_{EVA_CLASS}.pkl'\n",
    "class_array_eva = pkl.load(open(class_eva_path, 'rb'))\n",
    "\n",
    "parmas_eval = {\n",
    "  'num_classes':  EVA_MODEL_CLASS,\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': EVA_TYPE,\n",
    "  'units': EVA_UNITS,\n",
    "  'inner_layers': EVA_INNER\n",
    "}\n",
    "model_eva = AnimeClassifier(**parmas_eval)\n",
    "model_eva.build(input_shape=(None, *parmas_eval['input_shape']))\n",
    "PATH_BEST_EVA = f'./models/{EVA_TYPE}_{EVA_MODEL_CLASS}class_{EVA_UNITS}_units_{EVA_INNER}{SUFIX}.h5'\n",
    "model_eva.load_weights(PATH_BEST_EVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TFRECORD_PATH_EVA = f'./data/anime{\"_faces\" if USE_FACES else \"\"}_data_{EVA_CLASS}.tfrecord'\n",
    "tf_record_eva = load_tfrecord_dataset(TFRECORD_PATH_EVA, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_eva)\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_eva = tf_record_eva.shuffle(all_ds_len, seed=SEED)\n",
    "tf_record_eva = tf_record_eva.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}') \"\"\"\n",
    "\n",
    "tf_record_eva = test_ds.batch(32)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for images, label in tf_record_eva:\n",
    "  preds = model_eva.predict(images)\n",
    "  all_labels.extend(label)\n",
    "  all_preds.extend(np.argmax(preds, axis=1))\n",
    "del tf_record_eva\n",
    "\n",
    "confusion_matrix= tf.math.confusion_matrix(all_labels, all_preds, num_classes=EVA_CLASS)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title(f'Confusion matrix - {EVA_CLASS} classes - {EVA_TYPE} - {EVA_UNITS} units - {EVA_INNER} inner layers - {\"Faces\" if USE_FACES else \"\"}')\n",
    "sns.heatmap(confusion_matrix.numpy(), annot=True, cmap='Blues')\n",
    "plt.savefig(f'./metrics/confusion_matrix_{EVA_CLASS}_{EVA_TYPE}_{EVA_UNITS}_{EVA_INNER}_inner{SUFIX}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_cpu(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def cosine_similarity_cpum(u, v):\n",
    "  u_dot_v = np.sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = np.sqrt(np.sum(u*u))\n",
    "  mod_v = np.sqrt(np.sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tf(a, b):\n",
    "  return tf.tensordot(a, b, axes=1) / (tf.norm(a) * tf.norm(b))\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tfm(u, v):\n",
    "  u_dot_v = tf.reduce_sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = tf.sqrt(tf.reduce_sum(u*u))\n",
    "  mod_v = tf.sqrt(tf.reduce_sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@numba.guvectorize([\"void(float64[:], float64[:], float64[:])\"], \"(n),(n)->()\", target='parallel', fastmath =True)\n",
    "def fast_cosine_gufunc(u, v, result):\n",
    "    m = u.shape[0]\n",
    "    udotv = 0\n",
    "    u_norm = 0\n",
    "    v_norm = 0\n",
    "    for i in range(m):\n",
    "        if (np.isnan(u[i])) or (np.isnan(v[i])):\n",
    "            continue\n",
    "\n",
    "        udotv += u[i] * v[i]\n",
    "        u_norm += u[i] * u[i]\n",
    "        v_norm += v[i] * v[i]\n",
    "\n",
    "    u_norm = np.sqrt(u_norm)\n",
    "    v_norm = np.sqrt(v_norm)\n",
    "\n",
    "    if (u_norm == 0) or (v_norm == 0):\n",
    "        ratio = 1.0\n",
    "    else:\n",
    "        ratio = udotv / (u_norm * v_norm)\n",
    "    result[:] = ratio\n",
    "\n",
    "@numba.jit(nopython=False, parallel=True)\n",
    "def cosine_similarity_numba(u, v):\n",
    "  uv = 0\n",
    "  uu = 0\n",
    "  vv = 0\n",
    "  \n",
    "  for i in range(u.shape[0]):\n",
    "    uv += u[i]*v[i]\n",
    "    uu += u[i]*u[i]\n",
    "    vv += v[i]*v[i]\n",
    "  cos_theta = 1\n",
    "  \n",
    "  if uu != 0 and vv != 0:\n",
    "    cos_theta = uv / np.sqrt(uu*vv)\n",
    "  return cos_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_vec(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'class_name': tf.io.FixedLenFeature([], tf.string),\n",
    "  })\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  y_train = x['class_name']\n",
    "  if y_train is None:\n",
    "    y_train = ''\n",
    "\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset_vec(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord_vec(x, size))\n",
    "\n",
    "def parse_record_vec(combination):\n",
    "  item_1, item_2 = combination\n",
    "  img_1, label_1 = item_1\n",
    "  img_2, label_2 = item_2\n",
    "  return (img_1, img_2, label_1 == label_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "#TFRECORD_PATH_VEC = './data/anime_faces_data_32.tfrecord'\n",
    "#tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "tf_record_vec = test_ds\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "\n",
    "#n_train = int(all_ds_len * 0.95)\n",
    "#n_test = int(all_ds_len * 0.05)\n",
    "\n",
    "#tf_record_vec = tf_record_vec.shuffle(n_train + n_test, seed=SEED)\n",
    "#tf_record_vec = tf_record_vec.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {all_ds_len}')\n",
    "\n",
    "all_combinations = list(itertools.combinations(tf_record_vec, 2))\n",
    "\n",
    "all_combinations = list(map(parse_record_vec, all_combinations))\n",
    "#rd.shuffle(all_combinations)\n",
    "del tf_record_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(combinations, model):\n",
    "  result = []\n",
    "  for idx, item in enumerate(combinations):\n",
    "    img_1, img_2, label = item\n",
    "    pred_1, pred_2 = model.vectorize(np.array([img_1, img_2]))\n",
    "    #cos_sim = cosine_similarity_cpu(pred_1, pred_2)\n",
    "    pred_1 = np.array(pred_1)\n",
    "    pred_2 = np.array(pred_2)\n",
    "    cos_sim = cosine_similarity_numba(pred_1, pred_2)\n",
    "    result.append((cos_sim, label))\n",
    "    #print(f'{idx + 1}/{len(combinations)} - {round(((idx + 1) / len(combinations)) * 100, 2)}%')\n",
    "  return result\n",
    "\n",
    "result = calculate_cosine_similarity(all_combinations, model_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['cos_sim', 'label']\n",
    "df.label = df.label.astype(bool)\n",
    "\n",
    "current_f1 = -1\n",
    "metrics = {}\n",
    "for threshold in range(0, 100, 1):\n",
    "  pred_label = df.apply(lambda x: True if x.cos_sim > (threshold/100) else False, axis=1)\n",
    "  precision_result = sk_metrics.precision_score(df.label, pred_label) # Impact when the model predict to lot False positives\n",
    "  recall_result = sk_metrics.recall_score(df.label, pred_label) # Impact when the model predict to lot False negatives\n",
    "  f1_result = sk_metrics.f1_score(df.label, pred_label) # Impact when the model predict to lot False positives and False negatives\n",
    "  if f1_result > current_f1:\n",
    "    current_f1 = f1_result\n",
    "    metrics['precision'] = precision_result\n",
    "    metrics['recall'] = recall_result\n",
    "    metrics['f1'] = f1_result\n",
    "    metrics['threshold'] = threshold / 100\n",
    "\n",
    "print(metrics)\n",
    "json.dump(metrics, open(f'./metrics/metrics_{EVA_CLASS}_{EVA_TYPE}_{EVA_UNITS}_{EVA_INNER}_inner{SUFIX}_temp.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embeddings with Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "TFRECORD_PATH_VEC = './data/anime_faces_data_32.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = tf_record_vec.reduce(0, lambda x,_: x+1).numpy()\n",
    "LEN_TF_RECORD = all_ds_len if True else MAX_VEC_LEN\n",
    "print(LEN_TF_RECORD)\n",
    "\n",
    "tf_record_vec = tf_record_vec.take(LEN_TF_RECORD)\n",
    "#tf_record_vec = tf_record_vec.shuffle(LEN_TF_RECORD, seed=SEED)\n",
    "\n",
    "data_image = []\n",
    "BATCH = 64\n",
    "for idx, batch in enumerate(tf_record_vec.batch(BATCH)):\n",
    "  imgs, class_names = batch\n",
    "  pred_vecs = model_eva.vectorize(imgs)\n",
    "  pred_vecs = pred_vecs.numpy().astype('float32')\n",
    "  class_names = class_names.numpy().astype('str')\n",
    "  data_image.extend(list(zip(pred_vecs, class_names)))\n",
    "  print(f'{idx + 1}/{LEN_TF_RECORD // BATCH} - {round(((idx + 1) / (LEN_TF_RECORD // BATCH)) * 100, 2)}%')\n",
    "  #img, class_name = item\n",
    "  #pred_vec = model_eva.vectorize(np.array([img]))[0]\n",
    "  #data_item = (\n",
    "  #  pred_vec.numpy().astype('float32'),\n",
    "  #  class_name.numpy().decode('utf-8')\n",
    "  #)\n",
    "  #data_image.append(data_item)\n",
    "  #print(f'{idx + 1}/{LEN_TF_RECORD} - {round(((idx + 1) / LEN_TF_RECORD) * 100, 2)}%')\n",
    "\n",
    "pkl.dump(np.array(data_image), open(f'./data/data_image_{all_ds_len}{SUFIX}.pkl', 'wb'))\n",
    "del data_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_image = pkl.load(open(f'./data/data_image_{4007}{SUFIX}.pkl', 'rb'))\n",
    "vector_images = np.array(list(data_image[:, 0])) \n",
    "\n",
    "d = 2048 #Shape of vector result: (2048,)\n",
    "nb = 4007\n",
    "res = faiss.StandardGpuResources()  # use a single GPU\n",
    "\n",
    "# build a flat (CPU) index\n",
    "index_flat = faiss.IndexFlatL2(d)\n",
    "# make it into a gpu index\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index_flat.add(vector_images)\n",
    "print(f'Vectors {gpu_index_flat.ntotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 # we want to see 4 nearest neighbors\n",
    "idx_search = 670\n",
    "print(f'Target: ', data_image[idx_search][1])\n",
    "\n",
    "dimension_vec, ids_result = gpu_index_flat.search(vector_images[idx_search: idx_search + 1], k) # actual search\n",
    "\n",
    "current_id = 1\n",
    "result = {}\n",
    "for idx, id_v in enumerate(ids_result[0]):\n",
    "  class_name = data_image[id_v][1]\n",
    "  if class_name not in result:\n",
    "    result[class_name] = 0\n",
    "  result[class_name] += 1\n",
    "\n",
    "print(f'\\nResult: {result}')\n",
    "print(f'\\nOnly name: {\", \".join(list(result.keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 # we want to see 4 nearest neighbors\n",
    "idx_search = 670\n",
    "izumi = load_img(\"./images/unnamed.jpg\", target_size=(224, 224))\n",
    "izumi_2 = load_img(\"./images/izumi____98..png\", target_size=(224, 224))\n",
    "miyamura = load_img(\"./images/miyamura____127..png\", target_size=(224, 224))\n",
    "aika = load_img(\"./images/aika____10..png\", target_size=(224, 224))\n",
    "\n",
    "aika = np.array(aika)\n",
    "izumi = np.array(izumi)\n",
    "miyamura = np.array(miyamura)\n",
    "izumi_2 = np.array(izumi_2)\n",
    "\n",
    "images = preprocess_input(np.array([izumi, izumi_2, miyamura, aika]), mode='tf')\n",
    "\n",
    "result = model_eva.vectorize(images)\n",
    "result = result.numpy().astype('float32')\n",
    "\n",
    "dimension_vec, ids_result = gpu_index_flat.search(result[0:1], k) # actual search\n",
    "\n",
    "current_id = 1\n",
    "result = {}\n",
    "for idx, id_v in enumerate(ids_result[0]):\n",
    "  class_name = data_image[id_v][1]\n",
    "  if class_name not in result:\n",
    "    result[class_name] = 0\n",
    "  result[class_name] += 1\n",
    "\n",
    "print(f'\\nResult: {result}')\n",
    "print(f'\\nOnly name: {\", \".join(list(result.keys()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalaute time TF vs Numba vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "results = model_16.predict(images)\n",
    "iterations = 1000\n",
    "\n",
    "cpu_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  cpu_r.append(cosine_similarity_cpum(tg_vector, results))\n",
    "  #for reuslt in results:\n",
    "  #  cpu_r.append(cosine_similarity_cpu(reuslt, reuslt))\n",
    "end = time.time()\n",
    "print(f'Time to compute on CPU: {end - start}')\n",
    "\n",
    "numba_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  numba_r.append(fast_cosine_gufunc(results, tg_vector))\n",
    "end = time.time()\n",
    "print(f'Time to compute on Numba: {end - start}')\n",
    "\n",
    "images_gpu = [tf.convert_to_tensor(result) for result in results]\n",
    "tf_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = images_gpu[0]\n",
    "  tf_r.append(cosine_similarity_tfm(tg_vector, results))\n",
    "  #for reuslt in images_gpu:\n",
    "  #  tf_r.append(cosine_similarity_tf(result, images_gpu[0]))\n",
    "end = time.time()\n",
    "print(f'Time to compute on TF: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeClassifier(\n",
    "  num_classes=len(class_array),\n",
    "  input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "  type_extractor='vgg',\n",
    "  units=UNITS,\n",
    "  inner_layers=1\n",
    ")\n",
    "model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "PATH_BEST = './models/vgg_16class_1024_units_aqr.h5'\n",
    "model.load_weights(PATH_BEST)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vector and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "izumi = load_img(\"./images/izumi____110..png\", target_size=(224, 224))\n",
    "izumi_2 = load_img(\"./images/izumi____98..png\", target_size=(224, 224))\n",
    "miyamura = load_img(\"./images/miyamura____127..png\", target_size=(224, 224))\n",
    "aika = load_img(\"./images/aika____10..png\", target_size=(224, 224))\n",
    "aika_ext = load_img(\"./images/Aika_Teen.webp\", target_size=(224, 224))\n",
    "\n",
    "aika = np.array(aika)\n",
    "izumi = np.array(izumi)\n",
    "miyamura = np.array(miyamura)\n",
    "izumi_2 = np.array(izumi_2)\n",
    "aika_ext = np.array(aika_ext)\n",
    "\n",
    "images = preprocess_input(np.array([izumi, izumi_2, miyamura, aika, aika_ext]), mode='tf')\n",
    "\n",
    "izumi_v, izumi_2_v, miyamura_v, aika_v, aika_ext_v= model_eva.vectorize(images)\n",
    "print(f'Shape of vectors: {izumi_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_numba(np.array(aika_ext_v), np.array(aika_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_numba(np.array(db1_v), np.array(db2_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg = pd.read_csv('./data/run-scalars_best_32_class_1024_train-tag-epoch_accuracy.csv')\n",
    "df_res = pd.read_csv('./data/run-scalars_besT_32_class_1024_resnet_train-tag-epoch_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg.columns = ['Wall time', 'Step', 'Accuracy']\n",
    "df_res.columns = ['Wall time', 'Step', 'Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plor line with legent\n",
    "#smooth line 0.6\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x='Step', y='Accuracy', data=df_vgg, label='VGG', ci=None)\n",
    "sns.lineplot(x='Step', y='Accuracy', data=df_res, label='ResNet', ci=None)\n",
    "plt.legend()\n",
    "#font szie 18 x y\n",
    "plt.xlabel('Step', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "#font legend size 18\n",
    "plt.legend(fontsize=16)\n",
    "#Save\n",
    "plt.savefig('./images/accuracy_plot.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATCH TEST\n",
    "generate_patch_layer = GeneratePatch(patch_size=PATCH_SIZE)\n",
    "\n",
    "image = load_img('./images/db1.jpg', target_size=(224, 224))\n",
    "image = np.array(image)\n",
    "image = preprocess_input(image, mode='tf')\n",
    "\n",
    "patches = generate_patch_layer(np.array([image]))\n",
    "print ('patch per image and patches shape: ', patches.shape[1], '\\n', patches.shape)\n",
    "\n",
    "render_image_and_patches(np.array([image]), patches, PATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "852bc408046ca7dfc5c8f91ce764d8630d2287ca09c7fe9d1b4d9cd156705bcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
