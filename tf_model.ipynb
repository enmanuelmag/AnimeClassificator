{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import numba\n",
    "import faiss\n",
    "import hashlib\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from numba import vectorize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk_metrics\n",
    "from alive_progress import alive_bar\n",
    "from notifier import Notifier, notify\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import pickle as pkl\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "SIZE_IMG = 224 #224#224#416\n",
    "UNITS = 1024 #2048 1024 128 256 512-seq\n",
    "MAX_CLASS = 32 #1024 32 16 8\n",
    "\n",
    "DATASET_PATH = './data/animes'\n",
    "DATASET_FACES_PATH = './data/faces'\n",
    "CLASS_ARRAY_PATH = f'./data/class_array_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_FACES_PATH = f'./data/class_array_faces_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_VEC_PATH = f'./data/class_array_vec_{MAX_CLASS}.pkl'\n",
    "\n",
    "DATASET_JSON_PATH = './data/anime_data.json'\n",
    "\n",
    "AMOUNT_TABLE_PATH = './data/anime_amount.pkl'\n",
    "AMOUNT_FACES_TABLE_PATH = './data/faces_amount.pkl'\n",
    "\n",
    "DATASET_JSON_RANK = './data/anime_rank.json'\n",
    "\n",
    "TFRECORD_PATH = f'./data/anime_data_{MAX_CLASS}.tfrecord'\n",
    "TFRECORD_FACES_PATH = f'./data/anime_faces_data_{MAX_CLASS}.tfrecord'\n",
    "\n",
    "TG_ID = \"293701727\"\n",
    "TG_TOKEN = \"1878628343:AAEFVRsqDz63ycmaLOFS7gvsG969wdAsJ0w\"\n",
    "WEBHOOK_URL = \"https://discord.com/api/webhooks/796406472459288616/PAkiGGwqe0_PwtBxXYQvOzbk78B4RQP6VWRkvpBtw6Av0sc_mDa3saaIlwVPFjOIeIbt\"\n",
    "\n",
    "#seed random seed to 42 for reproducibility\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if False:\n",
    "  #DATASET_PATH = DATASET_FACES_PATH\n",
    "  CLASS_ARRAY_PATH = CLASS_ARRAY_FACES_PATH\n",
    "  TFRECORD_PATH = TFRECORD_FACES_PATH\n",
    "  AMOUNT_TABLE_PATH = AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_image(url, anime_name, idx):\n",
    "  #download image from url\n",
    "  file_path = f'./data/animes/{anime_name}____{idx}.jpg' \n",
    "  if os.path.exists(file_path):\n",
    "    return\n",
    "\n",
    "  img_data = requests.get(url).content\n",
    "  with open(file_path, 'wb') as handler:\n",
    "    handler.write(img_data)\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Anime images',\n",
    "  msg='Finished downloading anime images'\n",
    ")\n",
    "def get_images(data):\n",
    "  #with alive_bar(len(data)) as bar:\n",
    "  for idx_a, anime_name in enumerate(data):\n",
    "    urls = data[anime_name]\n",
    "    for idx, url in enumerate(urls):\n",
    "      if idx >= 400:\n",
    "        break\n",
    "      name_clean = re.sub(r'_+', r'_', re.sub(r'[\\W\\s]', r'_', anime_name))\n",
    "      try:\n",
    "        dowload_image(url['image'], name_clean, idx)\n",
    "      except Exception as e:\n",
    "        print(f'Error on download image {idx + 1} of {anime_name}')\n",
    "        pass\n",
    "    #bar()\n",
    "    print(f'Progress: {idx_a + 1}/{len(data)} - {round((idx_a + 1)/len(data)*100, 2)}%')\n",
    "\n",
    "def get_classes_anime(path):\n",
    "  classes = set()\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    classes.add(class_name)\n",
    "  return list(classes)\n",
    "\n",
    "def wait_for_it(driver, xpath, timeout=3):\n",
    "  try:\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "def iter_post(driver):\n",
    "  anime_data = []\n",
    "\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = True\n",
    "\n",
    "  while next_button is not None:\n",
    "    if len(anime_data) > 400:\n",
    "      break\n",
    "    ul_element = wait_for_it(driver, '//ul[@id=\"post-list-posts\"]')\n",
    "    if ul_element is None:\n",
    "      next_button = wait_for_it(driver, xpath_next)\n",
    "      if next_button is not None:\n",
    "        next_button.click()\n",
    "        time.sleep(1)\n",
    "      continue\n",
    "    for i, li_element in enumerate(ul_element.find_elements(By.TAG_NAME, 'li')):\n",
    "      a_video = li_element.find_element(By.XPATH, './a').get_attribute('href')\n",
    "      a_image = li_element.find_element(By.XPATH, './div/a/img').get_attribute('src')\n",
    "      anime_data.append({\n",
    "        'video': a_video,\n",
    "        'image': a_image\n",
    "      })\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "    if next_button is not None:\n",
    "      next_button.click()\n",
    "      time.sleep(rd.randint(1, 2))\n",
    "  return anime_data\n",
    "\n",
    "def get_images_links(url, driver, anime_name):\n",
    "  url_search = url + anime_name\n",
    "  driver.get(url_search)\n",
    "  return iter_post(driver)\n",
    "\n",
    "def get_names(driver):\n",
    "  names = []\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = wait_for_it(driver, xpath_next)\n",
    "  \n",
    "  while next_button is not None:\n",
    "    for tr_element in driver.find_elements(By.XPATH, '//table[@class=\"highlightable\"]/tbody/tr'):\n",
    "      try:\n",
    "        amount_post = tr_element.find_element(By.XPATH, './td[1]').text\n",
    "        amount_post = int(amount_post)\n",
    "        if amount_post >= 10:\n",
    "          a_name = tr_element.find_element(By.XPATH, './td[2]/a[2]' ).text\n",
    "          names.append(a_name)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    next_button.click()\n",
    "    time.sleep(rd.randint(1, 2))\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "  return names\n",
    "\n",
    "def get_score(anime_name, driver):\n",
    "  url_search = f'https://myanimelist.net/anime.php?cat=anime&q={anime_name}'\n",
    "  driver.get(url_search)\n",
    "  score = 0\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    score += 1\n",
    "  return score\n",
    "\n",
    "def relevant_anime(anime_name, df_anime, amount_table, threshold=350, rank=True):\n",
    "  \n",
    "  if amount_table.get(anime_name, 0) <= threshold:\n",
    "    return False\n",
    "\n",
    "  if not rank:\n",
    "    return True\n",
    "\n",
    "  anime_name = re.sub(r'_', r' ', anime_name)\n",
    "  df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "\n",
    "  if df_result.empty:\n",
    "    anime_name = ' '.join(anime_name.split(' ')[:3])\n",
    "    df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "  return not df_result.empty\n",
    "\n",
    "def amount_anime_table(datapath):\n",
    "  dic = {}\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    dic[class_name] = dic.get(class_name, 0) + 1\n",
    "  return dic\n",
    "\n",
    "def detect(filename, cascade_file):\n",
    "  if not os.path.isfile(cascade_file):\n",
    "    raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "  cascade = cv2.CascadeClassifier(cascade_file)\n",
    "  image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "  #src = cv2.cuda_GpuMat()\n",
    "  #src.upload(image)\n",
    "\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray = cv2.equalizeHist(gray)\n",
    "\n",
    "  faces = cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor = 1.1,\n",
    "    minNeighbors = 5,\n",
    "    minSize = (24, 24)\n",
    "  )\n",
    "\n",
    "  new_images = []\n",
    "  for (x, y, w, h) in faces:\n",
    "    new_images.append(image[y:y+h, x:x+w])\n",
    "  #clahe = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "  #dst = clahe.apply(src, cv2.cuda_Stream.Null())\n",
    "  #result = dst.download()\n",
    "  return new_images\n",
    "\n",
    "def extract_faces(datapath):\n",
    "  faces_amount = 0\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    new_images = []\n",
    "    try:\n",
    "      new_images = detect(datapath + '/' + filename, './data/haar/lbpcascade_animeface.xml')\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      pass\n",
    "    if len(new_images) > 0:\n",
    "      for idx, img in enumerate(new_images):\n",
    "        new_face_name = f'./data/faces/{class_name}____{idx}.jpg'\n",
    "        try:\n",
    "          if not os.path.exists(new_face_name):\n",
    "            cv2.imwrite(new_face_name, img)\n",
    "            faces_amount += 1\n",
    "        except:\n",
    "          pass\n",
    "  print(f'Faces amount: {faces_amount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anime images\n",
    "anime_data = json.load(open(DATASET_JSON_PATH))\n",
    "get_images(anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only faces\n",
    "extract_faces(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FACES Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_FACES_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_anime_rank.pkl')\n",
    "\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "\n",
    "class_array = set()\n",
    "for anime_name in all_class_array:\n",
    "  if relevant_anime(anime_name, df, amount_table, threshold=100, rank=True):\n",
    "    class_array.add((anime_name, amount_table[anime_name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "class_array = [x[0] for x in class_array]\n",
    "\n",
    "mean_keyframes = np.mean([amount_table[x] for x in class_array])\n",
    "pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)} - Mean keyframes: {int(mean_keyframes)}')\n",
    "\n",
    "del all_class_array\n",
    "del mean_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for faces\n",
    "all_class_array = get_classes_anime(DATASET_FACES_PATH)\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "\n",
    "class_array = set()\n",
    "for name in all_class_array:\n",
    "  class_array.add((name, amount_table[name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "\n",
    "class_array = list(class_array)\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)}')\n",
    "del all_class_array\n",
    "\n",
    "class_array = [x[0] for x in class_array]\n",
    "pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(class_name):\n",
    "  return class_array.index(class_name)\n",
    "\n",
    "def build_example(path_file, class_name):\n",
    "  img_array = open(path_file, 'rb').read()\n",
    "  \n",
    "  #img = load_img(path_file, target_size=(SIZE_IMG, SIZE_IMG))\n",
    "  #img_array = np.array(img)\n",
    "  #img_array = preprocess_input(img_array, mode='tf')\n",
    "  #key = hashlib.sha256(img_array).hexdigest()\n",
    "  example = tf.train.Example(\n",
    "    features=tf.train.Features(feature={\n",
    "    #'key': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf-8')])),\n",
    "    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array])),\n",
    "    #'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array.tobytes()])),\n",
    "    'class_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[get_class_id(class_name)])),\n",
    "    'class_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[class_name.encode('utf-8')])),\n",
    "    'filepath': tf.train.Feature(bytes_list=tf.train.BytesList(value=[path_file.encode('utf-8')]))\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "def create_tfrecord(data_path, withe_list, path_tfrecord):\n",
    "  files = os.listdir(data_path)\n",
    "  writer = tf.io.TFRecordWriter(path_tfrecord)\n",
    "  \n",
    "  print('Started creating tfrecord')\n",
    "  for idx, filename in enumerate(files):\n",
    "    class_name, _ = filename.split('____')\n",
    "  \n",
    "    if class_name in withe_list:\n",
    "      path_file = os.path.join(data_path, filename)\n",
    "      tf_example = build_example(path_file, class_name)\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "  print('Finished creating tfrecord')\n",
    "  writer.close()\n",
    "\n",
    "def parse_tfrecord(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  #class_id = tf.sparse.to_dense(x['class_id'], default_value=-1)\n",
    "  class_id = x['class_id']\n",
    "  if class_id is None:\n",
    "    class_id = -1\n",
    "\n",
    "  labels = tf.cast(class_id, tf.int64)\n",
    "  y_train = labels\n",
    "  #y_train = tf.stack([ labels ], axis=1)\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord(x, size))\n",
    "\n",
    "def create_model(num_classes, input_shape, units, type_extractor = 'vgg') -> tf.keras.Model:\n",
    "  if type_extractor == 'vgg':\n",
    "    feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'inception':\n",
    "    feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'resnet':\n",
    "    feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  else:\n",
    "    raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(tf.keras.layers.Input(input_shape, name='input'))\n",
    "  model.add(feature_extractor)\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  #new\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "  return model\n",
    "\n",
    "\n",
    "class AnimeClassifier(tf.keras.Model):\n",
    "  def __init__(self, num_classes, input_shape, units=1024, inner_layers=12, type_extractor='vgg'):\n",
    "    assert type_extractor in ['vgg', 'inception', 'resnet']\n",
    "    assert inner_layers >= 1\n",
    "    assert num_classes >= 8\n",
    "    assert len(input_shape) == 3\n",
    "    assert units >= 64\n",
    "\n",
    "    super(AnimeClassifier, self).__init__(name='AnimeClassifier')\n",
    "\n",
    "    self.units = units\n",
    "    self.in_layer = tf.keras.layers.Input(input_shape, name='input')\n",
    "\n",
    "    if type_extractor == 'vgg':\n",
    "      feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=self.in_layer)\n",
    "    elif type_extractor == 'inception':\n",
    "      feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=self.in_layer)\n",
    "    elif type_extractor == 'resnet':\n",
    "      feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=self.in_layer)\n",
    "    else:\n",
    "      raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.global_average_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    self.hidden_mlp = []\n",
    "    for i in range(inner_layers):\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
    "\n",
    "    self.out_layer = tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax)\n",
    "\n",
    "  def call(self, inputs, training=None, mask=None):\n",
    "    x = self.feature_extractor(inputs, training=training)\n",
    "    x = self.global_average_pooling(x)\n",
    "    x = self.flatten(x, training=training)\n",
    "    for layer in self.hidden_mlp:\n",
    "      x = layer(x, training=training)\n",
    "    return self.out_layer(x, training=training)\n",
    "\n",
    "  def predict_classes(self, x):\n",
    "    return tf.argmax(self(x), axis=1)\n",
    "\n",
    "  def vectorize(self, x, flatten=True):\n",
    "    x = self.feature_extractor(x)\n",
    "    x = self.global_average_pooling(x)\n",
    "    if flatten:\n",
    "      return self.flatten(x)\n",
    "    return x\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Train model',\n",
    "  msg='Training has finished'\n",
    ")\n",
    "def train(model, train_ds, val_ds, units, epochs=15, mode='fit', type_model='vgg', save_weights_only=False, inner_ly=1):\n",
    "  logdir = \"logs/scalars/\" + time.strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "  if mode == 'eager_tf':\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "      for batch, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(images, training=True)\n",
    "          regularization_loss = tf.reduce_sum(model.losses)\n",
    "          pred_loss = []\n",
    "          for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "          total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"{}_train_{}, {}, {}\".format(\n",
    "          epoch, batch, total_loss.numpy(),\n",
    "          list(map(lambda x: np.sum(x.numpy()), pred_loss))\n",
    "        ))\n",
    "        avg_loss.update_state(total_loss)\n",
    "  elif mode == 'fit':\n",
    "    callbacks = [\n",
    "      ReduceLROnPlateau(verbose=1),\n",
    "      EarlyStopping(patience=15, verbose=1),\n",
    "      ModelCheckpoint(\n",
    "        f'checkpoints/{type_model}_{MAX_CLASS}class_{units}_units_{inner_ly}_checkpoint.h5', \n",
    "        verbose=1,\n",
    "        monitor='accuracy',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=save_weights_only,\n",
    "      ),\n",
    "      TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_data=val_ds\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Total Training Time: {end_time} seconds')\n",
    "\n",
    "IMAGE_FEATURE_MAP = {\n",
    "  'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  'class_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "if False:\n",
    "  class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "  if os.path.exists(TFRECORD_PATH):\n",
    "    os.remove(TFRECORD_PATH)\n",
    "  #DATASET_PATH DATASET_FACES_PATH\n",
    "  create_tfrecord(DATASET_PATH, class_array, TFRECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 11753\n"
     ]
    }
   ],
   "source": [
    "tf_record = load_tfrecord_dataset(TFRECORD_PATH, SIZE_IMG) #TFRECORD_PATH\n",
    "# 32: 11753\n",
    "all_ds_len = sum(1 for _ in tf_record)\n",
    "print(f'Total number of images: {all_ds_len}')\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_valid = int(all_ds_len * 0.1)\n",
    "n_test = all_ds_len - n_train - n_valid\n",
    "\n",
    "tf_record = tf_record.shuffle(n_train + n_valid + n_test, seed=SEED)\n",
    "train_ds = tf_record.take(n_train)\n",
    "valid_ds = tf_record.skip(n_train).take(n_valid)\n",
    "test_ds = tf_record.skip(n_train + n_valid).take(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Acc** |  **LR** | **Epochs** | **Batch** | **Units** | **Layers** | **Class** | **Type** | Passed |\n",
    "|:-------:|:-------:|:----------:|:---------:|:---------:|:----------:|:---------:|:---------:|:--------:|\n",
    "|    0.975 | 0.00001 |        300 |        32 |      1024 |          1 |         8 |      VGG | - |\n",
    "|    0.925 | 0.000025|        300 |        32 |      1024 |          1 |        16 |      VGG | x |\n",
    "|    0.903 | 0.000025|        300 |        32 |      1024 |          1 |        32 |      VGG | - |\n",
    "|    FACES |\n",
    "|          | 0.000025|        300 |        32 |      1024 |          1 |        32 |       VGG| Face detector doesn't work |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "    294/Unknown - 56s 134ms/step - loss: 3.5263 - accuracy: 0.0358\n",
      "Epoch 1: accuracy improved from -inf to 0.03584, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 70s 182ms/step - loss: 3.5263 - accuracy: 0.0358 - val_loss: 3.4603 - val_accuracy: 0.0298 - lr: 5.0000e-05\n",
      "Epoch 2/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.4610 - accuracy: 0.0397\n",
      "Epoch 2: accuracy improved from 0.03584 to 0.03967, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 180ms/step - loss: 3.4610 - accuracy: 0.0397 - val_loss: 3.4535 - val_accuracy: 0.0468 - lr: 5.0000e-05\n",
      "Epoch 3/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.4425 - accuracy: 0.0334\n",
      "Epoch 3: accuracy did not improve from 0.03967\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 3.4425 - accuracy: 0.0334 - val_loss: 3.4277 - val_accuracy: 0.0391 - lr: 5.0000e-05\n",
      "Epoch 4/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.3954 - accuracy: 0.0400\n",
      "Epoch 4: accuracy improved from 0.03967 to 0.03999, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 3.3954 - accuracy: 0.0400 - val_loss: 3.3660 - val_accuracy: 0.0570 - lr: 5.0000e-05\n",
      "Epoch 5/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.3496 - accuracy: 0.0444\n",
      "Epoch 5: accuracy improved from 0.03999 to 0.04435, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 3.3496 - accuracy: 0.0444 - val_loss: 3.3403 - val_accuracy: 0.0749 - lr: 5.0000e-05\n",
      "Epoch 6/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.3039 - accuracy: 0.0540\n",
      "Epoch 6: accuracy improved from 0.04435 to 0.05403, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 3.3039 - accuracy: 0.0540 - val_loss: 3.3153 - val_accuracy: 0.0672 - lr: 5.0000e-05\n",
      "Epoch 7/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.2631 - accuracy: 0.0591\n",
      "Epoch 7: accuracy improved from 0.05403 to 0.05914, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 3.2631 - accuracy: 0.0591 - val_loss: 3.2637 - val_accuracy: 0.0689 - lr: 5.0000e-05\n",
      "Epoch 8/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.2279 - accuracy: 0.0648\n",
      "Epoch 8: accuracy improved from 0.05914 to 0.06477, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 3.2279 - accuracy: 0.0648 - val_loss: 3.2405 - val_accuracy: 0.0732 - lr: 5.0000e-05\n",
      "Epoch 9/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.2008 - accuracy: 0.0758\n",
      "Epoch 9: accuracy improved from 0.06477 to 0.07583, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 3.2008 - accuracy: 0.0758 - val_loss: 3.1465 - val_accuracy: 0.0826 - lr: 5.0000e-05\n",
      "Epoch 10/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.1671 - accuracy: 0.0752\n",
      "Epoch 10: accuracy did not improve from 0.07583\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 3.1671 - accuracy: 0.0752 - val_loss: 3.1728 - val_accuracy: 0.0919 - lr: 5.0000e-05\n",
      "Epoch 11/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.1375 - accuracy: 0.0770\n",
      "Epoch 11: accuracy improved from 0.07583 to 0.07700, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 180ms/step - loss: 3.1375 - accuracy: 0.0770 - val_loss: 3.1893 - val_accuracy: 0.1055 - lr: 5.0000e-05\n",
      "Epoch 12/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.0886 - accuracy: 0.0852\n",
      "Epoch 12: accuracy improved from 0.07700 to 0.08519, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 181ms/step - loss: 3.0886 - accuracy: 0.0852 - val_loss: 3.0738 - val_accuracy: 0.1157 - lr: 5.0000e-05\n",
      "Epoch 13/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 3.0024 - accuracy: 0.0932\n",
      "Epoch 13: accuracy improved from 0.08519 to 0.09317, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 180ms/step - loss: 3.0024 - accuracy: 0.0932 - val_loss: 2.9010 - val_accuracy: 0.1183 - lr: 5.0000e-05\n",
      "Epoch 14/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.8995 - accuracy: 0.1146\n",
      "Epoch 14: accuracy improved from 0.09317 to 0.11455, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 180ms/step - loss: 2.8995 - accuracy: 0.1146 - val_loss: 2.8892 - val_accuracy: 0.1396 - lr: 5.0000e-05\n",
      "Epoch 15/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.8279 - accuracy: 0.1230\n",
      "Epoch 15: accuracy improved from 0.11455 to 0.12295, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 2.8279 - accuracy: 0.1230 - val_loss: 2.7400 - val_accuracy: 0.1345 - lr: 5.0000e-05\n",
      "Epoch 16/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.7605 - accuracy: 0.1353\n",
      "Epoch 16: accuracy improved from 0.12295 to 0.13529, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 58s 180ms/step - loss: 2.7605 - accuracy: 0.1353 - val_loss: 2.6492 - val_accuracy: 0.1557 - lr: 5.0000e-05\n",
      "Epoch 17/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.6871 - accuracy: 0.1398\n",
      "Epoch 17: accuracy improved from 0.13529 to 0.13976, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 2.6871 - accuracy: 0.1398 - val_loss: 2.4894 - val_accuracy: 0.1949 - lr: 5.0000e-05\n",
      "Epoch 18/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.6268 - accuracy: 0.1541\n",
      "Epoch 18: accuracy improved from 0.13976 to 0.15412, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 2.6268 - accuracy: 0.1541 - val_loss: 2.4786 - val_accuracy: 0.1932 - lr: 5.0000e-05\n",
      "Epoch 19/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.5573 - accuracy: 0.1728\n",
      "Epoch 19: accuracy improved from 0.15412 to 0.17284, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 2.5573 - accuracy: 0.1728 - val_loss: 2.4865 - val_accuracy: 0.2000 - lr: 5.0000e-05\n",
      "Epoch 20/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.4942 - accuracy: 0.1845\n",
      "Epoch 20: accuracy improved from 0.17284 to 0.18454, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 2.4942 - accuracy: 0.1845 - val_loss: 2.3244 - val_accuracy: 0.2281 - lr: 5.0000e-05\n",
      "Epoch 21/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.4208 - accuracy: 0.1989\n",
      "Epoch 21: accuracy improved from 0.18454 to 0.19889, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 2.4208 - accuracy: 0.1989 - val_loss: 2.3181 - val_accuracy: 0.2349 - lr: 5.0000e-05\n",
      "Epoch 22/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.3440 - accuracy: 0.2160\n",
      "Epoch 22: accuracy improved from 0.19889 to 0.21602, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 2.3440 - accuracy: 0.2160 - val_loss: 2.1745 - val_accuracy: 0.2689 - lr: 5.0000e-05\n",
      "Epoch 23/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.2684 - accuracy: 0.2346\n",
      "Epoch 23: accuracy improved from 0.21602 to 0.23463, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 181ms/step - loss: 2.2684 - accuracy: 0.2346 - val_loss: 2.0876 - val_accuracy: 0.2902 - lr: 5.0000e-05\n",
      "Epoch 24/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.1966 - accuracy: 0.2570\n",
      "Epoch 24: accuracy improved from 0.23463 to 0.25697, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 2.1966 - accuracy: 0.2570 - val_loss: 1.9732 - val_accuracy: 0.3336 - lr: 5.0000e-05\n",
      "Epoch 25/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.1102 - accuracy: 0.2747\n",
      "Epoch 25: accuracy improved from 0.25697 to 0.27473, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 2.1102 - accuracy: 0.2747 - val_loss: 1.9255 - val_accuracy: 0.3319 - lr: 5.0000e-05\n",
      "Epoch 26/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 2.0421 - accuracy: 0.2916\n",
      "Epoch 26: accuracy improved from 0.27473 to 0.29164, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 2.0421 - accuracy: 0.2916 - val_loss: 1.7507 - val_accuracy: 0.3932 - lr: 5.0000e-05\n",
      "Epoch 27/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.9610 - accuracy: 0.3156\n",
      "Epoch 27: accuracy improved from 0.29164 to 0.31557, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.9610 - accuracy: 0.3156 - val_loss: 1.7133 - val_accuracy: 0.3855 - lr: 5.0000e-05\n",
      "Epoch 28/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.8771 - accuracy: 0.3262\n",
      "Epoch 28: accuracy improved from 0.31557 to 0.32621, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.8771 - accuracy: 0.3262 - val_loss: 1.6338 - val_accuracy: 0.4043 - lr: 5.0000e-05\n",
      "Epoch 29/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.7847 - accuracy: 0.3534\n",
      "Epoch 29: accuracy improved from 0.32621 to 0.35344, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.7847 - accuracy: 0.3534 - val_loss: 1.4891 - val_accuracy: 0.4553 - lr: 5.0000e-05\n",
      "Epoch 30/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.7259 - accuracy: 0.3805\n",
      "Epoch 30: accuracy improved from 0.35344 to 0.38045, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.7259 - accuracy: 0.3805 - val_loss: 1.4478 - val_accuracy: 0.4715 - lr: 5.0000e-05\n",
      "Epoch 31/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.6467 - accuracy: 0.3934\n",
      "Epoch 31: accuracy improved from 0.38045 to 0.39343, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.6467 - accuracy: 0.3934 - val_loss: 1.3133 - val_accuracy: 0.5166 - lr: 5.0000e-05\n",
      "Epoch 32/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.5747 - accuracy: 0.4159\n",
      "Epoch 32: accuracy improved from 0.39343 to 0.41587, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.5747 - accuracy: 0.4159 - val_loss: 1.2726 - val_accuracy: 0.5379 - lr: 5.0000e-05\n",
      "Epoch 33/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.5075 - accuracy: 0.4405\n",
      "Epoch 33: accuracy improved from 0.41587 to 0.44054, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.5075 - accuracy: 0.4405 - val_loss: 1.1857 - val_accuracy: 0.5549 - lr: 5.0000e-05\n",
      "Epoch 34/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.4240 - accuracy: 0.4662\n",
      "Epoch 34: accuracy improved from 0.44054 to 0.46618, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 1.4240 - accuracy: 0.4662 - val_loss: 1.1053 - val_accuracy: 0.6102 - lr: 5.0000e-05\n",
      "Epoch 35/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.3343 - accuracy: 0.5061\n",
      "Epoch 35: accuracy improved from 0.46618 to 0.50606, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.3343 - accuracy: 0.5061 - val_loss: 1.0391 - val_accuracy: 0.5966 - lr: 5.0000e-05\n",
      "Epoch 36/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.2870 - accuracy: 0.5119\n",
      "Epoch 36: accuracy improved from 0.50606 to 0.51191, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.2870 - accuracy: 0.5119 - val_loss: 0.9255 - val_accuracy: 0.6494 - lr: 5.0000e-05\n",
      "Epoch 37/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.2281 - accuracy: 0.5463\n",
      "Epoch 37: accuracy improved from 0.51191 to 0.54627, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.2281 - accuracy: 0.5463 - val_loss: 0.8786 - val_accuracy: 0.6834 - lr: 5.0000e-05\n",
      "Epoch 38/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.1477 - accuracy: 0.5623\n",
      "Epoch 38: accuracy improved from 0.54627 to 0.56233, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 1.1477 - accuracy: 0.5623 - val_loss: 0.8405 - val_accuracy: 0.6885 - lr: 5.0000e-05\n",
      "Epoch 39/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.5899\n",
      "Epoch 39: accuracy improved from 0.56233 to 0.58987, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 1.0925 - accuracy: 0.5899 - val_loss: 0.7766 - val_accuracy: 0.6902 - lr: 5.0000e-05\n",
      "Epoch 40/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.0409 - accuracy: 0.6004\n",
      "Epoch 40: accuracy improved from 0.58987 to 0.60040, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 1.0409 - accuracy: 0.6004 - val_loss: 0.7193 - val_accuracy: 0.7217 - lr: 5.0000e-05\n",
      "Epoch 41/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 1.0024 - accuracy: 0.6204\n",
      "Epoch 41: accuracy improved from 0.60040 to 0.62040, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 1.0024 - accuracy: 0.6204 - val_loss: 0.7662 - val_accuracy: 0.6996 - lr: 5.0000e-05\n",
      "Epoch 42/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9459 - accuracy: 0.6369\n",
      "Epoch 42: accuracy improved from 0.62040 to 0.63689, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.9459 - accuracy: 0.6369 - val_loss: 0.6958 - val_accuracy: 0.7174 - lr: 5.0000e-05\n",
      "Epoch 43/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.6592\n",
      "Epoch 43: accuracy improved from 0.63689 to 0.65922, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.9055 - accuracy: 0.6592 - val_loss: 0.6694 - val_accuracy: 0.7277 - lr: 5.0000e-05\n",
      "Epoch 44/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.8754 - accuracy: 0.6637\n",
      "Epoch 44: accuracy improved from 0.65922 to 0.66369, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 0.8754 - accuracy: 0.6637 - val_loss: 0.6259 - val_accuracy: 0.7464 - lr: 5.0000e-05\n",
      "Epoch 45/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.6789\n",
      "Epoch 45: accuracy improved from 0.66369 to 0.67890, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 0.8204 - accuracy: 0.6789 - val_loss: 0.5545 - val_accuracy: 0.7923 - lr: 5.0000e-05\n",
      "Epoch 46/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.7991 - accuracy: 0.6926\n",
      "Epoch 46: accuracy improved from 0.67890 to 0.69262, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.7991 - accuracy: 0.6926 - val_loss: 0.5837 - val_accuracy: 0.7855 - lr: 5.0000e-05\n",
      "Epoch 47/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.7687 - accuracy: 0.7076\n",
      "Epoch 47: accuracy improved from 0.69262 to 0.70762, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.7687 - accuracy: 0.7076 - val_loss: 0.5053 - val_accuracy: 0.7915 - lr: 5.0000e-05\n",
      "Epoch 48/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.7339 - accuracy: 0.7161\n",
      "Epoch 48: accuracy improved from 0.70762 to 0.71612, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.7339 - accuracy: 0.7161 - val_loss: 0.4855 - val_accuracy: 0.8026 - lr: 5.0000e-05\n",
      "Epoch 49/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.7288\n",
      "Epoch 49: accuracy improved from 0.71612 to 0.72878, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 0.7078 - accuracy: 0.7288 - val_loss: 0.4440 - val_accuracy: 0.8170 - lr: 5.0000e-05\n",
      "Epoch 50/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.6734 - accuracy: 0.7414\n",
      "Epoch 50: accuracy improved from 0.72878 to 0.74144, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.6734 - accuracy: 0.7414 - val_loss: 0.4580 - val_accuracy: 0.8119 - lr: 5.0000e-05\n",
      "Epoch 51/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.7510\n",
      "Epoch 51: accuracy improved from 0.74144 to 0.75101, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.6375 - accuracy: 0.7510 - val_loss: 0.4348 - val_accuracy: 0.8094 - lr: 5.0000e-05\n",
      "Epoch 52/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.7513\n",
      "Epoch 52: accuracy improved from 0.75101 to 0.75133, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.6445 - accuracy: 0.7513 - val_loss: 0.4345 - val_accuracy: 0.8230 - lr: 5.0000e-05\n",
      "Epoch 53/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.7642\n",
      "Epoch 53: accuracy improved from 0.75133 to 0.76420, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.6198 - accuracy: 0.7642 - val_loss: 0.4120 - val_accuracy: 0.8272 - lr: 5.0000e-05\n",
      "Epoch 54/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.7760\n",
      "Epoch 54: accuracy improved from 0.76420 to 0.77601, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.5774 - accuracy: 0.7760 - val_loss: 0.3806 - val_accuracy: 0.8255 - lr: 5.0000e-05\n",
      "Epoch 55/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.7773\n",
      "Epoch 55: accuracy improved from 0.77601 to 0.77728, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.5604 - accuracy: 0.7773 - val_loss: 0.4522 - val_accuracy: 0.8077 - lr: 5.0000e-05\n",
      "Epoch 56/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.5596 - accuracy: 0.7814\n",
      "Epoch 56: accuracy improved from 0.77728 to 0.78143, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.5596 - accuracy: 0.7814 - val_loss: 0.4032 - val_accuracy: 0.8289 - lr: 5.0000e-05\n",
      "Epoch 57/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.7887\n",
      "Epoch 57: accuracy improved from 0.78143 to 0.78866, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.5356 - accuracy: 0.7887 - val_loss: 0.3478 - val_accuracy: 0.8400 - lr: 5.0000e-05\n",
      "Epoch 58/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.7861\n",
      "Epoch 58: accuracy did not improve from 0.78866\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.5371 - accuracy: 0.7861 - val_loss: 0.3673 - val_accuracy: 0.8485 - lr: 5.0000e-05\n",
      "Epoch 59/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8057\n",
      "Epoch 59: accuracy improved from 0.78866 to 0.80568, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.4932 - accuracy: 0.8057 - val_loss: 0.2900 - val_accuracy: 0.8800 - lr: 5.0000e-05\n",
      "Epoch 60/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.8050\n",
      "Epoch 60: accuracy did not improve from 0.80568\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4921 - accuracy: 0.8050 - val_loss: 0.3014 - val_accuracy: 0.8519 - lr: 5.0000e-05\n",
      "Epoch 61/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.8047\n",
      "Epoch 61: accuracy did not improve from 0.80568\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4797 - accuracy: 0.8047 - val_loss: 0.3511 - val_accuracy: 0.8417 - lr: 5.0000e-05\n",
      "Epoch 62/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.8114\n",
      "Epoch 62: accuracy improved from 0.80568 to 0.81142, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4556 - accuracy: 0.8114 - val_loss: 0.2817 - val_accuracy: 0.8655 - lr: 5.0000e-05\n",
      "Epoch 63/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.8150\n",
      "Epoch 63: accuracy improved from 0.81142 to 0.81504, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.4567 - accuracy: 0.8150 - val_loss: 0.3441 - val_accuracy: 0.8451 - lr: 5.0000e-05\n",
      "Epoch 64/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.8189\n",
      "Epoch 64: accuracy improved from 0.81504 to 0.81887, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4500 - accuracy: 0.8189 - val_loss: 0.2823 - val_accuracy: 0.8570 - lr: 5.0000e-05\n",
      "Epoch 65/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8177\n",
      "Epoch 65: accuracy did not improve from 0.81887\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4394 - accuracy: 0.8177 - val_loss: 0.3217 - val_accuracy: 0.8545 - lr: 5.0000e-05\n",
      "Epoch 66/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.8288\n",
      "Epoch 66: accuracy improved from 0.81887 to 0.82876, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4324 - accuracy: 0.8288 - val_loss: 0.3143 - val_accuracy: 0.8545 - lr: 5.0000e-05\n",
      "Epoch 67/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8308\n",
      "Epoch 67: accuracy improved from 0.82876 to 0.83078, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 61s 186ms/step - loss: 0.4118 - accuracy: 0.8308 - val_loss: 0.3376 - val_accuracy: 0.8536 - lr: 5.0000e-05\n",
      "Epoch 68/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.8256\n",
      "Epoch 68: accuracy did not improve from 0.83078\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.4173 - accuracy: 0.8256 - val_loss: 0.3125 - val_accuracy: 0.8630 - lr: 5.0000e-05\n",
      "Epoch 69/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8344\n",
      "Epoch 69: accuracy improved from 0.83078 to 0.83440, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.4094 - accuracy: 0.8344 - val_loss: 0.3189 - val_accuracy: 0.8579 - lr: 5.0000e-05\n",
      "Epoch 70/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.8345\n",
      "Epoch 70: accuracy improved from 0.83440 to 0.83450, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.4046 - accuracy: 0.8345 - val_loss: 0.3008 - val_accuracy: 0.8570 - lr: 5.0000e-05\n",
      "Epoch 71/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.3930 - accuracy: 0.8323\n",
      "Epoch 71: accuracy did not improve from 0.83450\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.3930 - accuracy: 0.8323 - val_loss: 0.3042 - val_accuracy: 0.8757 - lr: 5.0000e-05\n",
      "Epoch 72/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.3954 - accuracy: 0.8378\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 72: accuracy improved from 0.83450 to 0.83780, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.3954 - accuracy: 0.8378 - val_loss: 0.2990 - val_accuracy: 0.8621 - lr: 5.0000e-05\n",
      "Epoch 73/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8459\n",
      "Epoch 73: accuracy improved from 0.83780 to 0.84588, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.3462 - accuracy: 0.8459 - val_loss: 0.2590 - val_accuracy: 0.8613 - lr: 5.0000e-06\n",
      "Epoch 74/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8551\n",
      "Epoch 74: accuracy improved from 0.84588 to 0.85514, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.3170 - accuracy: 0.8551 - val_loss: 0.2527 - val_accuracy: 0.8732 - lr: 5.0000e-06\n",
      "Epoch 75/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8609\n",
      "Epoch 75: accuracy improved from 0.85514 to 0.86088, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.2912 - accuracy: 0.8609 - val_loss: 0.2093 - val_accuracy: 0.8715 - lr: 5.0000e-06\n",
      "Epoch 76/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8589\n",
      "Epoch 76: accuracy did not improve from 0.86088\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2843 - accuracy: 0.8589 - val_loss: 0.2060 - val_accuracy: 0.8749 - lr: 5.0000e-06\n",
      "Epoch 77/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8635\n",
      "Epoch 77: accuracy improved from 0.86088 to 0.86354, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2696 - accuracy: 0.8635 - val_loss: 0.2264 - val_accuracy: 0.8672 - lr: 5.0000e-06\n",
      "Epoch 78/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.8689\n",
      "Epoch 78: accuracy improved from 0.86354 to 0.86886, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 61s 187ms/step - loss: 0.2620 - accuracy: 0.8689 - val_loss: 0.2090 - val_accuracy: 0.8911 - lr: 5.0000e-06\n",
      "Epoch 79/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.8629\n",
      "Epoch 79: accuracy did not improve from 0.86886\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2631 - accuracy: 0.8629 - val_loss: 0.2041 - val_accuracy: 0.8826 - lr: 5.0000e-06\n",
      "Epoch 80/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.8715\n",
      "Epoch 80: accuracy improved from 0.86886 to 0.87152, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2551 - accuracy: 0.8715 - val_loss: 0.2251 - val_accuracy: 0.8621 - lr: 5.0000e-06\n",
      "Epoch 81/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.8652\n",
      "Epoch 81: accuracy did not improve from 0.87152\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2531 - accuracy: 0.8652 - val_loss: 0.2177 - val_accuracy: 0.8596 - lr: 5.0000e-06\n",
      "Epoch 82/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.8655\n",
      "Epoch 82: accuracy did not improve from 0.87152\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2509 - accuracy: 0.8655 - val_loss: 0.1966 - val_accuracy: 0.8681 - lr: 5.0000e-06\n",
      "Epoch 83/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.8703\n",
      "Epoch 83: accuracy did not improve from 0.87152\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2448 - accuracy: 0.8703 - val_loss: 0.2145 - val_accuracy: 0.8740 - lr: 5.0000e-06\n",
      "Epoch 84/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.8742\n",
      "Epoch 84: accuracy improved from 0.87152 to 0.87418, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2423 - accuracy: 0.8742 - val_loss: 0.2138 - val_accuracy: 0.8698 - lr: 5.0000e-06\n",
      "Epoch 85/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.8735\n",
      "Epoch 85: accuracy did not improve from 0.87418\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2418 - accuracy: 0.8735 - val_loss: 0.2015 - val_accuracy: 0.8877 - lr: 5.0000e-06\n",
      "Epoch 86/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.8679\n",
      "Epoch 86: accuracy did not improve from 0.87418\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2427 - accuracy: 0.8679 - val_loss: 0.2117 - val_accuracy: 0.8732 - lr: 5.0000e-06\n",
      "Epoch 87/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.8669\n",
      "Epoch 87: accuracy did not improve from 0.87418\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2482 - accuracy: 0.8669 - val_loss: 0.2017 - val_accuracy: 0.8860 - lr: 5.0000e-06\n",
      "Epoch 88/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.8723\n",
      "Epoch 88: accuracy did not improve from 0.87418\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2409 - accuracy: 0.8723 - val_loss: 0.2026 - val_accuracy: 0.8834 - lr: 5.0000e-06\n",
      "Epoch 89/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.8708\n",
      "Epoch 89: accuracy did not improve from 0.87418\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2417 - accuracy: 0.8708 - val_loss: 0.1972 - val_accuracy: 0.8800 - lr: 5.0000e-06\n",
      "Epoch 90/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.8756\n",
      "Epoch 90: accuracy improved from 0.87418 to 0.87556, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2322 - accuracy: 0.8756 - val_loss: 0.2181 - val_accuracy: 0.8689 - lr: 5.0000e-06\n",
      "Epoch 91/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.8773\n",
      "Epoch 91: accuracy improved from 0.87556 to 0.87726, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2344 - accuracy: 0.8773 - val_loss: 0.2167 - val_accuracy: 0.8689 - lr: 5.0000e-06\n",
      "Epoch 92/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.8684\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-07.\n",
      "\n",
      "Epoch 92: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2375 - accuracy: 0.8684 - val_loss: 0.2041 - val_accuracy: 0.8749 - lr: 5.0000e-06\n",
      "Epoch 93/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.8762\n",
      "Epoch 93: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2339 - accuracy: 0.8762 - val_loss: 0.1984 - val_accuracy: 0.8834 - lr: 5.0000e-07\n",
      "Epoch 94/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.8674\n",
      "Epoch 94: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2365 - accuracy: 0.8674 - val_loss: 0.1977 - val_accuracy: 0.8809 - lr: 5.0000e-07\n",
      "Epoch 95/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.8727\n",
      "Epoch 95: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2340 - accuracy: 0.8727 - val_loss: 0.2048 - val_accuracy: 0.8843 - lr: 5.0000e-07\n",
      "Epoch 96/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2320 - accuracy: 0.8724\n",
      "Epoch 96: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2320 - accuracy: 0.8724 - val_loss: 0.1878 - val_accuracy: 0.8843 - lr: 5.0000e-07\n",
      "Epoch 97/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.8694\n",
      "Epoch 97: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2328 - accuracy: 0.8694 - val_loss: 0.2054 - val_accuracy: 0.8817 - lr: 5.0000e-07\n",
      "Epoch 98/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.8744\n",
      "Epoch 98: accuracy did not improve from 0.87726\n",
      "294/294 [==============================] - 59s 182ms/step - loss: 0.2285 - accuracy: 0.8744 - val_loss: 0.2158 - val_accuracy: 0.8791 - lr: 5.0000e-07\n",
      "Epoch 99/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.8796\n",
      "Epoch 99: accuracy improved from 0.87726 to 0.87960, saving model to checkpoints\\resnet_32class_1024_units_6_checkpoint.h5\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2267 - accuracy: 0.8796 - val_loss: 0.1955 - val_accuracy: 0.8919 - lr: 5.0000e-07\n",
      "Epoch 100/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.8752\n",
      "Epoch 100: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 59s 183ms/step - loss: 0.2267 - accuracy: 0.8752 - val_loss: 0.1908 - val_accuracy: 0.8860 - lr: 5.0000e-07\n",
      "Epoch 101/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.8689\n",
      "Epoch 101: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2333 - accuracy: 0.8689 - val_loss: 0.2085 - val_accuracy: 0.8851 - lr: 5.0000e-07\n",
      "Epoch 102/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.8726\n",
      "Epoch 102: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2299 - accuracy: 0.8726 - val_loss: 0.1885 - val_accuracy: 0.8877 - lr: 5.0000e-07\n",
      "Epoch 103/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.8698\n",
      "Epoch 103: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2349 - accuracy: 0.8698 - val_loss: 0.1998 - val_accuracy: 0.8851 - lr: 5.0000e-07\n",
      "Epoch 104/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.8743\n",
      "Epoch 104: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2237 - accuracy: 0.8743 - val_loss: 0.1895 - val_accuracy: 0.8902 - lr: 5.0000e-07\n",
      "Epoch 105/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.8748\n",
      "Epoch 105: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2255 - accuracy: 0.8748 - val_loss: 0.1786 - val_accuracy: 0.8817 - lr: 5.0000e-07\n",
      "Epoch 106/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2238 - accuracy: 0.8747\n",
      "Epoch 106: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2238 - accuracy: 0.8747 - val_loss: 0.1787 - val_accuracy: 0.8928 - lr: 5.0000e-07\n",
      "Epoch 107/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.8772\n",
      "Epoch 107: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2237 - accuracy: 0.8772 - val_loss: 0.2197 - val_accuracy: 0.8809 - lr: 5.0000e-07\n",
      "Epoch 108/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.8757\n",
      "Epoch 108: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2257 - accuracy: 0.8757 - val_loss: 0.1988 - val_accuracy: 0.8766 - lr: 5.0000e-07\n",
      "Epoch 109/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.8760\n",
      "Epoch 109: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2295 - accuracy: 0.8760 - val_loss: 0.1814 - val_accuracy: 0.8962 - lr: 5.0000e-07\n",
      "Epoch 110/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.8768\n",
      "Epoch 110: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2226 - accuracy: 0.8768 - val_loss: 0.2116 - val_accuracy: 0.8826 - lr: 5.0000e-07\n",
      "Epoch 111/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.8765\n",
      "Epoch 111: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2255 - accuracy: 0.8765 - val_loss: 0.1896 - val_accuracy: 0.8877 - lr: 5.0000e-07\n",
      "Epoch 112/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.8758\n",
      "Epoch 112: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2226 - accuracy: 0.8758 - val_loss: 0.1881 - val_accuracy: 0.8800 - lr: 5.0000e-07\n",
      "Epoch 113/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.8679\n",
      "Epoch 113: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2268 - accuracy: 0.8679 - val_loss: 0.1917 - val_accuracy: 0.8774 - lr: 5.0000e-07\n",
      "Epoch 114/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.8776\n",
      "Epoch 114: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2185 - accuracy: 0.8776 - val_loss: 0.2157 - val_accuracy: 0.8732 - lr: 5.0000e-07\n",
      "Epoch 115/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.8739\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-08.\n",
      "\n",
      "Epoch 115: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2272 - accuracy: 0.8739 - val_loss: 0.1837 - val_accuracy: 0.8800 - lr: 5.0000e-07\n",
      "Epoch 116/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.8759\n",
      "Epoch 116: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2228 - accuracy: 0.8759 - val_loss: 0.2154 - val_accuracy: 0.8596 - lr: 5.0000e-08\n",
      "Epoch 117/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.8761\n",
      "Epoch 117: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 186ms/step - loss: 0.2267 - accuracy: 0.8761 - val_loss: 0.2137 - val_accuracy: 0.8698 - lr: 5.0000e-08\n",
      "Epoch 118/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.8709\n",
      "Epoch 118: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 185ms/step - loss: 0.2284 - accuracy: 0.8709 - val_loss: 0.1988 - val_accuracy: 0.8868 - lr: 5.0000e-08\n",
      "Epoch 119/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.8766\n",
      "Epoch 119: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 184ms/step - loss: 0.2224 - accuracy: 0.8766 - val_loss: 0.1908 - val_accuracy: 0.8809 - lr: 5.0000e-08\n",
      "Epoch 120/200\n",
      "294/294 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.8730\n",
      "Epoch 120: accuracy did not improve from 0.87960\n",
      "294/294 [==============================] - 60s 183ms/step - loss: 0.2279 - accuracy: 0.8730 - val_loss: 0.1796 - val_accuracy: 0.8894 - lr: 5.0000e-08\n",
      "Epoch 120: early stopping\n",
      "Total Training Time: 7173.508592367172 seconds\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000050, clipnorm=1.0) #0.000025 0.00001\n",
    "\n",
    "model = None\n",
    "vanilla_model = False\n",
    "INNER_LY = 6\n",
    "EXTRACTOR_MODEL = 'resnet' #vgg inception resnet\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "\n",
    "if vanilla_model:\n",
    "  model = create_model(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor='vgg',\n",
    "    units=UNITS\n",
    "  )\n",
    "else:\n",
    "  model = AnimeClassifier(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor=EXTRACTOR_MODEL,\n",
    "    units=UNITS,\n",
    "    inner_layers=INNER_LY\n",
    "  )\n",
    "  model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "train(\n",
    "  model=model,\n",
    "  epochs=200,\n",
    "  units=UNITS,\n",
    "  val_ds=valid_ds.batch(32),\n",
    "  train_ds=train_ds.batch(32),\n",
    "  save_weights_only=False if vanilla_model else True,\n",
    "  mode='fit', type_model=EXTRACTOR_MODEL, inner_ly=INNER_LY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVA_INNER = 3\n",
    "EVA_CLASS = 32\n",
    "EVA_UNITS = 1024\n",
    "EVA_TYPE  = 'resnet'\n",
    "EVA_MODEL_CLASS = 32\n",
    "MAX_VEC_LEN = 1024 * 6\n",
    "#Shape of vector result: (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_array_eva = pkl.load(open( f'./data/class_array_{EVA_CLASS}.pkl', 'rb'))\n",
    "\n",
    "parmas_eval = {\n",
    "  'num_classes':  EVA_MODEL_CLASS,\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': EVA_TYPE,\n",
    "  'units': EVA_UNITS,\n",
    "  'inner_layers': EVA_INNER\n",
    "}\n",
    "model_eva = AnimeClassifier(**parmas_eval)\n",
    "model_eva.build(input_shape=(None, *parmas_eval['input_shape']))\n",
    "PATH_BEST_EVA = f'./models/{EVA_TYPE}_{EVA_MODEL_CLASS}class_{EVA_UNITS}_units_{EVA_INNER}.h5'\n",
    "model_eva.load_weights(PATH_BEST_EVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_PATH_EVA = f'./data/anime_data_{EVA_CLASS}.tfrecord'\n",
    "tf_record_eva = load_tfrecord_dataset(TFRECORD_PATH_EVA, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_eva)\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_eva = tf_record_eva.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_eva = tf_record_eva.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_eva = tf_record_eva.batch(32)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for images, label in tf_record_eva:\n",
    "  preds = model_eva.predict(images)\n",
    "  all_labels.extend(label)\n",
    "  all_preds.extend(np.argmax(preds, axis=1))\n",
    "del tf_record_eva\n",
    "\n",
    "confusion_matrix= tf.math.confusion_matrix(all_labels, all_preds, num_classes=EVA_CLASS)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title(f'Confusion matrix - {EVA_CLASS} classes - {EVA_TYPE} - {EVA_UNITS} units - {EVA_INNER} inner layers')\n",
    "sns.heatmap(confusion_matrix.numpy(), annot=True, cmap='Blues')\n",
    "plt.savefig(f'./metrics/confusion_matrix_{EVA_CLASS}_{EVA_TYPE}_{EVA_UNITS}_{EVA_INNER}_inner.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_cpu(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def cosine_similarity_cpum(u, v):\n",
    "  u_dot_v = np.sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = np.sqrt(np.sum(u*u))\n",
    "  mod_v = np.sqrt(np.sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tf(a, b):\n",
    "  return tf.tensordot(a, b, axes=1) / (tf.norm(a) * tf.norm(b))\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tfm(u, v):\n",
    "  u_dot_v = tf.reduce_sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = tf.sqrt(tf.reduce_sum(u*u))\n",
    "  mod_v = tf.sqrt(tf.reduce_sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@numba.guvectorize([\"void(float64[:], float64[:], float64[:])\"], \"(n),(n)->()\", target='parallel', fastmath =True)\n",
    "def fast_cosine_gufunc(u, v, result):\n",
    "    m = u.shape[0]\n",
    "    udotv = 0\n",
    "    u_norm = 0\n",
    "    v_norm = 0\n",
    "    for i in range(m):\n",
    "        if (np.isnan(u[i])) or (np.isnan(v[i])):\n",
    "            continue\n",
    "\n",
    "        udotv += u[i] * v[i]\n",
    "        u_norm += u[i] * u[i]\n",
    "        v_norm += v[i] * v[i]\n",
    "\n",
    "    u_norm = np.sqrt(u_norm)\n",
    "    v_norm = np.sqrt(v_norm)\n",
    "\n",
    "    if (u_norm == 0) or (v_norm == 0):\n",
    "        ratio = 1.0\n",
    "    else:\n",
    "        ratio = udotv / (u_norm * v_norm)\n",
    "    result[:] = ratio\n",
    "\n",
    "@numba.jit(nopython=False, parallel=True)\n",
    "def cosine_similarity_numba(u, v):\n",
    "  uv = 0\n",
    "  uu = 0\n",
    "  vv = 0\n",
    "  \n",
    "  for i in range(u.shape[0]):\n",
    "    uv += u[i]*v[i]\n",
    "    uu += u[i]*u[i]\n",
    "    vv += v[i]*v[i]\n",
    "  cos_theta = 1\n",
    "  \n",
    "  if uu != 0 and vv != 0:\n",
    "    cos_theta = uv / np.sqrt(uu*vv)\n",
    "  return cos_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_vec(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'class_name': tf.io.FixedLenFeature([], tf.string),\n",
    "  })\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  y_train = x['class_name']\n",
    "  if y_train is None:\n",
    "    y_train = ''\n",
    "\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset_vec(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord_vec(x, size))\n",
    "\n",
    "def parse_record_vec(combination):\n",
    "  item_1, item_2 = combination\n",
    "  img_1, label_1 = item_1\n",
    "  img_2, label_2 = item_2\n",
    "  return (img_1, img_2, label_1 == label_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "\n",
    "n_train = int(all_ds_len * 0.95)\n",
    "n_test = int(all_ds_len * 0.05)\n",
    "\n",
    "tf_record_vec = tf_record_vec.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_vec = tf_record_vec.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "\n",
    "all_combinations = list(itertools.combinations(tf_record_vec, 2))\n",
    "\n",
    "all_combinations = list(map(parse_record_vec, all_combinations))\n",
    "rd.shuffle(all_combinations)\n",
    "del tf_record_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(combinations, model):\n",
    "  result = []\n",
    "  for idx, item in enumerate(combinations):\n",
    "    img_1, img_2, label = item\n",
    "    pred_1, pred_2 = model.vectorize(np.array([img_1, img_2]))\n",
    "    #cos_sim = cosine_similarity_cpu(pred_1, pred_2)\n",
    "    pred_1 = np.array(pred_1)\n",
    "    pred_2 = np.array(pred_2)\n",
    "    cos_sim = cosine_similarity_numba(pred_1, pred_2)\n",
    "    result.append((cos_sim, label))\n",
    "    #print(f'{idx + 1}/{len(combinations)} - {round(((idx + 1) / len(combinations)) * 100, 2)}%')\n",
    "  return result\n",
    "\n",
    "result = calculate_cosine_similarity(all_combinations[:MAX_VEC_LEN], model_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.70625, 'recall': 0.601063829787234, 'f1': 0.6494252873563219, 'threshold': 0.6}\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['cos_sim', 'label']\n",
    "df.label = df.label.astype(bool)\n",
    "\n",
    "current_f1 = -1\n",
    "metrics = {}\n",
    "for threshold in range(0, 100, 1):\n",
    "  pred_label = df.apply(lambda x: True if x.cos_sim > (threshold/100) else False, axis=1)\n",
    "  precision_result = sk_metrics.precision_score(df.label, pred_label) # Impact when the model predict to lot False positives\n",
    "  recall_result = sk_metrics.recall_score(df.label, pred_label) # Impact when the model predict to lot False negatives\n",
    "  f1_result = sk_metrics.f1_score(df.label, pred_label) # Impact when the model predict to lot False positives and False negatives\n",
    "  if f1_result > current_f1:\n",
    "    current_f1 = f1_result\n",
    "    metrics['precision'] = precision_result\n",
    "    metrics['recall'] = recall_result\n",
    "    metrics['f1'] = f1_result\n",
    "    metrics['threshold'] = threshold / 100\n",
    "\n",
    "print(metrics)\n",
    "json.dump(metrics, open(f'./metrics/metrics_{EVA_CLASS}_{EVA_TYPE}_{EVA_UNITS}_{EVA_INNER}_inner.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embeddings with Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "tf_record_vec = tf_record_vec.shuffle(all_ds_len, seed=SEED)\n",
    "tf_record_vec = tf_record_vec.take(MAX_VEC_LEN)\n",
    "del all_ds_len\n",
    "\n",
    "data_image = []\n",
    "for idx, item in enumerate(tf_record_vec):\n",
    "  img, class_name = item\n",
    "  pred_vec = model_eva.vectorize(np.array([img]))[0]\n",
    "  data_item = (\n",
    "    pred_vec.numpy().astype('float32'),\n",
    "    class_name.numpy().decode('utf-8')\n",
    "  )\n",
    "  data_image.append(data_item)\n",
    "  print(f'{idx + 1}/{MAX_VEC_LEN} - {round(((idx + 1) / MAX_VEC_LEN) * 100, 2)}%')\n",
    "\n",
    "pkl.dump(np.array(data_image), open(f'./data/data_image.pkl', 'wb'))\n",
    "del data_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors 6144\n"
     ]
    }
   ],
   "source": [
    "data_image = pkl.load(open(f'./data/data_image.pkl', 'rb'))\n",
    "vector_images = np.array(list(data_image[:, 0])) \n",
    "\n",
    "d = 2048 #Shape of vector result: (2048,)\n",
    "nb = MAX_VEC_LEN\n",
    "res = faiss.StandardGpuResources()  # use a single GPU\n",
    "\n",
    "# build a flat (CPU) index\n",
    "index_flat = faiss.IndexFlatL2(d)\n",
    "# make it into a gpu index\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index_flat.add(vector_images)\n",
    "print(f'Vectors {gpu_index_flat.ntotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'blood': 84, 'ice': 9, 'detective_conan': 1, 'naruto': 3, 'gintama': 2, 'one_piece': 1}\n",
      "Only name: blood, ice, detective_conan, naruto, gintama, one_piece\n"
     ]
    }
   ],
   "source": [
    "k = 100 # we want to see 4 nearest neighbors\n",
    "dimension_vec, ids_result = gpu_index_flat.search(vector_numpy[2:3], k) # actual search\n",
    "\n",
    "current_id = 1\n",
    "result = {}\n",
    "for idx, id_v in enumerate(ids_result[0]):\n",
    "  class_name = data_image[id_v][1]\n",
    "  if class_name not in result:\n",
    "    result[class_name] = 0\n",
    "  result[class_name] += 1\n",
    "\n",
    "print(f'Result: {result}')\n",
    "print(f'Only name: {\", \".join(list(result.keys()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalaute time TF vs Numba vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "results = model_16.predict(images)\n",
    "iterations = 1000\n",
    "\n",
    "cpu_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  cpu_r.append(cosine_similarity_cpum(tg_vector, results))\n",
    "  #for reuslt in results:\n",
    "  #  cpu_r.append(cosine_similarity_cpu(reuslt, reuslt))\n",
    "end = time.time()\n",
    "print(f'Time to compute on CPU: {end - start}')\n",
    "\n",
    "numba_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  numba_r.append(fast_cosine_gufunc(results, tg_vector))\n",
    "end = time.time()\n",
    "print(f'Time to compute on Numba: {end - start}')\n",
    "\n",
    "images_gpu = [tf.convert_to_tensor(result) for result in results]\n",
    "tf_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = images_gpu[0]\n",
    "  tf_r.append(cosine_similarity_tfm(tg_vector, results))\n",
    "  #for reuslt in images_gpu:\n",
    "  #  tf_r.append(cosine_similarity_tf(result, images_gpu[0]))\n",
    "end = time.time()\n",
    "print(f'Time to compute on TF: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeClassifier(\n",
    "  num_classes=len(class_array),\n",
    "  input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "  type_extractor='vgg',\n",
    "  units=UNITS,\n",
    "  inner_layers=1\n",
    ")\n",
    "model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "PATH_BEST = './models/vgg_16class_1024_units_aqr.h5'\n",
    "model.load_weights(PATH_BEST)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vector and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vectors: (2048,)\n"
     ]
    }
   ],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "db1_v, db2_v, nr1_v = model_eva.vectorize(images)\n",
    "print(f'Shape of vectors: {db1_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28875878120702914"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_numba(np.array(db1_v), np.array(nr1_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6586562579012494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_numba(np.array(db1_v), np.array(db2_v))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "852bc408046ca7dfc5c8f91ce764d8630d2287ca09c7fe9d1b4d9cd156705bcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
