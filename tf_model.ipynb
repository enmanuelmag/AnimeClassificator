{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import numba\n",
    "import faiss\n",
    "import hashlib\n",
    "import shutil\n",
    "import warnings\n",
    "import requests\n",
    "import importlib\n",
    "import itertools\n",
    "import visualkeras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import tf_functions\n",
    "import pickle as pkl\n",
    "import transformer_v2\n",
    "import transformer_v3\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "from numba import vectorize\n",
    "from alive_progress import alive_bar\n",
    "from notifier import Notifier, notify\n",
    "from sklearn import metrics as sk_metrics\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from PIL import Image\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "importlib.reload(transformer_v2)\n",
    "VitModel = transformer_v2.VitModel\n",
    "\n",
    "GeneratePatch = tf_functions.GeneratePatch\n",
    "AnimeClassifier = tf_functions.AnimeClassifier\n",
    "ReleaseMemory = tf_functions.ReleaseMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "MAX_CLASS = 32\n",
    "USE_FACE = False\n",
    "WEIGHT_DECAY = 0.0001\n",
    "SIZE_IMG = 128 # 88_ //RestNet 128 240 224 416\n",
    "\n",
    "DATASET_PATH = './data/animes'\n",
    "DATASET_FACES_PATH = './data/faces'\n",
    "DATASET_FACE_FOLDER = './data/moeimouto-faces'\n",
    "CLASS_ARRAY_PATH = f'./data/class_array_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_FACES_PATH = f'./data/class_array_faces_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_VEC_PATH = f'./data/class_array_vec_{MAX_CLASS}.pkl'\n",
    "DATASET_JSON_PATH = './data/anime_data.json'\n",
    "AMOUNT_TABLE_PATH = './data/anime_amount.pkl'\n",
    "AMOUNT_FACES_TABLE_PATH = './data/faces_amount.pkl'\n",
    "DATASET_JSON_RANK = './data/anime_rank.json'\n",
    "TFRECORD_PATH = f'./data/anime_data_{MAX_CLASS}.tfrecord'\n",
    "TFRECORD_FACES_PATH = f'./data/anime_faces_data_{MAX_CLASS}.tfrecord'\n",
    "\n",
    "#seed random seed to 42 for reproducibility\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "if USE_FACE:\n",
    "  #DATASET_PATH = DATASET_FACES_PATH\n",
    "  CLASS_ARRAY_PATH = CLASS_ARRAY_FACES_PATH\n",
    "  TFRECORD_PATH = TFRECORD_FACES_PATH\n",
    "  AMOUNT_TABLE_PATH = AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_image(url, anime_name, idx):\n",
    "  #download image from url\n",
    "  file_path = f'./data/animes/{anime_name}____{idx}.jpg' \n",
    "  if os.path.exists(file_path):\n",
    "    return\n",
    "\n",
    "  img_data = requests.get(url).content\n",
    "  with open(file_path, 'wb') as handler:\n",
    "    handler.write(img_data)\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Anime images',\n",
    "  msg='Finished downloading anime images'\n",
    ")\n",
    "def get_images(data):\n",
    "  #with alive_bar(len(data)) as bar:\n",
    "  for idx_a, anime_name in enumerate(data):\n",
    "    urls = data[anime_name]\n",
    "    for idx, url in enumerate(urls):\n",
    "      if idx >= 400:\n",
    "        break\n",
    "      name_clean = re.sub(r'_+', r'_', re.sub(r'[\\W\\s]', r'_', anime_name))\n",
    "      try:\n",
    "        dowload_image(url['image'], name_clean, idx)\n",
    "      except Exception as e:\n",
    "        print(f'Error on download image {idx + 1} of {anime_name}')\n",
    "        pass\n",
    "    #bar()\n",
    "    print(f'Progress: {idx_a + 1}/{len(data)} - {round((idx_a + 1)/len(data)*100, 2)}%')\n",
    "\n",
    "def get_classes_anime(path):\n",
    "  classes = set()\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    classes.add(class_name)\n",
    "  return list(classes)\n",
    "\n",
    "def wait_for_it(driver, xpath, timeout=3):\n",
    "  try:\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "def iter_post(driver):\n",
    "  anime_data = []\n",
    "\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = True\n",
    "\n",
    "  while next_button is not None:\n",
    "    if len(anime_data) > 400:\n",
    "      break\n",
    "    ul_element = wait_for_it(driver, '//ul[@id=\"post-list-posts\"]')\n",
    "    if ul_element is None:\n",
    "      next_button = wait_for_it(driver, xpath_next)\n",
    "      if next_button is not None:\n",
    "        next_button.click()\n",
    "        time.sleep(1)\n",
    "      continue\n",
    "    for i, li_element in enumerate(ul_element.find_elements(By.TAG_NAME, 'li')):\n",
    "      a_video = li_element.find_element(By.XPATH, './a').get_attribute('href')\n",
    "      a_image = li_element.find_element(By.XPATH, './div/a/img').get_attribute('src')\n",
    "      anime_data.append({\n",
    "        'video': a_video,\n",
    "        'image': a_image\n",
    "      })\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "    if next_button is not None:\n",
    "      next_button.click()\n",
    "      time.sleep(rd.randint(1, 2))\n",
    "  return anime_data\n",
    "\n",
    "def get_images_links(url, driver, anime_name):\n",
    "  url_search = url + anime_name\n",
    "  driver.get(url_search)\n",
    "  return iter_post(driver)\n",
    "\n",
    "def get_names(driver):\n",
    "  names = []\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = wait_for_it(driver, xpath_next)\n",
    "  \n",
    "  while next_button is not None:\n",
    "    for tr_element in driver.find_elements(By.XPATH, '//table[@class=\"highlightable\"]/tbody/tr'):\n",
    "      try:\n",
    "        amount_post = tr_element.find_element(By.XPATH, './td[1]').text\n",
    "        amount_post = int(amount_post)\n",
    "        if amount_post >= 10:\n",
    "          a_name = tr_element.find_element(By.XPATH, './td[2]/a[2]' ).text\n",
    "          names.append(a_name)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    next_button.click()\n",
    "    time.sleep(rd.randint(1, 2))\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "  return names\n",
    "\n",
    "def get_score(anime_name, driver):\n",
    "  url_search = f'https://myanimelist.net/anime.php?cat=anime&q={anime_name}'\n",
    "  driver.get(url_search)\n",
    "  score = 0\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    score += 1\n",
    "  return score\n",
    "\n",
    "def relevant_anime(anime_name, df_anime, amount_table, threshold=350, rank=True):\n",
    "  \n",
    "  if amount_table.get(anime_name, 0) <= threshold:\n",
    "    return False\n",
    "\n",
    "  if not rank:\n",
    "    return True\n",
    "\n",
    "  anime_name = re.sub(r'_', r' ', anime_name)\n",
    "  df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "\n",
    "  if df_result.empty:\n",
    "    anime_name = ' '.join(anime_name.split(' ')[:3])\n",
    "    df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "  return not df_result.empty\n",
    "\n",
    "def amount_anime_table(datapath):\n",
    "  dic = {}\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    dic[class_name] = dic.get(class_name, 0) + 1\n",
    "  return dic\n",
    "\n",
    "def detect(filename, cascade_file):\n",
    "  if not os.path.isfile(cascade_file):\n",
    "    raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "  cascade = cv2.CascadeClassifier(cascade_file)\n",
    "  image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "  #src = cv2.cuda_GpuMat()\n",
    "  #src.upload(image)\n",
    "\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray = cv2.equalizeHist(gray)\n",
    "\n",
    "  faces = cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor = 1.1,\n",
    "    minNeighbors = 5,\n",
    "    minSize = (24, 24)\n",
    "  )\n",
    "\n",
    "  new_images = []\n",
    "  for (x, y, w, h) in faces:\n",
    "    new_images.append(image[y:y+h, x:x+w])\n",
    "  #clahe = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "  #dst = clahe.apply(src, cv2.cuda_Stream.Null())\n",
    "  #result = dst.download()\n",
    "  return new_images\n",
    "\n",
    "def extract_faces(datapath):\n",
    "  faces_amount = 0\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    new_images = []\n",
    "    try:\n",
    "      new_images = detect(datapath + '/' + filename, './data/haar/lbpcascade_animeface.xml')\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      pass\n",
    "    if len(new_images) > 0:\n",
    "      for idx, img in enumerate(new_images):\n",
    "        new_face_name = f'./data/faces/{class_name}____{idx}.jpg'\n",
    "        try:\n",
    "          if not os.path.exists(new_face_name):\n",
    "            cv2.imwrite(new_face_name, img)\n",
    "            faces_amount += 1\n",
    "        except:\n",
    "          pass\n",
    "  print(f'Faces amount: {faces_amount}')\n",
    "\n",
    "def parse_face_dataset(face_path, out_path):\n",
    "  for char_name in os.listdir(face_path):\n",
    "    if char_name == '.DS_Store':\n",
    "      continue\n",
    "    for idx, filename in enumerate(os.listdir(face_path + '/' + char_name)):\n",
    "      if filename == '.DS_Store':\n",
    "        continue\n",
    "      ext = os.path.splitext(filename)[1]\n",
    "      if ext == '.csv':\n",
    "        continue\n",
    "      clean_char_name = char_name.split('_')[1]\n",
    "      new_filename = f'{clean_char_name}____{idx}.{ext}'\n",
    "      shutil.copy(face_path + '/' + char_name + '/' + filename, out_path + '/' + new_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anime images\n",
    "anime_data = json.load(open(DATASET_JSON_PATH))\n",
    "get_images(anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only faces with OpenCV\n",
    "extract_faces(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse dataset faces\n",
    "parse_face_dataset(DATASET_FACE_FOLDER, DATASET_FACES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FACES Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_FACES_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_anime_rank.pkl')\n",
    "\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "\n",
    "class_array = set()\n",
    "for anime_name in all_class_array:\n",
    "  if relevant_anime(anime_name, df, amount_table, threshold=100, rank=True):\n",
    "    class_array.add((anime_name, amount_table[anime_name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "class_array = [x[0] for x in class_array]\n",
    "\n",
    "mean_keyframes = np.mean([amount_table[x] for x in class_array])\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)} - Mean keyframes: {int(mean_keyframes)}')\n",
    "#pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))\n",
    "del all_class_array\n",
    "del mean_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for faces\n",
    "all_class_array = get_classes_anime(DATASET_FACES_PATH)\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "\n",
    "class_array = set()\n",
    "for name in all_class_array:\n",
    "  class_array.add((name, amount_table[name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "class_array = [x[0] for x in class_array]\n",
    "\n",
    "mean_keyframes = np.mean([amount_table[x] for x in class_array])\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)} - Mean keyframes: {int(mean_keyframes)}')\n",
    "#pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))\n",
    "del all_class_array\n",
    "del mean_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLASS_ARRAY_PATH)\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "class_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(class_name):\n",
    "  return class_array.index(class_name)\n",
    "\n",
    "def build_example(path_file, class_name):\n",
    "  img_array = open(path_file, 'rb').read()\n",
    "  \n",
    "  #img = load_img(path_file, target_size=(SIZE_IMG, SIZE_IMG))\n",
    "  #img_array = np.array(img)\n",
    "  #img_array = preprocess_input(img_array, mode='tf')\n",
    "  #key = hashlib.sha256(img_array).hexdigest()\n",
    "  example = tf.train.Example(\n",
    "    features=tf.train.Features(feature={\n",
    "    #'key': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf-8')])),\n",
    "    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array])),\n",
    "    #'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array.tobytes()])),\n",
    "    'class_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[get_class_id(class_name)])),\n",
    "    'class_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[class_name.encode('utf-8')])),\n",
    "    'filepath': tf.train.Feature(bytes_list=tf.train.BytesList(value=[path_file.encode('utf-8')]))\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "def create_tfrecord(data_path, withe_list, path_tfrecord):\n",
    "  files = os.listdir(data_path)\n",
    "  writer = tf.io.TFRecordWriter(path_tfrecord)\n",
    "  \n",
    "  print('Started creating tfrecord')\n",
    "  for idx, filename in enumerate(files):\n",
    "    class_name, _ = filename.split('____')\n",
    "  \n",
    "    if class_name in withe_list:\n",
    "      path_file = os.path.join(data_path, filename)\n",
    "      tf_example = build_example(path_file, class_name)\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "  print('Finished creating tfrecord')\n",
    "  writer.close()\n",
    "\n",
    "def parse_tfrecord(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  #class_id = tf.sparse.to_dense(x['class_id'], default_value=-1)\n",
    "  class_id = x['class_id']\n",
    "  if class_id is None:\n",
    "    class_id = -1\n",
    "  labels = tf.cast(class_id, tf.int64)\n",
    "  \n",
    "  #labels = []\n",
    "  #for i in range(len(class_array)):\n",
    "  #  labels.append(1 if i == class_id else 0)\n",
    "\n",
    "  y_train = labels\n",
    "  #y_train = tf.stack([ labels ], axis=1)\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, size, use_cache=False, use_file=True):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  dataset = dataset.map(lambda x: parse_tfrecord(x, size))\n",
    "  if use_file and use_cache:\n",
    "    dataset = dataset.cache(f'./cache/tf_record.cache')\n",
    "  elif use_cache:\n",
    "    dataset = dataset.cache()\n",
    "  return dataset\n",
    "\n",
    "def create_model(num_classes, input_shape, units, type_extractor = 'vgg') -> tf.keras.Model:\n",
    "  if type_extractor == 'vgg':\n",
    "    feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'inception':\n",
    "    feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'resnet':\n",
    "    feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  else:\n",
    "    raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(tf.keras.layers.Input(input_shape, name='input'))\n",
    "  model.add(feature_extractor)\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  #new\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "  return model\n",
    "\n",
    "def render_image_and_patches(image, patches, patch_size):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.imshow(tf.cast(image[0], tf.uint8))\n",
    "  #plt.xlabel(class_types [np.argmax(train_iter_7label)], fontsize=13)\n",
    "  n = int(np.sqrt(patches.shape[1]))\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  #plt.suptitle(f\"Image Patches\", size=13)\n",
    "  for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i+1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    ax.axis('off') \n",
    "\n",
    "def build_model(model_params, extractor_model='vgg', input_shape=(None, SIZE_IMG, SIZE_IMG, 3)):\n",
    "  model = None\n",
    "\n",
    "  if extractor_model in ['vgg', 'resnet', 'inception']:\n",
    "    model = AnimeClassifier(**model_params)\n",
    "  else:\n",
    "    model = VitModel(**model_params)\n",
    "  model.build(input_shape)\n",
    "\n",
    "  prefix_data = f'{extractor_model}_{SIZE_IMG}'\n",
    "  params_str = '_'.join(map(str, model_params.values()))\n",
    "  name = f'{prefix_data}_{len(class_array)}_{params_str}'\n",
    "  check_name = f'checkpoints/{name}_checkpoint.h5'\n",
    "\n",
    "  return model, name, check_name\n",
    "\n",
    "def train(model, train_ds, val_ds, check_name, initial_epoch, epochs=15, mode='fit', type_model='vgg'):\n",
    "  logdir = \"logs/\"+ type_model + \"/\" + time.strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "  if mode == 'eager_tf':\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "      for batch, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(images, training=True)\n",
    "          regularization_loss = tf.reduce_sum(model.losses)\n",
    "          pred_loss = []\n",
    "          for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "          total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"{}_train_{}, {}, {}\".format(\n",
    "          epoch, batch, total_loss.numpy(),\n",
    "          list(map(lambda x: np.sum(x.numpy()), pred_loss))\n",
    "        ))\n",
    "        avg_loss.update_state(total_loss)\n",
    "  elif mode == 'fit':\n",
    "    callbacks = [\n",
    "      #ReleaseMemory(),\n",
    "      ReduceLROnPlateau(verbose=1),\n",
    "      #EarlyStopping(patience=8, verbose=1),\n",
    "      ModelCheckpoint(\n",
    "        check_name, \n",
    "        verbose=1,\n",
    "        monitor='accuracy',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "      ),\n",
    "      TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_data=val_ds,\n",
    "      initial_epoch=initial_epoch\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Total Training Time: {end_time} seconds')\n",
    "\n",
    "\n",
    "IMAGE_FEATURE_MAP = {\n",
    "  'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  'class_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "if os.path.exists(TFRECORD_PATH):\n",
    "  os.remove(TFRECORD_PATH)\n",
    "create_tfrecord(\n",
    "  DATASET_FACES_PATH if USE_FACE else DATASET_PATH,\n",
    "  class_array,\n",
    "  TFRECORD_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 11753\n"
     ]
    }
   ],
   "source": [
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "tf_record = load_tfrecord_dataset(TFRECORD_PATH, SIZE_IMG, False)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record)\n",
    "factor_mini = 1\n",
    "\n",
    "tf_record = tf_record.shuffle(all_ds_len, seed=SEED)\n",
    "all_ds_len = int(all_ds_len * factor_mini)\n",
    "print(f'Total number of images: {all_ds_len}')\n",
    "tf_record = tf_record.take(all_ds_len)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_valid = int(all_ds_len * 0.1)\n",
    "n_test = all_ds_len - n_train - n_valid\n",
    "\n",
    "train_ds = tf_record.take(n_train)\n",
    "valid_ds = tf_record.skip(n_train).take(n_valid)\n",
    "test_ds = tf_record.skip(n_train + n_valid).take(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "    147/Unknown - 64s 217ms/step - loss: 4.0953 - accuracy: 0.0386 - top-5-accuracy: 0.1842\n",
      "Epoch 1: accuracy improved from -inf to 0.03861, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 74s 287ms/step - loss: 4.0953 - accuracy: 0.0386 - top-5-accuracy: 0.1842 - val_loss: 3.4425 - val_accuracy: 0.0391 - val_top-5-accuracy: 0.2187 - lr: 5.0000e-05\n",
      "Epoch 2/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4882 - accuracy: 0.0395 - top-5-accuracy: 0.1843\n",
      "Epoch 2: accuracy improved from 0.03861 to 0.03946, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 39s 256ms/step - loss: 3.4882 - accuracy: 0.0395 - top-5-accuracy: 0.1843 - val_loss: 3.4400 - val_accuracy: 0.0494 - val_top-5-accuracy: 0.2043 - lr: 5.0000e-05\n",
      "Epoch 3/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4622 - accuracy: 0.0404 - top-5-accuracy: 0.1856\n",
      "Epoch 3: accuracy improved from 0.03946 to 0.04042, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 39s 253ms/step - loss: 3.4622 - accuracy: 0.0404 - top-5-accuracy: 0.1856 - val_loss: 3.4273 - val_accuracy: 0.0383 - val_top-5-accuracy: 0.2221 - lr: 5.0000e-05\n",
      "Epoch 4/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4446 - accuracy: 0.0456 - top-5-accuracy: 0.1997\n",
      "Epoch 4: accuracy improved from 0.04042 to 0.04563, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 39s 257ms/step - loss: 3.4446 - accuracy: 0.0456 - top-5-accuracy: 0.1997 - val_loss: 3.4056 - val_accuracy: 0.0460 - val_top-5-accuracy: 0.2170 - lr: 5.0000e-05\n",
      "Epoch 5/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4319 - accuracy: 0.0507 - top-5-accuracy: 0.2236\n",
      "Epoch 5: accuracy improved from 0.04563 to 0.05073, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 40s 261ms/step - loss: 3.4319 - accuracy: 0.0507 - top-5-accuracy: 0.2236 - val_loss: 3.3954 - val_accuracy: 0.0570 - val_top-5-accuracy: 0.2221 - lr: 5.0000e-05\n",
      "Epoch 6/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4227 - accuracy: 0.0532 - top-5-accuracy: 0.2276\n",
      "Epoch 6: accuracy improved from 0.05073 to 0.05318, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 41s 265ms/step - loss: 3.4227 - accuracy: 0.0532 - top-5-accuracy: 0.2276 - val_loss: 3.3795 - val_accuracy: 0.0843 - val_top-5-accuracy: 0.2834 - lr: 5.0000e-05\n",
      "Epoch 7/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.4141 - accuracy: 0.0571 - top-5-accuracy: 0.2343\n",
      "Epoch 7: accuracy improved from 0.05318 to 0.05712, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 41s 268ms/step - loss: 3.4141 - accuracy: 0.0571 - top-5-accuracy: 0.2343 - val_loss: 3.3660 - val_accuracy: 0.0860 - val_top-5-accuracy: 0.2851 - lr: 5.0000e-05\n",
      "Epoch 8/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3945 - accuracy: 0.0599 - top-5-accuracy: 0.2471\n",
      "Epoch 8: accuracy improved from 0.05712 to 0.05988, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 42s 270ms/step - loss: 3.3945 - accuracy: 0.0599 - top-5-accuracy: 0.2471 - val_loss: 3.3710 - val_accuracy: 0.0817 - val_top-5-accuracy: 0.2996 - lr: 5.0000e-05\n",
      "Epoch 9/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3718 - accuracy: 0.0687 - top-5-accuracy: 0.2624\n",
      "Epoch 9: accuracy improved from 0.05988 to 0.06871, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 42s 269ms/step - loss: 3.3718 - accuracy: 0.0687 - top-5-accuracy: 0.2624 - val_loss: 3.3166 - val_accuracy: 0.0860 - val_top-5-accuracy: 0.3234 - lr: 5.0000e-05\n",
      "Epoch 10/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3585 - accuracy: 0.0697 - top-5-accuracy: 0.2713\n",
      "Epoch 10: accuracy improved from 0.06871 to 0.06967, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 41s 264ms/step - loss: 3.3585 - accuracy: 0.0697 - top-5-accuracy: 0.2713 - val_loss: 3.2974 - val_accuracy: 0.0868 - val_top-5-accuracy: 0.3285 - lr: 5.0000e-05\n",
      "Epoch 11/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3403 - accuracy: 0.0752 - top-5-accuracy: 0.2813\n",
      "Epoch 11: accuracy improved from 0.06967 to 0.07520, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 41s 265ms/step - loss: 3.3403 - accuracy: 0.0752 - top-5-accuracy: 0.2813 - val_loss: 3.2470 - val_accuracy: 0.1072 - val_top-5-accuracy: 0.3523 - lr: 5.0000e-05\n",
      "Epoch 12/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3302 - accuracy: 0.0729 - top-5-accuracy: 0.2931\n",
      "Epoch 12: accuracy did not improve from 0.07520\n",
      "147/147 [==============================] - 41s 262ms/step - loss: 3.3302 - accuracy: 0.0729 - top-5-accuracy: 0.2931 - val_loss: 3.2286 - val_accuracy: 0.0928 - val_top-5-accuracy: 0.3532 - lr: 5.0000e-05\n",
      "Epoch 13/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3203 - accuracy: 0.0814 - top-5-accuracy: 0.2923\n",
      "Epoch 13: accuracy improved from 0.07520 to 0.08137, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 41s 265ms/step - loss: 3.3203 - accuracy: 0.0814 - top-5-accuracy: 0.2923 - val_loss: 3.2423 - val_accuracy: 0.1055 - val_top-5-accuracy: 0.3643 - lr: 5.0000e-05\n",
      "Epoch 14/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2995 - accuracy: 0.0877 - top-5-accuracy: 0.3061\n",
      "Epoch 14: accuracy improved from 0.08137 to 0.08775, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 42s 267ms/step - loss: 3.2995 - accuracy: 0.0877 - top-5-accuracy: 0.3061 - val_loss: 3.2191 - val_accuracy: 0.1140 - val_top-5-accuracy: 0.3387 - lr: 5.0000e-05\n",
      "Epoch 15/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.3014 - accuracy: 0.0884 - top-5-accuracy: 0.3118\n",
      "Epoch 15: accuracy improved from 0.08775 to 0.08839, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 42s 269ms/step - loss: 3.3014 - accuracy: 0.0884 - top-5-accuracy: 0.3118 - val_loss: 3.1574 - val_accuracy: 0.1268 - val_top-5-accuracy: 0.3796 - lr: 5.0000e-05\n",
      "Epoch 16/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2871 - accuracy: 0.0913 - top-5-accuracy: 0.3158\n",
      "Epoch 16: accuracy improved from 0.08839 to 0.09126, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 43s 272ms/step - loss: 3.2871 - accuracy: 0.0913 - top-5-accuracy: 0.3158 - val_loss: 3.1747 - val_accuracy: 0.1285 - val_top-5-accuracy: 0.3855 - lr: 5.0000e-05\n",
      "Epoch 17/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2750 - accuracy: 0.0922 - top-5-accuracy: 0.3200\n",
      "Epoch 17: accuracy improved from 0.09126 to 0.09221, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 43s 271ms/step - loss: 3.2750 - accuracy: 0.0922 - top-5-accuracy: 0.3200 - val_loss: 3.1126 - val_accuracy: 0.1404 - val_top-5-accuracy: 0.4051 - lr: 5.0000e-05\n",
      "Epoch 18/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2574 - accuracy: 0.1002 - top-5-accuracy: 0.3309\n",
      "Epoch 18: accuracy improved from 0.09221 to 0.10019, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 43s 271ms/step - loss: 3.2574 - accuracy: 0.1002 - top-5-accuracy: 0.3309 - val_loss: 3.1259 - val_accuracy: 0.1174 - val_top-5-accuracy: 0.4111 - lr: 5.0000e-05\n",
      "Epoch 19/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2540 - accuracy: 0.0967 - top-5-accuracy: 0.3333\n",
      "Epoch 19: accuracy did not improve from 0.10019\n",
      "147/147 [==============================] - 43s 272ms/step - loss: 3.2540 - accuracy: 0.0967 - top-5-accuracy: 0.3333 - val_loss: 3.1195 - val_accuracy: 0.1328 - val_top-5-accuracy: 0.3974 - lr: 5.0000e-05\n",
      "Epoch 20/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2359 - accuracy: 0.0998 - top-5-accuracy: 0.3382\n",
      "Epoch 20: accuracy did not improve from 0.10019\n",
      "147/147 [==============================] - 43s 271ms/step - loss: 3.2359 - accuracy: 0.0998 - top-5-accuracy: 0.3382 - val_loss: 3.0995 - val_accuracy: 0.1404 - val_top-5-accuracy: 0.4034 - lr: 5.0000e-05\n",
      "Epoch 21/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2148 - accuracy: 0.1106 - top-5-accuracy: 0.3523\n",
      "Epoch 21: accuracy improved from 0.10019 to 0.11061, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 43s 272ms/step - loss: 3.2148 - accuracy: 0.1106 - top-5-accuracy: 0.3523 - val_loss: 3.0609 - val_accuracy: 0.1294 - val_top-5-accuracy: 0.4400 - lr: 5.0000e-05\n",
      "Epoch 22/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.2081 - accuracy: 0.1073 - top-5-accuracy: 0.3517\n",
      "Epoch 22: accuracy did not improve from 0.11061\n",
      "147/147 [==============================] - 43s 271ms/step - loss: 3.2081 - accuracy: 0.1073 - top-5-accuracy: 0.3517 - val_loss: 3.0529 - val_accuracy: 0.1430 - val_top-5-accuracy: 0.4366 - lr: 5.0000e-05\n",
      "Epoch 23/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1929 - accuracy: 0.1180 - top-5-accuracy: 0.3608\n",
      "Epoch 23: accuracy improved from 0.11061 to 0.11795, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 275ms/step - loss: 3.1929 - accuracy: 0.1180 - top-5-accuracy: 0.3608 - val_loss: 3.0271 - val_accuracy: 0.1464 - val_top-5-accuracy: 0.4451 - lr: 5.0000e-05\n",
      "Epoch 24/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1765 - accuracy: 0.1139 - top-5-accuracy: 0.3665\n",
      "Epoch 24: accuracy did not improve from 0.11795\n",
      "147/147 [==============================] - 43s 273ms/step - loss: 3.1765 - accuracy: 0.1139 - top-5-accuracy: 0.3665 - val_loss: 3.0544 - val_accuracy: 0.1472 - val_top-5-accuracy: 0.4409 - lr: 5.0000e-05\n",
      "Epoch 25/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1590 - accuracy: 0.1192 - top-5-accuracy: 0.3751\n",
      "Epoch 25: accuracy improved from 0.11795 to 0.11923, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 3.1590 - accuracy: 0.1192 - top-5-accuracy: 0.3751 - val_loss: 3.0055 - val_accuracy: 0.1583 - val_top-5-accuracy: 0.4681 - lr: 5.0000e-05\n",
      "Epoch 26/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1488 - accuracy: 0.1307 - top-5-accuracy: 0.3793\n",
      "Epoch 26: accuracy improved from 0.11923 to 0.13072, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 3.1488 - accuracy: 0.1307 - top-5-accuracy: 0.3793 - val_loss: 2.9145 - val_accuracy: 0.1898 - val_top-5-accuracy: 0.4740 - lr: 5.0000e-05\n",
      "Epoch 27/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1313 - accuracy: 0.1282 - top-5-accuracy: 0.3902\n",
      "Epoch 27: accuracy did not improve from 0.13072\n",
      "147/147 [==============================] - 44s 274ms/step - loss: 3.1313 - accuracy: 0.1282 - top-5-accuracy: 0.3902 - val_loss: 2.9583 - val_accuracy: 0.1600 - val_top-5-accuracy: 0.4613 - lr: 5.0000e-05\n",
      "Epoch 28/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1246 - accuracy: 0.1278 - top-5-accuracy: 0.3995\n",
      "Epoch 28: accuracy did not improve from 0.13072\n",
      "147/147 [==============================] - 44s 274ms/step - loss: 3.1246 - accuracy: 0.1278 - top-5-accuracy: 0.3995 - val_loss: 2.8945 - val_accuracy: 0.2009 - val_top-5-accuracy: 0.4970 - lr: 5.0000e-05\n",
      "Epoch 29/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.1051 - accuracy: 0.1378 - top-5-accuracy: 0.4001\n",
      "Epoch 29: accuracy improved from 0.13072 to 0.13784, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 3.1051 - accuracy: 0.1378 - top-5-accuracy: 0.4001 - val_loss: 2.9791 - val_accuracy: 0.1651 - val_top-5-accuracy: 0.4689 - lr: 5.0000e-05\n",
      "Epoch 30/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0851 - accuracy: 0.1407 - top-5-accuracy: 0.4112\n",
      "Epoch 30: accuracy improved from 0.13784 to 0.14071, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 279ms/step - loss: 3.0851 - accuracy: 0.1407 - top-5-accuracy: 0.4112 - val_loss: 2.8963 - val_accuracy: 0.1872 - val_top-5-accuracy: 0.4945 - lr: 5.0000e-05\n",
      "Epoch 31/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0748 - accuracy: 0.1397 - top-5-accuracy: 0.4103\n",
      "Epoch 31: accuracy did not improve from 0.14071\n",
      "147/147 [==============================] - 44s 279ms/step - loss: 3.0748 - accuracy: 0.1397 - top-5-accuracy: 0.4103 - val_loss: 2.8945 - val_accuracy: 0.1770 - val_top-5-accuracy: 0.4783 - lr: 5.0000e-05\n",
      "Epoch 32/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0697 - accuracy: 0.1440 - top-5-accuracy: 0.4133\n",
      "Epoch 32: accuracy improved from 0.14071 to 0.14401, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 3.0697 - accuracy: 0.1440 - top-5-accuracy: 0.4133 - val_loss: 2.8656 - val_accuracy: 0.1898 - val_top-5-accuracy: 0.5174 - lr: 5.0000e-05\n",
      "Epoch 33/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0454 - accuracy: 0.1460 - top-5-accuracy: 0.4165\n",
      "Epoch 33: accuracy improved from 0.14401 to 0.14603, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 3.0454 - accuracy: 0.1460 - top-5-accuracy: 0.4165 - val_loss: 2.8891 - val_accuracy: 0.1779 - val_top-5-accuracy: 0.4962 - lr: 5.0000e-05\n",
      "Epoch 34/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0334 - accuracy: 0.1524 - top-5-accuracy: 0.4328\n",
      "Epoch 34: accuracy improved from 0.14603 to 0.15241, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 3.0334 - accuracy: 0.1524 - top-5-accuracy: 0.4328 - val_loss: 2.8442 - val_accuracy: 0.2119 - val_top-5-accuracy: 0.4953 - lr: 5.0000e-05\n",
      "Epoch 35/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0077 - accuracy: 0.1579 - top-5-accuracy: 0.4425\n",
      "Epoch 35: accuracy improved from 0.15241 to 0.15795, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 3.0077 - accuracy: 0.1579 - top-5-accuracy: 0.4425 - val_loss: 2.8047 - val_accuracy: 0.2221 - val_top-5-accuracy: 0.5226 - lr: 5.0000e-05\n",
      "Epoch 36/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 3.0055 - accuracy: 0.1562 - top-5-accuracy: 0.4401\n",
      "Epoch 36: accuracy did not improve from 0.15795\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 3.0055 - accuracy: 0.1562 - top-5-accuracy: 0.4401 - val_loss: 2.8195 - val_accuracy: 0.2068 - val_top-5-accuracy: 0.5183 - lr: 5.0000e-05\n",
      "Epoch 37/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9946 - accuracy: 0.1654 - top-5-accuracy: 0.4418\n",
      "Epoch 37: accuracy improved from 0.15795 to 0.16539, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 2.9946 - accuracy: 0.1654 - top-5-accuracy: 0.4418 - val_loss: 2.8543 - val_accuracy: 0.1923 - val_top-5-accuracy: 0.5004 - lr: 5.0000e-05\n",
      "Epoch 38/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9823 - accuracy: 0.1689 - top-5-accuracy: 0.4480\n",
      "Epoch 38: accuracy improved from 0.16539 to 0.16890, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 279ms/step - loss: 2.9823 - accuracy: 0.1689 - top-5-accuracy: 0.4480 - val_loss: 2.7553 - val_accuracy: 0.2145 - val_top-5-accuracy: 0.5455 - lr: 5.0000e-05\n",
      "Epoch 39/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9600 - accuracy: 0.1703 - top-5-accuracy: 0.4638\n",
      "Epoch 39: accuracy improved from 0.16890 to 0.17028, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 2.9600 - accuracy: 0.1703 - top-5-accuracy: 0.4638 - val_loss: 2.6754 - val_accuracy: 0.2528 - val_top-5-accuracy: 0.5796 - lr: 5.0000e-05\n",
      "Epoch 40/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9472 - accuracy: 0.1723 - top-5-accuracy: 0.4681\n",
      "Epoch 40: accuracy improved from 0.17028 to 0.17230, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.9472 - accuracy: 0.1723 - top-5-accuracy: 0.4681 - val_loss: 2.7159 - val_accuracy: 0.2332 - val_top-5-accuracy: 0.5685 - lr: 5.0000e-05\n",
      "Epoch 41/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9317 - accuracy: 0.1806 - top-5-accuracy: 0.4687\n",
      "Epoch 41: accuracy improved from 0.17230 to 0.18060, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.9317 - accuracy: 0.1806 - top-5-accuracy: 0.4687 - val_loss: 2.6698 - val_accuracy: 0.2306 - val_top-5-accuracy: 0.5626 - lr: 5.0000e-05\n",
      "Epoch 42/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9176 - accuracy: 0.1783 - top-5-accuracy: 0.4759\n",
      "Epoch 42: accuracy did not improve from 0.18060\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 2.9176 - accuracy: 0.1783 - top-5-accuracy: 0.4759 - val_loss: 2.6924 - val_accuracy: 0.2289 - val_top-5-accuracy: 0.5609 - lr: 5.0000e-05\n",
      "Epoch 43/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.9044 - accuracy: 0.1875 - top-5-accuracy: 0.4716\n",
      "Epoch 43: accuracy improved from 0.18060 to 0.18751, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.9044 - accuracy: 0.1875 - top-5-accuracy: 0.4716 - val_loss: 2.6335 - val_accuracy: 0.2715 - val_top-5-accuracy: 0.5745 - lr: 5.0000e-05\n",
      "Epoch 44/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8740 - accuracy: 0.1929 - top-5-accuracy: 0.4934\n",
      "Epoch 44: accuracy improved from 0.18751 to 0.19294, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 2.8740 - accuracy: 0.1929 - top-5-accuracy: 0.4934 - val_loss: 2.6714 - val_accuracy: 0.2426 - val_top-5-accuracy: 0.5566 - lr: 5.0000e-05\n",
      "Epoch 45/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8762 - accuracy: 0.1908 - top-5-accuracy: 0.4872\n",
      "Epoch 45: accuracy did not improve from 0.19294\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.8762 - accuracy: 0.1908 - top-5-accuracy: 0.4872 - val_loss: 2.6084 - val_accuracy: 0.2536 - val_top-5-accuracy: 0.5804 - lr: 5.0000e-05\n",
      "Epoch 46/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8487 - accuracy: 0.1969 - top-5-accuracy: 0.5029\n",
      "Epoch 46: accuracy improved from 0.19294 to 0.19687, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 2.8487 - accuracy: 0.1969 - top-5-accuracy: 0.5029 - val_loss: 2.5541 - val_accuracy: 0.2681 - val_top-5-accuracy: 0.5923 - lr: 5.0000e-05\n",
      "Epoch 47/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8273 - accuracy: 0.1987 - top-5-accuracy: 0.5106\n",
      "Epoch 47: accuracy improved from 0.19687 to 0.19868, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.8273 - accuracy: 0.1987 - top-5-accuracy: 0.5106 - val_loss: 2.5170 - val_accuracy: 0.2749 - val_top-5-accuracy: 0.6238 - lr: 5.0000e-05\n",
      "Epoch 48/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8152 - accuracy: 0.2055 - top-5-accuracy: 0.5129\n",
      "Epoch 48: accuracy improved from 0.19868 to 0.20549, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.8152 - accuracy: 0.2055 - top-5-accuracy: 0.5129 - val_loss: 2.5416 - val_accuracy: 0.2783 - val_top-5-accuracy: 0.6162 - lr: 5.0000e-05\n",
      "Epoch 49/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.8140 - accuracy: 0.2033 - top-5-accuracy: 0.5106\n",
      "Epoch 49: accuracy did not improve from 0.20549\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.8140 - accuracy: 0.2033 - top-5-accuracy: 0.5106 - val_loss: 2.4805 - val_accuracy: 0.2928 - val_top-5-accuracy: 0.6374 - lr: 5.0000e-05\n",
      "Epoch 50/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7929 - accuracy: 0.2126 - top-5-accuracy: 0.5212\n",
      "Epoch 50: accuracy improved from 0.20549 to 0.21261, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 2.7929 - accuracy: 0.2126 - top-5-accuracy: 0.5212 - val_loss: 2.4368 - val_accuracy: 0.3055 - val_top-5-accuracy: 0.6485 - lr: 5.0000e-05\n",
      "Epoch 51/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7714 - accuracy: 0.2187 - top-5-accuracy: 0.5305\n",
      "Epoch 51: accuracy improved from 0.21261 to 0.21868, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.7714 - accuracy: 0.2187 - top-5-accuracy: 0.5305 - val_loss: 2.4626 - val_accuracy: 0.2919 - val_top-5-accuracy: 0.6272 - lr: 5.0000e-05\n",
      "Epoch 52/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7486 - accuracy: 0.2214 - top-5-accuracy: 0.5395\n",
      "Epoch 52: accuracy improved from 0.21868 to 0.22144, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 2.7486 - accuracy: 0.2214 - top-5-accuracy: 0.5395 - val_loss: 2.4889 - val_accuracy: 0.2843 - val_top-5-accuracy: 0.6204 - lr: 5.0000e-05\n",
      "Epoch 53/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7233 - accuracy: 0.2224 - top-5-accuracy: 0.5427\n",
      "Epoch 53: accuracy improved from 0.22144 to 0.22240, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.7233 - accuracy: 0.2224 - top-5-accuracy: 0.5427 - val_loss: 2.4235 - val_accuracy: 0.3021 - val_top-5-accuracy: 0.6340 - lr: 5.0000e-05\n",
      "Epoch 54/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7233 - accuracy: 0.2227 - top-5-accuracy: 0.5441\n",
      "Epoch 54: accuracy improved from 0.22240 to 0.22272, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.7233 - accuracy: 0.2227 - top-5-accuracy: 0.5441 - val_loss: 2.3924 - val_accuracy: 0.3260 - val_top-5-accuracy: 0.6613 - lr: 5.0000e-05\n",
      "Epoch 55/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.7077 - accuracy: 0.2293 - top-5-accuracy: 0.5457\n",
      "Epoch 55: accuracy improved from 0.22272 to 0.22931, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 2.7077 - accuracy: 0.2293 - top-5-accuracy: 0.5457 - val_loss: 2.3360 - val_accuracy: 0.3285 - val_top-5-accuracy: 0.6791 - lr: 5.0000e-05\n",
      "Epoch 56/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.6791 - accuracy: 0.2373 - top-5-accuracy: 0.5681\n",
      "Epoch 56: accuracy improved from 0.22931 to 0.23729, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.6791 - accuracy: 0.2373 - top-5-accuracy: 0.5681 - val_loss: 2.4058 - val_accuracy: 0.3106 - val_top-5-accuracy: 0.6434 - lr: 5.0000e-05\n",
      "Epoch 57/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.6724 - accuracy: 0.2336 - top-5-accuracy: 0.5553\n",
      "Epoch 57: accuracy did not improve from 0.23729\n",
      "147/147 [==============================] - 45s 277ms/step - loss: 2.6724 - accuracy: 0.2336 - top-5-accuracy: 0.5553 - val_loss: 2.2588 - val_accuracy: 0.3455 - val_top-5-accuracy: 0.6902 - lr: 5.0000e-05\n",
      "Epoch 58/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.6517 - accuracy: 0.2395 - top-5-accuracy: 0.5753\n",
      "Epoch 58: accuracy improved from 0.23729 to 0.23952, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.6517 - accuracy: 0.2395 - top-5-accuracy: 0.5753 - val_loss: 2.2848 - val_accuracy: 0.3498 - val_top-5-accuracy: 0.6851 - lr: 5.0000e-05\n",
      "Epoch 59/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.6308 - accuracy: 0.2536 - top-5-accuracy: 0.5715\n",
      "Epoch 59: accuracy improved from 0.23952 to 0.25356, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 2.6308 - accuracy: 0.2536 - top-5-accuracy: 0.5715 - val_loss: 2.2893 - val_accuracy: 0.3404 - val_top-5-accuracy: 0.6800 - lr: 5.0000e-05\n",
      "Epoch 60/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.6139 - accuracy: 0.2559 - top-5-accuracy: 0.5829\n",
      "Epoch 60: accuracy improved from 0.25356 to 0.25590, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.6139 - accuracy: 0.2559 - top-5-accuracy: 0.5829 - val_loss: 2.2652 - val_accuracy: 0.3498 - val_top-5-accuracy: 0.6953 - lr: 5.0000e-05\n",
      "Epoch 61/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.5844 - accuracy: 0.2574 - top-5-accuracy: 0.5838\n",
      "Epoch 61: accuracy improved from 0.25590 to 0.25739, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.5844 - accuracy: 0.2574 - top-5-accuracy: 0.5838 - val_loss: 2.2056 - val_accuracy: 0.3651 - val_top-5-accuracy: 0.7089 - lr: 5.0000e-05\n",
      "Epoch 62/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.5666 - accuracy: 0.2677 - top-5-accuracy: 0.5974\n",
      "Epoch 62: accuracy improved from 0.25739 to 0.26771, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.5666 - accuracy: 0.2677 - top-5-accuracy: 0.5974 - val_loss: 2.1874 - val_accuracy: 0.3609 - val_top-5-accuracy: 0.6996 - lr: 5.0000e-05\n",
      "Epoch 63/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.5544 - accuracy: 0.2686 - top-5-accuracy: 0.5988\n",
      "Epoch 63: accuracy improved from 0.26771 to 0.26856, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.5544 - accuracy: 0.2686 - top-5-accuracy: 0.5988 - val_loss: 2.1110 - val_accuracy: 0.3983 - val_top-5-accuracy: 0.7191 - lr: 5.0000e-05\n",
      "Epoch 64/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.5374 - accuracy: 0.2703 - top-5-accuracy: 0.6044\n",
      "Epoch 64: accuracy improved from 0.26856 to 0.27026, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 2.5374 - accuracy: 0.2703 - top-5-accuracy: 0.6044 - val_loss: 2.0598 - val_accuracy: 0.3983 - val_top-5-accuracy: 0.7404 - lr: 5.0000e-05\n",
      "Epoch 65/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.5150 - accuracy: 0.2749 - top-5-accuracy: 0.6083\n",
      "Epoch 65: accuracy improved from 0.27026 to 0.27494, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 46s 285ms/step - loss: 2.5150 - accuracy: 0.2749 - top-5-accuracy: 0.6083 - val_loss: 2.1008 - val_accuracy: 0.3855 - val_top-5-accuracy: 0.7328 - lr: 5.0000e-05\n",
      "Epoch 66/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.4986 - accuracy: 0.2795 - top-5-accuracy: 0.6107\n",
      "Epoch 66: accuracy improved from 0.27494 to 0.27951, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 2.4986 - accuracy: 0.2795 - top-5-accuracy: 0.6107 - val_loss: 2.1094 - val_accuracy: 0.3881 - val_top-5-accuracy: 0.7413 - lr: 5.0000e-05\n",
      "Epoch 67/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.4812 - accuracy: 0.2858 - top-5-accuracy: 0.6185\n",
      "Epoch 67: accuracy improved from 0.27951 to 0.28579, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.4812 - accuracy: 0.2858 - top-5-accuracy: 0.6185 - val_loss: 2.0292 - val_accuracy: 0.4085 - val_top-5-accuracy: 0.7557 - lr: 5.0000e-05\n",
      "Epoch 68/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.4620 - accuracy: 0.2878 - top-5-accuracy: 0.6249\n",
      "Epoch 68: accuracy improved from 0.28579 to 0.28781, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.4620 - accuracy: 0.2878 - top-5-accuracy: 0.6249 - val_loss: 2.0498 - val_accuracy: 0.4051 - val_top-5-accuracy: 0.7319 - lr: 5.0000e-05\n",
      "Epoch 69/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.4435 - accuracy: 0.2860 - top-5-accuracy: 0.6328\n",
      "Epoch 69: accuracy did not improve from 0.28781\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.4435 - accuracy: 0.2860 - top-5-accuracy: 0.6328 - val_loss: 2.0019 - val_accuracy: 0.3915 - val_top-5-accuracy: 0.7677 - lr: 5.0000e-05\n",
      "Epoch 70/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.4212 - accuracy: 0.2944 - top-5-accuracy: 0.6351\n",
      "Epoch 70: accuracy improved from 0.28781 to 0.29441, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.4212 - accuracy: 0.2944 - top-5-accuracy: 0.6351 - val_loss: 1.9261 - val_accuracy: 0.4426 - val_top-5-accuracy: 0.7719 - lr: 5.0000e-05\n",
      "Epoch 71/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3924 - accuracy: 0.3101 - top-5-accuracy: 0.6390\n",
      "Epoch 71: accuracy improved from 0.29441 to 0.31015, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.3924 - accuracy: 0.3101 - top-5-accuracy: 0.6390 - val_loss: 1.9517 - val_accuracy: 0.4391 - val_top-5-accuracy: 0.7643 - lr: 5.0000e-05\n",
      "Epoch 72/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3880 - accuracy: 0.3044 - top-5-accuracy: 0.6439\n",
      "Epoch 72: accuracy did not improve from 0.31015\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.3880 - accuracy: 0.3044 - top-5-accuracy: 0.6439 - val_loss: 1.8494 - val_accuracy: 0.4494 - val_top-5-accuracy: 0.7915 - lr: 5.0000e-05\n",
      "Epoch 73/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3461 - accuracy: 0.3154 - top-5-accuracy: 0.6584\n",
      "Epoch 73: accuracy improved from 0.31015 to 0.31536, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 2.3461 - accuracy: 0.3154 - top-5-accuracy: 0.6584 - val_loss: 1.9168 - val_accuracy: 0.4298 - val_top-5-accuracy: 0.7702 - lr: 5.0000e-05\n",
      "Epoch 74/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3490 - accuracy: 0.3150 - top-5-accuracy: 0.6596\n",
      "Epoch 74: accuracy did not improve from 0.31536\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.3490 - accuracy: 0.3150 - top-5-accuracy: 0.6596 - val_loss: 1.9032 - val_accuracy: 0.4562 - val_top-5-accuracy: 0.7719 - lr: 5.0000e-05\n",
      "Epoch 75/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3063 - accuracy: 0.3266 - top-5-accuracy: 0.6733\n",
      "Epoch 75: accuracy improved from 0.31536 to 0.32663, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.3063 - accuracy: 0.3266 - top-5-accuracy: 0.6733 - val_loss: 1.8667 - val_accuracy: 0.4570 - val_top-5-accuracy: 0.7787 - lr: 5.0000e-05\n",
      "Epoch 76/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.3005 - accuracy: 0.3261 - top-5-accuracy: 0.6732\n",
      "Epoch 76: accuracy did not improve from 0.32663\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.3005 - accuracy: 0.3261 - top-5-accuracy: 0.6732 - val_loss: 1.8903 - val_accuracy: 0.4732 - val_top-5-accuracy: 0.7770 - lr: 5.0000e-05\n",
      "Epoch 77/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.2754 - accuracy: 0.3323 - top-5-accuracy: 0.6826\n",
      "Epoch 77: accuracy improved from 0.32663 to 0.33227, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 2.2754 - accuracy: 0.3323 - top-5-accuracy: 0.6826 - val_loss: 1.8199 - val_accuracy: 0.4800 - val_top-5-accuracy: 0.7957 - lr: 5.0000e-05\n",
      "Epoch 78/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.2431 - accuracy: 0.3404 - top-5-accuracy: 0.6820\n",
      "Epoch 78: accuracy improved from 0.33227 to 0.34035, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.2431 - accuracy: 0.3404 - top-5-accuracy: 0.6820 - val_loss: 1.8836 - val_accuracy: 0.4443 - val_top-5-accuracy: 0.7753 - lr: 5.0000e-05\n",
      "Epoch 79/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.2511 - accuracy: 0.3388 - top-5-accuracy: 0.6869\n",
      "Epoch 79: accuracy did not improve from 0.34035\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.2511 - accuracy: 0.3388 - top-5-accuracy: 0.6869 - val_loss: 1.7250 - val_accuracy: 0.5013 - val_top-5-accuracy: 0.8298 - lr: 5.0000e-05\n",
      "Epoch 80/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.2248 - accuracy: 0.3443 - top-5-accuracy: 0.6952\n",
      "Epoch 80: accuracy improved from 0.34035 to 0.34429, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 2.2248 - accuracy: 0.3443 - top-5-accuracy: 0.6952 - val_loss: 1.6579 - val_accuracy: 0.5123 - val_top-5-accuracy: 0.8204 - lr: 5.0000e-05\n",
      "Epoch 81/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1968 - accuracy: 0.3486 - top-5-accuracy: 0.6922\n",
      "Epoch 81: accuracy improved from 0.34429 to 0.34865, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.1968 - accuracy: 0.3486 - top-5-accuracy: 0.6922 - val_loss: 1.6567 - val_accuracy: 0.5055 - val_top-5-accuracy: 0.8451 - lr: 5.0000e-05\n",
      "Epoch 82/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1868 - accuracy: 0.3597 - top-5-accuracy: 0.7022\n",
      "Epoch 82: accuracy improved from 0.34865 to 0.35971, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 2.1868 - accuracy: 0.3597 - top-5-accuracy: 0.7022 - val_loss: 1.6104 - val_accuracy: 0.5123 - val_top-5-accuracy: 0.8451 - lr: 5.0000e-05\n",
      "Epoch 83/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1585 - accuracy: 0.3624 - top-5-accuracy: 0.7074\n",
      "Epoch 83: accuracy improved from 0.35971 to 0.36237, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 2.1585 - accuracy: 0.3624 - top-5-accuracy: 0.7074 - val_loss: 1.6298 - val_accuracy: 0.5243 - val_top-5-accuracy: 0.8170 - lr: 5.0000e-05\n",
      "Epoch 84/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1501 - accuracy: 0.3629 - top-5-accuracy: 0.7080\n",
      "Epoch 84: accuracy improved from 0.36237 to 0.36290, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.1501 - accuracy: 0.3629 - top-5-accuracy: 0.7080 - val_loss: 1.6103 - val_accuracy: 0.5345 - val_top-5-accuracy: 0.8468 - lr: 5.0000e-05\n",
      "Epoch 85/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1243 - accuracy: 0.3769 - top-5-accuracy: 0.7167\n",
      "Epoch 85: accuracy improved from 0.36290 to 0.37694, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.1243 - accuracy: 0.3769 - top-5-accuracy: 0.7167 - val_loss: 1.6175 - val_accuracy: 0.5174 - val_top-5-accuracy: 0.8417 - lr: 5.0000e-05\n",
      "Epoch 86/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.1052 - accuracy: 0.3750 - top-5-accuracy: 0.7278\n",
      "Epoch 86: accuracy did not improve from 0.37694\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 2.1052 - accuracy: 0.3750 - top-5-accuracy: 0.7278 - val_loss: 1.5145 - val_accuracy: 0.5540 - val_top-5-accuracy: 0.8468 - lr: 5.0000e-05\n",
      "Epoch 87/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0908 - accuracy: 0.3832 - top-5-accuracy: 0.7247\n",
      "Epoch 87: accuracy improved from 0.37694 to 0.38322, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.0908 - accuracy: 0.3832 - top-5-accuracy: 0.7247 - val_loss: 1.4748 - val_accuracy: 0.5532 - val_top-5-accuracy: 0.8545 - lr: 5.0000e-05\n",
      "Epoch 88/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0664 - accuracy: 0.3849 - top-5-accuracy: 0.7328\n",
      "Epoch 88: accuracy improved from 0.38322 to 0.38492, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 284ms/step - loss: 2.0664 - accuracy: 0.3849 - top-5-accuracy: 0.7328 - val_loss: 1.5744 - val_accuracy: 0.5132 - val_top-5-accuracy: 0.8323 - lr: 5.0000e-05\n",
      "Epoch 89/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0543 - accuracy: 0.3854 - top-5-accuracy: 0.7369\n",
      "Epoch 89: accuracy improved from 0.38492 to 0.38545, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.0543 - accuracy: 0.3854 - top-5-accuracy: 0.7369 - val_loss: 1.4800 - val_accuracy: 0.5626 - val_top-5-accuracy: 0.8604 - lr: 5.0000e-05\n",
      "Epoch 90/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0326 - accuracy: 0.3931 - top-5-accuracy: 0.7365\n",
      "Epoch 90: accuracy improved from 0.38545 to 0.39311, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 2.0326 - accuracy: 0.3931 - top-5-accuracy: 0.7365 - val_loss: 1.5022 - val_accuracy: 0.5745 - val_top-5-accuracy: 0.8511 - lr: 5.0000e-05\n",
      "Epoch 91/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0057 - accuracy: 0.3995 - top-5-accuracy: 0.7444\n",
      "Epoch 91: accuracy improved from 0.39311 to 0.39949, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 277ms/step - loss: 2.0057 - accuracy: 0.3995 - top-5-accuracy: 0.7444 - val_loss: 1.4132 - val_accuracy: 0.5821 - val_top-5-accuracy: 0.8715 - lr: 5.0000e-05\n",
      "Epoch 92/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 2.0077 - accuracy: 0.3961 - top-5-accuracy: 0.7443\n",
      "Epoch 92: accuracy did not improve from 0.39949\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 2.0077 - accuracy: 0.3961 - top-5-accuracy: 0.7443 - val_loss: 1.4674 - val_accuracy: 0.5728 - val_top-5-accuracy: 0.8596 - lr: 5.0000e-05\n",
      "Epoch 93/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.9750 - accuracy: 0.4117 - top-5-accuracy: 0.7543\n",
      "Epoch 93: accuracy improved from 0.39949 to 0.41172, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.9750 - accuracy: 0.4117 - top-5-accuracy: 0.7543 - val_loss: 1.4106 - val_accuracy: 0.5643 - val_top-5-accuracy: 0.8681 - lr: 5.0000e-05\n",
      "Epoch 94/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.9672 - accuracy: 0.4075 - top-5-accuracy: 0.7541\n",
      "Epoch 94: accuracy did not improve from 0.41172\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.9672 - accuracy: 0.4075 - top-5-accuracy: 0.7541 - val_loss: 1.3066 - val_accuracy: 0.6060 - val_top-5-accuracy: 0.8766 - lr: 5.0000e-05\n",
      "Epoch 95/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.9447 - accuracy: 0.4166 - top-5-accuracy: 0.7609\n",
      "Epoch 95: accuracy improved from 0.41172 to 0.41661, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.9447 - accuracy: 0.4166 - top-5-accuracy: 0.7609 - val_loss: 1.3260 - val_accuracy: 0.6017 - val_top-5-accuracy: 0.8970 - lr: 5.0000e-05\n",
      "Epoch 96/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.9226 - accuracy: 0.4209 - top-5-accuracy: 0.7668\n",
      "Epoch 96: accuracy improved from 0.41661 to 0.42087, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.9226 - accuracy: 0.4209 - top-5-accuracy: 0.7668 - val_loss: 1.3198 - val_accuracy: 0.6102 - val_top-5-accuracy: 0.8749 - lr: 5.0000e-05\n",
      "Epoch 97/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.8904 - accuracy: 0.4257 - top-5-accuracy: 0.7724\n",
      "Epoch 97: accuracy improved from 0.42087 to 0.42565, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.8904 - accuracy: 0.4257 - top-5-accuracy: 0.7724 - val_loss: 1.2232 - val_accuracy: 0.6213 - val_top-5-accuracy: 0.8987 - lr: 5.0000e-05\n",
      "Epoch 98/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.8957 - accuracy: 0.4232 - top-5-accuracy: 0.7706\n",
      "Epoch 98: accuracy did not improve from 0.42565\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.8957 - accuracy: 0.4232 - top-5-accuracy: 0.7706 - val_loss: 1.2785 - val_accuracy: 0.6281 - val_top-5-accuracy: 0.8860 - lr: 5.0000e-05\n",
      "Epoch 99/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.8641 - accuracy: 0.4353 - top-5-accuracy: 0.7848\n",
      "Epoch 99: accuracy improved from 0.42565 to 0.43533, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.8641 - accuracy: 0.4353 - top-5-accuracy: 0.7848 - val_loss: 1.2835 - val_accuracy: 0.6196 - val_top-5-accuracy: 0.8962 - lr: 5.0000e-05\n",
      "Epoch 100/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.8329 - accuracy: 0.4378 - top-5-accuracy: 0.7895\n",
      "Epoch 100: accuracy improved from 0.43533 to 0.43778, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.8329 - accuracy: 0.4378 - top-5-accuracy: 0.7895 - val_loss: 1.2630 - val_accuracy: 0.6230 - val_top-5-accuracy: 0.8945 - lr: 5.0000e-05\n",
      "Epoch 101/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.8398 - accuracy: 0.4418 - top-5-accuracy: 0.7876\n",
      "Epoch 101: accuracy improved from 0.43778 to 0.44182, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 1.8398 - accuracy: 0.4418 - top-5-accuracy: 0.7876 - val_loss: 1.2332 - val_accuracy: 0.6409 - val_top-5-accuracy: 0.8979 - lr: 5.0000e-05\n",
      "Epoch 102/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7946 - accuracy: 0.4531 - top-5-accuracy: 0.7889\n",
      "Epoch 102: accuracy improved from 0.44182 to 0.45310, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.7946 - accuracy: 0.4531 - top-5-accuracy: 0.7889 - val_loss: 1.2269 - val_accuracy: 0.6119 - val_top-5-accuracy: 0.9038 - lr: 5.0000e-05\n",
      "Epoch 103/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7972 - accuracy: 0.4512 - top-5-accuracy: 0.7929\n",
      "Epoch 103: accuracy did not improve from 0.45310\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.7972 - accuracy: 0.4512 - top-5-accuracy: 0.7929 - val_loss: 1.1235 - val_accuracy: 0.6698 - val_top-5-accuracy: 0.9191 - lr: 5.0000e-05\n",
      "Epoch 104/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7820 - accuracy: 0.4592 - top-5-accuracy: 0.7961\n",
      "Epoch 104: accuracy improved from 0.45310 to 0.45916, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.7820 - accuracy: 0.4592 - top-5-accuracy: 0.7961 - val_loss: 1.1635 - val_accuracy: 0.6545 - val_top-5-accuracy: 0.9021 - lr: 5.0000e-05\n",
      "Epoch 105/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7546 - accuracy: 0.4636 - top-5-accuracy: 0.8033\n",
      "Epoch 105: accuracy improved from 0.45916 to 0.46362, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.7546 - accuracy: 0.4636 - top-5-accuracy: 0.8033 - val_loss: 1.1411 - val_accuracy: 0.6562 - val_top-5-accuracy: 0.9098 - lr: 5.0000e-05\n",
      "Epoch 106/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7471 - accuracy: 0.4666 - top-5-accuracy: 0.8045\n",
      "Epoch 106: accuracy improved from 0.46362 to 0.46660, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.7471 - accuracy: 0.4666 - top-5-accuracy: 0.8045 - val_loss: 1.1831 - val_accuracy: 0.6417 - val_top-5-accuracy: 0.9123 - lr: 5.0000e-05\n",
      "Epoch 107/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7313 - accuracy: 0.4690 - top-5-accuracy: 0.8054\n",
      "Epoch 107: accuracy improved from 0.46660 to 0.46905, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 1.7313 - accuracy: 0.4690 - top-5-accuracy: 0.8054 - val_loss: 1.1217 - val_accuracy: 0.6638 - val_top-5-accuracy: 0.9260 - lr: 5.0000e-05\n",
      "Epoch 108/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.7070 - accuracy: 0.4764 - top-5-accuracy: 0.8172\n",
      "Epoch 108: accuracy improved from 0.46905 to 0.47639, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.7070 - accuracy: 0.4764 - top-5-accuracy: 0.8172 - val_loss: 1.0996 - val_accuracy: 0.6689 - val_top-5-accuracy: 0.9209 - lr: 5.0000e-05\n",
      "Epoch 109/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6853 - accuracy: 0.4831 - top-5-accuracy: 0.8205\n",
      "Epoch 109: accuracy improved from 0.47639 to 0.48309, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.6853 - accuracy: 0.4831 - top-5-accuracy: 0.8205 - val_loss: 1.0971 - val_accuracy: 0.6698 - val_top-5-accuracy: 0.9166 - lr: 5.0000e-05\n",
      "Epoch 110/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6780 - accuracy: 0.4815 - top-5-accuracy: 0.8221\n",
      "Epoch 110: accuracy did not improve from 0.48309\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.6780 - accuracy: 0.4815 - top-5-accuracy: 0.8221 - val_loss: 1.0542 - val_accuracy: 0.6851 - val_top-5-accuracy: 0.9285 - lr: 5.0000e-05\n",
      "Epoch 111/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6522 - accuracy: 0.4852 - top-5-accuracy: 0.8258\n",
      "Epoch 111: accuracy improved from 0.48309 to 0.48522, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.6522 - accuracy: 0.4852 - top-5-accuracy: 0.8258 - val_loss: 1.0261 - val_accuracy: 0.6834 - val_top-5-accuracy: 0.9140 - lr: 5.0000e-05\n",
      "Epoch 112/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6334 - accuracy: 0.5005 - top-5-accuracy: 0.8291\n",
      "Epoch 112: accuracy improved from 0.48522 to 0.50053, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 1.6334 - accuracy: 0.5005 - top-5-accuracy: 0.8291 - val_loss: 0.9855 - val_accuracy: 0.6919 - val_top-5-accuracy: 0.9226 - lr: 5.0000e-05\n",
      "Epoch 113/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6228 - accuracy: 0.5006 - top-5-accuracy: 0.8334\n",
      "Epoch 113: accuracy improved from 0.50053 to 0.50064, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.6228 - accuracy: 0.5006 - top-5-accuracy: 0.8334 - val_loss: 0.9847 - val_accuracy: 0.6945 - val_top-5-accuracy: 0.9260 - lr: 5.0000e-05\n",
      "Epoch 114/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.6064 - accuracy: 0.5076 - top-5-accuracy: 0.8345\n",
      "Epoch 114: accuracy improved from 0.50064 to 0.50755, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.6064 - accuracy: 0.5076 - top-5-accuracy: 0.8345 - val_loss: 0.9974 - val_accuracy: 0.6894 - val_top-5-accuracy: 0.9362 - lr: 5.0000e-05\n",
      "Epoch 115/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5774 - accuracy: 0.5078 - top-5-accuracy: 0.8365\n",
      "Epoch 115: accuracy improved from 0.50755 to 0.50776, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 282ms/step - loss: 1.5774 - accuracy: 0.5078 - top-5-accuracy: 0.8365 - val_loss: 0.9943 - val_accuracy: 0.7183 - val_top-5-accuracy: 0.9285 - lr: 5.0000e-05\n",
      "Epoch 116/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5816 - accuracy: 0.5121 - top-5-accuracy: 0.8389\n",
      "Epoch 116: accuracy improved from 0.50776 to 0.51213, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.5816 - accuracy: 0.5121 - top-5-accuracy: 0.8389 - val_loss: 0.9577 - val_accuracy: 0.6962 - val_top-5-accuracy: 0.9302 - lr: 5.0000e-05\n",
      "Epoch 117/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5350 - accuracy: 0.5237 - top-5-accuracy: 0.8457\n",
      "Epoch 117: accuracy improved from 0.51213 to 0.52372, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 1.5350 - accuracy: 0.5237 - top-5-accuracy: 0.8457 - val_loss: 0.9206 - val_accuracy: 0.7183 - val_top-5-accuracy: 0.9234 - lr: 5.0000e-05\n",
      "Epoch 118/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5213 - accuracy: 0.5280 - top-5-accuracy: 0.8492\n",
      "Epoch 118: accuracy improved from 0.52372 to 0.52797, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 284ms/step - loss: 1.5213 - accuracy: 0.5280 - top-5-accuracy: 0.8492 - val_loss: 0.9030 - val_accuracy: 0.7217 - val_top-5-accuracy: 0.9345 - lr: 5.0000e-05\n",
      "Epoch 119/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5110 - accuracy: 0.5260 - top-5-accuracy: 0.8529\n",
      "Epoch 119: accuracy did not improve from 0.52797\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.5110 - accuracy: 0.5260 - top-5-accuracy: 0.8529 - val_loss: 0.9011 - val_accuracy: 0.7234 - val_top-5-accuracy: 0.9489 - lr: 5.0000e-05\n",
      "Epoch 120/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.5145 - accuracy: 0.5175 - top-5-accuracy: 0.8560\n",
      "Epoch 120: accuracy did not improve from 0.52797\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.5145 - accuracy: 0.5175 - top-5-accuracy: 0.8560 - val_loss: 0.9013 - val_accuracy: 0.7311 - val_top-5-accuracy: 0.9404 - lr: 5.0000e-05\n",
      "Epoch 121/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4977 - accuracy: 0.5332 - top-5-accuracy: 0.8555\n",
      "Epoch 121: accuracy improved from 0.52797 to 0.53318, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.4977 - accuracy: 0.5332 - top-5-accuracy: 0.8555 - val_loss: 0.9083 - val_accuracy: 0.7200 - val_top-5-accuracy: 0.9489 - lr: 5.0000e-05\n",
      "Epoch 122/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4971 - accuracy: 0.5344 - top-5-accuracy: 0.8542\n",
      "Epoch 122: accuracy improved from 0.53318 to 0.53435, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 1.4971 - accuracy: 0.5344 - top-5-accuracy: 0.8542 - val_loss: 0.8934 - val_accuracy: 0.7260 - val_top-5-accuracy: 0.9430 - lr: 5.0000e-05\n",
      "Epoch 123/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4741 - accuracy: 0.5355 - top-5-accuracy: 0.8574\n",
      "Epoch 123: accuracy improved from 0.53435 to 0.53552, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.4741 - accuracy: 0.5355 - top-5-accuracy: 0.8574 - val_loss: 0.8350 - val_accuracy: 0.7370 - val_top-5-accuracy: 0.9455 - lr: 5.0000e-05\n",
      "Epoch 124/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4443 - accuracy: 0.5423 - top-5-accuracy: 0.8635\n",
      "Epoch 124: accuracy improved from 0.53552 to 0.54233, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.4443 - accuracy: 0.5423 - top-5-accuracy: 0.8635 - val_loss: 0.8591 - val_accuracy: 0.7277 - val_top-5-accuracy: 0.9515 - lr: 5.0000e-05\n",
      "Epoch 125/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4413 - accuracy: 0.5480 - top-5-accuracy: 0.8686\n",
      "Epoch 125: accuracy improved from 0.54233 to 0.54797, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.4413 - accuracy: 0.5480 - top-5-accuracy: 0.8686 - val_loss: 0.8075 - val_accuracy: 0.7345 - val_top-5-accuracy: 0.9464 - lr: 5.0000e-05\n",
      "Epoch 126/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.4203 - accuracy: 0.5521 - top-5-accuracy: 0.8707\n",
      "Epoch 126: accuracy improved from 0.54797 to 0.55212, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.4203 - accuracy: 0.5521 - top-5-accuracy: 0.8707 - val_loss: 0.8000 - val_accuracy: 0.7668 - val_top-5-accuracy: 0.9591 - lr: 5.0000e-05\n",
      "Epoch 127/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3926 - accuracy: 0.5548 - top-5-accuracy: 0.8777\n",
      "Epoch 127: accuracy improved from 0.55212 to 0.55478, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.3926 - accuracy: 0.5548 - top-5-accuracy: 0.8777 - val_loss: 0.7713 - val_accuracy: 0.7481 - val_top-5-accuracy: 0.9498 - lr: 5.0000e-05\n",
      "Epoch 128/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3846 - accuracy: 0.5602 - top-5-accuracy: 0.8732\n",
      "Epoch 128: accuracy improved from 0.55478 to 0.56020, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.3846 - accuracy: 0.5602 - top-5-accuracy: 0.8732 - val_loss: 0.7478 - val_accuracy: 0.7591 - val_top-5-accuracy: 0.9583 - lr: 5.0000e-05\n",
      "Epoch 129/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3949 - accuracy: 0.5597 - top-5-accuracy: 0.8734\n",
      "Epoch 129: accuracy did not improve from 0.56020\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.3949 - accuracy: 0.5597 - top-5-accuracy: 0.8734 - val_loss: 0.7411 - val_accuracy: 0.7787 - val_top-5-accuracy: 0.9549 - lr: 5.0000e-05\n",
      "Epoch 130/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3827 - accuracy: 0.5624 - top-5-accuracy: 0.8756\n",
      "Epoch 130: accuracy improved from 0.56020 to 0.56243, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.3827 - accuracy: 0.5624 - top-5-accuracy: 0.8756 - val_loss: 0.7757 - val_accuracy: 0.7626 - val_top-5-accuracy: 0.9540 - lr: 5.0000e-05\n",
      "Epoch 131/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3616 - accuracy: 0.5673 - top-5-accuracy: 0.8784\n",
      "Epoch 131: accuracy improved from 0.56243 to 0.56733, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.3616 - accuracy: 0.5673 - top-5-accuracy: 0.8784 - val_loss: 0.7156 - val_accuracy: 0.7711 - val_top-5-accuracy: 0.9583 - lr: 5.0000e-05\n",
      "Epoch 132/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3247 - accuracy: 0.5734 - top-5-accuracy: 0.8818\n",
      "Epoch 132: accuracy improved from 0.56733 to 0.57339, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.3247 - accuracy: 0.5734 - top-5-accuracy: 0.8818 - val_loss: 0.6951 - val_accuracy: 0.7821 - val_top-5-accuracy: 0.9591 - lr: 5.0000e-05\n",
      "Epoch 133/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3285 - accuracy: 0.5799 - top-5-accuracy: 0.8829\n",
      "Epoch 133: accuracy improved from 0.57339 to 0.57988, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.3285 - accuracy: 0.5799 - top-5-accuracy: 0.8829 - val_loss: 0.7584 - val_accuracy: 0.7609 - val_top-5-accuracy: 0.9523 - lr: 5.0000e-05\n",
      "Epoch 134/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.3094 - accuracy: 0.5769 - top-5-accuracy: 0.8911\n",
      "Epoch 134: accuracy did not improve from 0.57988\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.3094 - accuracy: 0.5769 - top-5-accuracy: 0.8911 - val_loss: 0.7690 - val_accuracy: 0.7549 - val_top-5-accuracy: 0.9574 - lr: 5.0000e-05\n",
      "Epoch 135/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2987 - accuracy: 0.5733 - top-5-accuracy: 0.8906\n",
      "Epoch 135: accuracy did not improve from 0.57988\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.2987 - accuracy: 0.5733 - top-5-accuracy: 0.8906 - val_loss: 0.6848 - val_accuracy: 0.7838 - val_top-5-accuracy: 0.9600 - lr: 5.0000e-05\n",
      "Epoch 136/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2873 - accuracy: 0.5847 - top-5-accuracy: 0.8919\n",
      "Epoch 136: accuracy improved from 0.57988 to 0.58466, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.2873 - accuracy: 0.5847 - top-5-accuracy: 0.8919 - val_loss: 0.6646 - val_accuracy: 0.7881 - val_top-5-accuracy: 0.9557 - lr: 5.0000e-05\n",
      "Epoch 137/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2778 - accuracy: 0.5874 - top-5-accuracy: 0.8889\n",
      "Epoch 137: accuracy improved from 0.58466 to 0.58743, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.2778 - accuracy: 0.5874 - top-5-accuracy: 0.8889 - val_loss: 0.6616 - val_accuracy: 0.7983 - val_top-5-accuracy: 0.9583 - lr: 5.0000e-05\n",
      "Epoch 138/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2428 - accuracy: 0.5984 - top-5-accuracy: 0.8987\n",
      "Epoch 138: accuracy improved from 0.58743 to 0.59838, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.2428 - accuracy: 0.5984 - top-5-accuracy: 0.8987 - val_loss: 0.6601 - val_accuracy: 0.7779 - val_top-5-accuracy: 0.9634 - lr: 5.0000e-05\n",
      "Epoch 139/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2607 - accuracy: 0.6013 - top-5-accuracy: 0.8963\n",
      "Epoch 139: accuracy improved from 0.59838 to 0.60126, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 46s 288ms/step - loss: 1.2607 - accuracy: 0.6013 - top-5-accuracy: 0.8963 - val_loss: 0.6764 - val_accuracy: 0.7770 - val_top-5-accuracy: 0.9498 - lr: 5.0000e-05\n",
      "Epoch 140/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2019 - accuracy: 0.6143 - top-5-accuracy: 0.9044\n",
      "Epoch 140: accuracy improved from 0.60126 to 0.61434, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 47s 297ms/step - loss: 1.2019 - accuracy: 0.6143 - top-5-accuracy: 0.9044 - val_loss: 0.6336 - val_accuracy: 0.8009 - val_top-5-accuracy: 0.9634 - lr: 5.0000e-05\n",
      "Epoch 141/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2191 - accuracy: 0.6033 - top-5-accuracy: 0.8999\n",
      "Epoch 141: accuracy did not improve from 0.61434\n",
      "147/147 [==============================] - 47s 291ms/step - loss: 1.2191 - accuracy: 0.6033 - top-5-accuracy: 0.8999 - val_loss: 0.6105 - val_accuracy: 0.7949 - val_top-5-accuracy: 0.9694 - lr: 5.0000e-05\n",
      "Epoch 142/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.2047 - accuracy: 0.6092 - top-5-accuracy: 0.9037\n",
      "Epoch 142: accuracy did not improve from 0.61434\n",
      "147/147 [==============================] - 46s 285ms/step - loss: 1.2047 - accuracy: 0.6092 - top-5-accuracy: 0.9037 - val_loss: 0.6172 - val_accuracy: 0.7966 - val_top-5-accuracy: 0.9668 - lr: 5.0000e-05\n",
      "Epoch 143/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1983 - accuracy: 0.6057 - top-5-accuracy: 0.9037\n",
      "Epoch 143: accuracy did not improve from 0.61434\n",
      "147/147 [==============================] - 48s 298ms/step - loss: 1.1983 - accuracy: 0.6057 - top-5-accuracy: 0.9037 - val_loss: 0.6218 - val_accuracy: 0.7906 - val_top-5-accuracy: 0.9609 - lr: 5.0000e-05\n",
      "Epoch 144/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1774 - accuracy: 0.6220 - top-5-accuracy: 0.9058\n",
      "Epoch 144: accuracy improved from 0.61434 to 0.62200, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 50s 311ms/step - loss: 1.1774 - accuracy: 0.6220 - top-5-accuracy: 0.9058 - val_loss: 0.5695 - val_accuracy: 0.8051 - val_top-5-accuracy: 0.9677 - lr: 5.0000e-05\n",
      "Epoch 145/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1747 - accuracy: 0.6141 - top-5-accuracy: 0.9080\n",
      "Epoch 145: accuracy did not improve from 0.62200\n",
      "147/147 [==============================] - 47s 295ms/step - loss: 1.1747 - accuracy: 0.6141 - top-5-accuracy: 0.9080 - val_loss: 0.5961 - val_accuracy: 0.7949 - val_top-5-accuracy: 0.9711 - lr: 5.0000e-05\n",
      "Epoch 146/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1633 - accuracy: 0.6167 - top-5-accuracy: 0.9083\n",
      "Epoch 146: accuracy did not improve from 0.62200\n",
      "147/147 [==============================] - 46s 289ms/step - loss: 1.1633 - accuracy: 0.6167 - top-5-accuracy: 0.9083 - val_loss: 0.6173 - val_accuracy: 0.7932 - val_top-5-accuracy: 0.9660 - lr: 5.0000e-05\n",
      "Epoch 147/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1489 - accuracy: 0.6250 - top-5-accuracy: 0.9109\n",
      "Epoch 147: accuracy improved from 0.62200 to 0.62497, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 46s 289ms/step - loss: 1.1489 - accuracy: 0.6250 - top-5-accuracy: 0.9109 - val_loss: 0.5929 - val_accuracy: 0.7872 - val_top-5-accuracy: 0.9711 - lr: 5.0000e-05\n",
      "Epoch 148/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1478 - accuracy: 0.6264 - top-5-accuracy: 0.9096\n",
      "Epoch 148: accuracy improved from 0.62497 to 0.62636, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 48s 299ms/step - loss: 1.1478 - accuracy: 0.6264 - top-5-accuracy: 0.9096 - val_loss: 0.5699 - val_accuracy: 0.7923 - val_top-5-accuracy: 0.9753 - lr: 5.0000e-05\n",
      "Epoch 149/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.6321 - top-5-accuracy: 0.9145\n",
      "Epoch 149: accuracy improved from 0.62636 to 0.63210, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 55s 334ms/step - loss: 1.1325 - accuracy: 0.6321 - top-5-accuracy: 0.9145 - val_loss: 0.5602 - val_accuracy: 0.8111 - val_top-5-accuracy: 0.9694 - lr: 5.0000e-05\n",
      "Epoch 150/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1175 - accuracy: 0.6255 - top-5-accuracy: 0.9170\n",
      "Epoch 150: accuracy did not improve from 0.63210\n",
      "147/147 [==============================] - 54s 334ms/step - loss: 1.1175 - accuracy: 0.6255 - top-5-accuracy: 0.9170 - val_loss: 0.5173 - val_accuracy: 0.8179 - val_top-5-accuracy: 0.9813 - lr: 5.0000e-05\n",
      "Epoch 151/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.1216 - accuracy: 0.6319 - top-5-accuracy: 0.9155\n",
      "Epoch 151: accuracy did not improve from 0.63210\n",
      "147/147 [==============================] - 46s 280ms/step - loss: 1.1216 - accuracy: 0.6319 - top-5-accuracy: 0.9155 - val_loss: 0.5189 - val_accuracy: 0.8145 - val_top-5-accuracy: 0.9753 - lr: 5.0000e-05\n",
      "Epoch 152/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0993 - accuracy: 0.6402 - top-5-accuracy: 0.9171\n",
      "Epoch 152: accuracy improved from 0.63210 to 0.64018, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 1.0993 - accuracy: 0.6402 - top-5-accuracy: 0.9171 - val_loss: 0.5416 - val_accuracy: 0.8111 - val_top-5-accuracy: 0.9711 - lr: 5.0000e-05\n",
      "Epoch 153/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0902 - accuracy: 0.6439 - top-5-accuracy: 0.9232\n",
      "Epoch 153: accuracy improved from 0.64018 to 0.64391, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 1.0902 - accuracy: 0.6439 - top-5-accuracy: 0.9232 - val_loss: 0.5093 - val_accuracy: 0.8170 - val_top-5-accuracy: 0.9753 - lr: 5.0000e-05\n",
      "Epoch 154/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0719 - accuracy: 0.6397 - top-5-accuracy: 0.9245\n",
      "Epoch 154: accuracy did not improve from 0.64391\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.0719 - accuracy: 0.6397 - top-5-accuracy: 0.9245 - val_loss: 0.4924 - val_accuracy: 0.8315 - val_top-5-accuracy: 0.9711 - lr: 5.0000e-05\n",
      "Epoch 155/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0534 - accuracy: 0.6456 - top-5-accuracy: 0.9282\n",
      "Epoch 155: accuracy improved from 0.64391 to 0.64561, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 1.0534 - accuracy: 0.6456 - top-5-accuracy: 0.9282 - val_loss: 0.4691 - val_accuracy: 0.8204 - val_top-5-accuracy: 0.9872 - lr: 5.0000e-05\n",
      "Epoch 156/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.6550 - top-5-accuracy: 0.9252\n",
      "Epoch 156: accuracy improved from 0.64561 to 0.65497, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.0482 - accuracy: 0.6550 - top-5-accuracy: 0.9252 - val_loss: 0.4938 - val_accuracy: 0.8247 - val_top-5-accuracy: 0.9779 - lr: 5.0000e-05\n",
      "Epoch 157/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0521 - accuracy: 0.6554 - top-5-accuracy: 0.9265\n",
      "Epoch 157: accuracy improved from 0.65497 to 0.65539, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 1.0521 - accuracy: 0.6554 - top-5-accuracy: 0.9265 - val_loss: 0.5282 - val_accuracy: 0.8009 - val_top-5-accuracy: 0.9728 - lr: 5.0000e-05\n",
      "Epoch 158/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.6492 - top-5-accuracy: 0.9274\n",
      "Epoch 158: accuracy did not improve from 0.65539\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 1.0313 - accuracy: 0.6492 - top-5-accuracy: 0.9274 - val_loss: 0.4794 - val_accuracy: 0.8289 - val_top-5-accuracy: 0.9847 - lr: 5.0000e-05\n",
      "Epoch 159/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0293 - accuracy: 0.6628 - top-5-accuracy: 0.9285\n",
      "Epoch 159: accuracy improved from 0.65539 to 0.66284, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 1.0293 - accuracy: 0.6628 - top-5-accuracy: 0.9285 - val_loss: 0.5367 - val_accuracy: 0.8077 - val_top-5-accuracy: 0.9719 - lr: 5.0000e-05\n",
      "Epoch 160/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 1.0171 - accuracy: 0.6618 - top-5-accuracy: 0.9315\n",
      "Epoch 160: accuracy did not improve from 0.66284\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 1.0171 - accuracy: 0.6618 - top-5-accuracy: 0.9315 - val_loss: 0.4804 - val_accuracy: 0.8289 - val_top-5-accuracy: 0.9753 - lr: 5.0000e-05\n",
      "Epoch 161/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9904 - accuracy: 0.6667 - top-5-accuracy: 0.9317\n",
      "Epoch 161: accuracy improved from 0.66284 to 0.66667, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 0.9904 - accuracy: 0.6667 - top-5-accuracy: 0.9317 - val_loss: 0.5118 - val_accuracy: 0.8153 - val_top-5-accuracy: 0.9787 - lr: 5.0000e-05\n",
      "Epoch 162/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.6691 - top-5-accuracy: 0.9345\n",
      "Epoch 162: accuracy improved from 0.66667 to 0.66911, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 0.9947 - accuracy: 0.6691 - top-5-accuracy: 0.9345 - val_loss: 0.4548 - val_accuracy: 0.8196 - val_top-5-accuracy: 0.9787 - lr: 5.0000e-05\n",
      "Epoch 163/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9951 - accuracy: 0.6612 - top-5-accuracy: 0.9320\n",
      "Epoch 163: accuracy did not improve from 0.66911\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.9951 - accuracy: 0.6612 - top-5-accuracy: 0.9320 - val_loss: 0.4522 - val_accuracy: 0.8272 - val_top-5-accuracy: 0.9787 - lr: 5.0000e-05\n",
      "Epoch 164/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.6757 - top-5-accuracy: 0.9384\n",
      "Epoch 164: accuracy improved from 0.66911 to 0.67571, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 0.9702 - accuracy: 0.6757 - top-5-accuracy: 0.9384 - val_loss: 0.4220 - val_accuracy: 0.8502 - val_top-5-accuracy: 0.9847 - lr: 5.0000e-05\n",
      "Epoch 165/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9722 - accuracy: 0.6705 - top-5-accuracy: 0.9346\n",
      "Epoch 165: accuracy did not improve from 0.67571\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.9722 - accuracy: 0.6705 - top-5-accuracy: 0.9346 - val_loss: 0.4457 - val_accuracy: 0.8451 - val_top-5-accuracy: 0.9872 - lr: 5.0000e-05\n",
      "Epoch 166/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9629 - accuracy: 0.6756 - top-5-accuracy: 0.9347\n",
      "Epoch 166: accuracy did not improve from 0.67571\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 0.9629 - accuracy: 0.6756 - top-5-accuracy: 0.9347 - val_loss: 0.4161 - val_accuracy: 0.8511 - val_top-5-accuracy: 0.9864 - lr: 5.0000e-05\n",
      "Epoch 167/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9722 - accuracy: 0.6685 - top-5-accuracy: 0.9380\n",
      "Epoch 167: accuracy did not improve from 0.67571\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 0.9722 - accuracy: 0.6685 - top-5-accuracy: 0.9380 - val_loss: 0.4237 - val_accuracy: 0.8281 - val_top-5-accuracy: 0.9804 - lr: 5.0000e-05\n",
      "Epoch 168/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9493 - accuracy: 0.6746 - top-5-accuracy: 0.9385\n",
      "Epoch 168: accuracy did not improve from 0.67571\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.9493 - accuracy: 0.6746 - top-5-accuracy: 0.9385 - val_loss: 0.4020 - val_accuracy: 0.8391 - val_top-5-accuracy: 0.9847 - lr: 5.0000e-05\n",
      "Epoch 169/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9321 - accuracy: 0.6835 - top-5-accuracy: 0.9389\n",
      "Epoch 169: accuracy improved from 0.67571 to 0.68347, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 0.9321 - accuracy: 0.6835 - top-5-accuracy: 0.9389 - val_loss: 0.4407 - val_accuracy: 0.8349 - val_top-5-accuracy: 0.9821 - lr: 5.0000e-05\n",
      "Epoch 170/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9319 - accuracy: 0.6835 - top-5-accuracy: 0.9409\n",
      "Epoch 170: accuracy did not improve from 0.68347\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.9319 - accuracy: 0.6835 - top-5-accuracy: 0.9409 - val_loss: 0.4098 - val_accuracy: 0.8434 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 171/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9169 - accuracy: 0.6780 - top-5-accuracy: 0.9439\n",
      "Epoch 171: accuracy did not improve from 0.68347\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 0.9169 - accuracy: 0.6780 - top-5-accuracy: 0.9439 - val_loss: 0.4427 - val_accuracy: 0.8383 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 172/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9131 - accuracy: 0.6885 - top-5-accuracy: 0.9428\n",
      "Epoch 172: accuracy improved from 0.68347 to 0.68847, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 0.9131 - accuracy: 0.6885 - top-5-accuracy: 0.9428 - val_loss: 0.3976 - val_accuracy: 0.8502 - val_top-5-accuracy: 0.9830 - lr: 5.0000e-05\n",
      "Epoch 173/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.9078 - accuracy: 0.6907 - top-5-accuracy: 0.9444\n",
      "Epoch 173: accuracy improved from 0.68847 to 0.69070, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 0.9078 - accuracy: 0.6907 - top-5-accuracy: 0.9444 - val_loss: 0.3999 - val_accuracy: 0.8485 - val_top-5-accuracy: 0.9830 - lr: 5.0000e-05\n",
      "Epoch 174/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8993 - accuracy: 0.6911 - top-5-accuracy: 0.9421\n",
      "Epoch 174: accuracy improved from 0.69070 to 0.69113, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 0.8993 - accuracy: 0.6911 - top-5-accuracy: 0.9421 - val_loss: 0.3861 - val_accuracy: 0.8494 - val_top-5-accuracy: 0.9847 - lr: 5.0000e-05\n",
      "Epoch 175/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8875 - accuracy: 0.6943 - top-5-accuracy: 0.9489\n",
      "Epoch 175: accuracy improved from 0.69113 to 0.69432, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 281ms/step - loss: 0.8875 - accuracy: 0.6943 - top-5-accuracy: 0.9489 - val_loss: 0.4547 - val_accuracy: 0.8374 - val_top-5-accuracy: 0.9770 - lr: 5.0000e-05\n",
      "Epoch 176/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8768 - accuracy: 0.6975 - top-5-accuracy: 0.9495\n",
      "Epoch 176: accuracy improved from 0.69432 to 0.69751, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.8768 - accuracy: 0.6975 - top-5-accuracy: 0.9495 - val_loss: 0.3937 - val_accuracy: 0.8340 - val_top-5-accuracy: 0.9821 - lr: 5.0000e-05\n",
      "Epoch 177/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8821 - accuracy: 0.6935 - top-5-accuracy: 0.9472\n",
      "Epoch 177: accuracy did not improve from 0.69751\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 0.8821 - accuracy: 0.6935 - top-5-accuracy: 0.9472 - val_loss: 0.4523 - val_accuracy: 0.8170 - val_top-5-accuracy: 0.9813 - lr: 5.0000e-05\n",
      "Epoch 178/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8750 - accuracy: 0.6986 - top-5-accuracy: 0.9484\n",
      "Epoch 178: accuracy improved from 0.69751 to 0.69857, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.8750 - accuracy: 0.6986 - top-5-accuracy: 0.9484 - val_loss: 0.3413 - val_accuracy: 0.8570 - val_top-5-accuracy: 0.9847 - lr: 5.0000e-05\n",
      "Epoch 179/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8718 - accuracy: 0.6996 - top-5-accuracy: 0.9493\n",
      "Epoch 179: accuracy improved from 0.69857 to 0.69964, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 0.8718 - accuracy: 0.6996 - top-5-accuracy: 0.9493 - val_loss: 0.4198 - val_accuracy: 0.8213 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 180/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8464 - accuracy: 0.7073 - top-5-accuracy: 0.9518\n",
      "Epoch 180: accuracy improved from 0.69964 to 0.70730, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 0.8464 - accuracy: 0.7073 - top-5-accuracy: 0.9518 - val_loss: 0.3645 - val_accuracy: 0.8434 - val_top-5-accuracy: 0.9830 - lr: 5.0000e-05\n",
      "Epoch 181/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8537 - accuracy: 0.7002 - top-5-accuracy: 0.9517\n",
      "Epoch 181: accuracy did not improve from 0.70730\n",
      "147/147 [==============================] - 44s 277ms/step - loss: 0.8537 - accuracy: 0.7002 - top-5-accuracy: 0.9517 - val_loss: 0.3740 - val_accuracy: 0.8477 - val_top-5-accuracy: 0.9830 - lr: 5.0000e-05\n",
      "Epoch 182/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8438 - accuracy: 0.7072 - top-5-accuracy: 0.9502\n",
      "Epoch 182: accuracy did not improve from 0.70730\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 0.8438 - accuracy: 0.7072 - top-5-accuracy: 0.9502 - val_loss: 0.3585 - val_accuracy: 0.8536 - val_top-5-accuracy: 0.9872 - lr: 5.0000e-05\n",
      "Epoch 183/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8388 - accuracy: 0.7093 - top-5-accuracy: 0.9517\n",
      "Epoch 183: accuracy improved from 0.70730 to 0.70932, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 0.8388 - accuracy: 0.7093 - top-5-accuracy: 0.9517 - val_loss: 0.3672 - val_accuracy: 0.8621 - val_top-5-accuracy: 0.9872 - lr: 5.0000e-05\n",
      "Epoch 184/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8165 - accuracy: 0.7148 - top-5-accuracy: 0.9551\n",
      "Epoch 184: accuracy improved from 0.70932 to 0.71485, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 280ms/step - loss: 0.8165 - accuracy: 0.7148 - top-5-accuracy: 0.9551 - val_loss: 0.3778 - val_accuracy: 0.8562 - val_top-5-accuracy: 0.9821 - lr: 5.0000e-05\n",
      "Epoch 185/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.7128 - top-5-accuracy: 0.9547\n",
      "Epoch 185: accuracy did not improve from 0.71485\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 0.8221 - accuracy: 0.7128 - top-5-accuracy: 0.9547 - val_loss: 0.3565 - val_accuracy: 0.8655 - val_top-5-accuracy: 0.9855 - lr: 5.0000e-05\n",
      "Epoch 186/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8177 - accuracy: 0.7124 - top-5-accuracy: 0.9553\n",
      "Epoch 186: accuracy did not improve from 0.71485\n",
      "147/147 [==============================] - 44s 276ms/step - loss: 0.8177 - accuracy: 0.7124 - top-5-accuracy: 0.9553 - val_loss: 0.3638 - val_accuracy: 0.8536 - val_top-5-accuracy: 0.9872 - lr: 5.0000e-05\n",
      "Epoch 187/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.7183 - top-5-accuracy: 0.9545\n",
      "Epoch 187: accuracy improved from 0.71485 to 0.71825, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 44s 278ms/step - loss: 0.8122 - accuracy: 0.7183 - top-5-accuracy: 0.9545 - val_loss: 0.3225 - val_accuracy: 0.8766 - val_top-5-accuracy: 0.9881 - lr: 5.0000e-05\n",
      "Epoch 188/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.8061 - accuracy: 0.7215 - top-5-accuracy: 0.9565\n",
      "Epoch 188: accuracy improved from 0.71825 to 0.72155, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 279ms/step - loss: 0.8061 - accuracy: 0.7215 - top-5-accuracy: 0.9565 - val_loss: 0.3406 - val_accuracy: 0.8596 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 189/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7920 - accuracy: 0.7225 - top-5-accuracy: 0.9558\n",
      "Epoch 189: accuracy improved from 0.72155 to 0.72251, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.7920 - accuracy: 0.7225 - top-5-accuracy: 0.9558 - val_loss: 0.3430 - val_accuracy: 0.8468 - val_top-5-accuracy: 0.9855 - lr: 5.0000e-05\n",
      "Epoch 190/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7870 - accuracy: 0.7188 - top-5-accuracy: 0.9588\n",
      "Epoch 190: accuracy did not improve from 0.72251\n",
      "147/147 [==============================] - 45s 278ms/step - loss: 0.7870 - accuracy: 0.7188 - top-5-accuracy: 0.9588 - val_loss: 0.3492 - val_accuracy: 0.8698 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 191/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.7258 - top-5-accuracy: 0.9592\n",
      "Epoch 191: accuracy improved from 0.72251 to 0.72580, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 46s 286ms/step - loss: 0.7791 - accuracy: 0.7258 - top-5-accuracy: 0.9592 - val_loss: 0.3218 - val_accuracy: 0.8528 - val_top-5-accuracy: 0.9881 - lr: 5.0000e-05\n",
      "Epoch 192/500\n",
      "147/147 [==============================] - ETA: 0s - loss: 0.7653 - accuracy: 0.7271 - top-5-accuracy: 0.9612\n",
      "Epoch 192: accuracy improved from 0.72580 to 0.72708, saving model to checkpoints\\transformer_128_32_12_8_64_6_[1536]_32_(128, 128, 3)_256_[128, 64]_checkpoint.h5\n",
      "147/147 [==============================] - 45s 283ms/step - loss: 0.7653 - accuracy: 0.7271 - top-5-accuracy: 0.9612 - val_loss: 0.3602 - val_accuracy: 0.8570 - val_top-5-accuracy: 0.9838 - lr: 5.0000e-05\n",
      "Epoch 193/500\n",
      " 42/147 [=======>......................] - ETA: 22s - loss: 0.7543 - accuracy: 0.7385 - top-5-accuracy: 0.9583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\AI\\AnimeClass\\tf_model.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39mEXTRACTOR_MODEL \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtransformer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=37'>38</a>\u001b[0m   loss\u001b[39m=\u001b[39mloss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=38'>39</a>\u001b[0m   optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=42'>43</a>\u001b[0m   ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=43'>44</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=44'>45</a>\u001b[0m train(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=45'>46</a>\u001b[0m   mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=46'>47</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=47'>48</a>\u001b[0m   model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=48'>49</a>\u001b[0m   check_name\u001b[39m=\u001b[39;49mcheck_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=49'>50</a>\u001b[0m   type_model\u001b[39m=\u001b[39;49mEXTRACTOR_MODEL,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=50'>51</a>\u001b[0m   val_ds\u001b[39m=\u001b[39;49mvalid_ds\u001b[39m.\u001b[39;49mbatch(BATCH_SIZE),\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=51'>52</a>\u001b[0m   train_ds\u001b[39m=\u001b[39;49mtrain_ds\u001b[39m.\u001b[39;49mbatch(BATCH_SIZE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000018?line=52'>53</a>\u001b[0m )\n",
      "\u001b[1;32me:\\AI\\AnimeClass\\tf_model.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_ds, val_ds, check_name, epochs, mode, type_model)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=163'>164</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=164'>165</a>\u001b[0m   \u001b[39m#ReleaseMemory(),\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=165'>166</a>\u001b[0m   ReduceLROnPlateau(verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=175'>176</a>\u001b[0m   TensorBoard(log_dir\u001b[39m=\u001b[39mlogdir, histogram_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=176'>177</a>\u001b[0m ]\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=178'>179</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=179'>180</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=180'>181</a>\u001b[0m   train_ds,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=181'>182</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=182'>183</a>\u001b[0m   callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=183'>184</a>\u001b[0m   validation_data\u001b[39m=\u001b[39;49mval_ds\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=184'>185</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=185'>186</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/AI/AnimeClass/tf_model.ipynb#ch0000014?line=186'>187</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal Training Time: \u001b[39m\u001b[39m{\u001b[39;00mend_time\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\tfAnime\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///e%3A/Anaconda/envs/tfAnime/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.000050, # 0.000020 0.000100 _ 0.000105 0.000050 0.000020\n",
    "  clipnorm=1.0\n",
    ")\n",
    "\n",
    "EXTRACTOR_MODEL = 'transformer' # inception transformer resnet\n",
    "conv_params = {\n",
    "  'num_classes': len(class_array),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': EXTRACTOR_MODEL,\n",
    "  'units': 1024,\n",
    "  'inner_layers': 3\n",
    "}\n",
    "vit_params = {\n",
    "  'transformer_layers': 12,\n",
    "  'patch_size': 8, # 8_\n",
    "  'projection_dim': 64, # 128_\n",
    "  'num_heads': 6, # 2_\n",
    "  'mlp_head_units': [ 1536 ], # [ 1024 ]_ [ 1024, 1024 ]\n",
    "  'num_classes': len(class_array),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "}\n",
    "vit_params['num_patches'] = (SIZE_IMG // vit_params['patch_size']) ** 2\n",
    "vit_params['transformer_units'] = [\n",
    "  vit_params['projection_dim'] * 2,\n",
    "  vit_params['projection_dim']\n",
    "]\n",
    "\n",
    "model, model_ident, check_name = build_model(\n",
    "  vit_params if EXTRACTOR_MODEL == 'transformer' else conv_params,\n",
    "  EXTRACTOR_MODEL\n",
    ")\n",
    "model.built = True\n",
    "TOP_ACC = 5\n",
    "BATCH_SIZE = 64\n",
    "INITIAL_EPOCHS = 193\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=EXTRACTOR_MODEL == 'transformer')\n",
    "model.compile(\n",
    "  loss=loss,\n",
    "  optimizer=optimizer,\n",
    "  metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(TOP_ACC, name=f'top-{TOP_ACC}-accuracy'),\n",
    "  ],\n",
    ")\n",
    "train(\n",
    "  mode='fit', \n",
    "  epochs=500,\n",
    "  model=model,\n",
    "  check_name=check_name,\n",
    "  type_model=EXTRACTOR_MODEL,\n",
    "  initial_epoch=INITIAL_EPOCHS,\n",
    "  val_ds=valid_ds.batch(BATCH_SIZE),\n",
    "  train_ds=train_ds.batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, SIZE_IMG, SIZE_IMG, 3)\n",
    "model.expand(input_shape).summary()\n",
    "tf.keras.utils.plot_model(model.expand(input_shape), rankdir='TB', show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of vector result: (2048,)\n",
    "EVA_CLASS = 32\n",
    "USE_FACES = False\n",
    "EVA_MODEL_CLASS = 32\n",
    "MAX_VEC_LEN = 1024 * 8\n",
    "SUFIX = '_faces' if USE_FACES else ''\n",
    "class_eva_path = f'./data/class_array{\"_faces\" if USE_FACES else \"\"}_{EVA_CLASS}.pkl'\n",
    "class_array_eva = pkl.load(open(class_eva_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTOR_MODEL = 'resnet'\n",
    "\n",
    "conv_eval = {\n",
    "  'num_classes':  EVA_MODEL_CLASS,\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': EVA_TYPE,\n",
    "  'units': EVA_UNITS,\n",
    "  'inner_layers': EVA_INNER\n",
    "}\n",
    "\n",
    "vit_eval = {\n",
    "  'transformer_layers':  1,\n",
    "  'patch_size': 8,\n",
    "  'projection_dim': 128,\n",
    "  'num_heads': 2,\n",
    "  'mlp_head_units': [ 1024 ],\n",
    "  'num_classes': len(class_array_eva),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "}\n",
    "vit_params['num_patches'] = (SIZE_IMG // vit_params['patch_size']) ** 2\n",
    "vit_params['transformer_units'] = [\n",
    "  vit_params['projection_dim'] * 2,\n",
    "  vit_params['projection_dim']\n",
    "]\n",
    "\n",
    "model_eva, model_ident, check_name  = build_model(\n",
    "  vit_params if EXTRACTOR_MODEL == 'transformer' else conv_eval,\n",
    "  EXTRACTOR_MODEL\n",
    ")\n",
    "model_eva.build((None, SIZE_IMG, SIZE_IMG, 3))\n",
    "model_eva.load_weights(check_name.replace('checkpoints', 'models').replace('_checkpoint', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_record_eva = test_ds.batch(256)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for images, label in tf_record_eva:\n",
    "  preds = model_eva.predict(images)\n",
    "  all_labels.extend(label)\n",
    "  all_preds.extend(np.argmax(preds, axis=1))\n",
    "del tf_record_eva\n",
    "\n",
    "confusion_matrix= tf.math.confusion_matrix(all_labels, all_preds, num_classes=EVA_CLASS)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.heatmap(confusion_matrix.numpy(), annot=True, cmap='Blues')\n",
    "plt.savefig(f'./metrics/confusion_matrix_{model_ident}{SUFIX}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_cpu(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def cosine_similarity_cpum(u, v):\n",
    "  u_dot_v = np.sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = np.sqrt(np.sum(u*u))\n",
    "  mod_v = np.sqrt(np.sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tf(a, b):\n",
    "  return tf.tensordot(a, b, axes=1) / (tf.norm(a) * tf.norm(b))\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tfm(u, v):\n",
    "  u_dot_v = tf.reduce_sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = tf.sqrt(tf.reduce_sum(u*u))\n",
    "  mod_v = tf.sqrt(tf.reduce_sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "@numba.guvectorize([\"void(float64[:], float64[:], float64[:])\"], \"(n),(n)->()\", target='parallel', fastmath =True)\n",
    "def fast_cosine_gufunc(u, v, result):\n",
    "    m = u.shape[0]\n",
    "    udotv = 0\n",
    "    u_norm = 0\n",
    "    v_norm = 0\n",
    "    for i in range(m):\n",
    "        if (np.isnan(u[i])) or (np.isnan(v[i])):\n",
    "            continue\n",
    "\n",
    "        udotv += u[i] * v[i]\n",
    "        u_norm += u[i] * u[i]\n",
    "        v_norm += v[i] * v[i]\n",
    "\n",
    "    u_norm = np.sqrt(u_norm)\n",
    "    v_norm = np.sqrt(v_norm)\n",
    "\n",
    "    if (u_norm == 0) or (v_norm == 0):\n",
    "        ratio = 1.0\n",
    "    else:\n",
    "        ratio = udotv / (u_norm * v_norm)\n",
    "    result[:] = ratio\n",
    "\n",
    "@numba.jit(nopython=False, parallel=True)\n",
    "def cosine_similarity_numba(u, v):\n",
    "  uv = 0\n",
    "  uu = 0\n",
    "  vv = 0\n",
    "  \n",
    "  for i in range(u.shape[0]):\n",
    "    uv += u[i]*v[i]\n",
    "    uu += u[i]*u[i]\n",
    "    vv += v[i]*v[i]\n",
    "  cos_theta = 1\n",
    "  \n",
    "  if uu != 0 and vv != 0:\n",
    "    cos_theta = uv / np.sqrt(uu*vv)\n",
    "  return cos_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_vec(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'class_name': tf.io.FixedLenFeature([], tf.string),\n",
    "  })\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  y_train = x['class_name']\n",
    "  if y_train is None:\n",
    "    y_train = ''\n",
    "\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset_vec(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord_vec(x, size))\n",
    "\n",
    "def parse_record_vec(combination):\n",
    "  item_1, item_2 = combination\n",
    "  img_1, label_1 = item_1\n",
    "  img_2, label_2 = item_2\n",
    "  return (img_1, img_2, label_1 == label_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_record_vec = test_ds\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "print(f'Total number of images or test: {all_ds_len}')\n",
    "\n",
    "all_combinations = list(itertools.combinations(tf_record_vec, 2))\n",
    "all_combinations = list(map(parse_record_vec, all_combinations))\n",
    "rd.shuffle(all_combinations)\n",
    "del tf_record_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(combinations, model):\n",
    "  result = []\n",
    "  len_combs = MAX_VEC_LEN\n",
    "  for idx, item in enumerate(combinations):\n",
    "    img_1, img_2, label = item\n",
    "    pred_1, pred_2 = model.vectorize(np.array([img_1, img_2]))\n",
    "    #cos_sim = cosine_similarity_cpu(pred_1, pred_2)\n",
    "    pred_1 = np.array(pred_1)\n",
    "    pred_2 = np.array(pred_2)\n",
    "    cos_sim = cosine_similarity_numba(pred_1, pred_2)\n",
    "    result.append((cos_sim, label))\n",
    "    if (idx % 200) == 0 or idx == (len_combs - 1):\n",
    "      print(f'{idx + 1}/{len_combs} - {round(((idx + 1) / len_combs) * 100, 2)}%')\n",
    "  return result\n",
    "\n",
    "result = calculate_cosine_similarity(all_combinations[:MAX_VEC_LEN], model_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['cos_sim', 'label']\n",
    "df.label = df.label.astype(bool)\n",
    "\n",
    "current_f1 = -1\n",
    "metrics = {}\n",
    "for threshold in range(0, 100, 1):\n",
    "  pred_label = df.apply(lambda x: True if x.cos_sim > (threshold/100) else False, axis=1)\n",
    "  precision_result = sk_metrics.precision_score(df.label, pred_label) # Impact when the model predict to lot False positives\n",
    "  recall_result = sk_metrics.recall_score(df.label, pred_label) # Impact when the model predict to lot False negatives\n",
    "  f1_result = sk_metrics.f1_score(df.label, pred_label) # Impact when the model predict to lot False positives and False negatives\n",
    "  if f1_result > current_f1:\n",
    "    current_f1 = f1_result\n",
    "    metrics['precision'] = precision_result\n",
    "    metrics['recall'] = recall_result\n",
    "    metrics['f1'] = f1_result\n",
    "    metrics['threshold'] = threshold / 100\n",
    "\n",
    "print(metrics)\n",
    "json.dump(metrics, open(f'./metrics/metrics_{model_ident}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embeddings with Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "TFRECORD_PATH_VEC = './data/anime_faces_data_32.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = tf_record_vec.reduce(0, lambda x,_: x+1).numpy()\n",
    "LEN_TF_RECORD = all_ds_len if True else MAX_VEC_LEN\n",
    "print(LEN_TF_RECORD)\n",
    "\n",
    "tf_record_vec = tf_record_vec.take(LEN_TF_RECORD)\n",
    "#tf_record_vec = tf_record_vec.shuffle(LEN_TF_RECORD, seed=SEED)\n",
    "\n",
    "data_image = []\n",
    "BATCH = 64\n",
    "for idx, batch in enumerate(tf_record_vec.batch(BATCH)):\n",
    "  imgs, class_names = batch\n",
    "  pred_vecs = model_eva.vectorize(imgs)\n",
    "  pred_vecs = pred_vecs.numpy().astype('float32')\n",
    "  class_names = class_names.numpy().astype('str')\n",
    "  data_image.extend(list(zip(pred_vecs, class_names)))\n",
    "  print(f'{idx + 1}/{LEN_TF_RECORD // BATCH} - {round(((idx + 1) / (LEN_TF_RECORD // BATCH)) * 100, 2)}%')\n",
    "  #img, class_name = item\n",
    "  #pred_vec = model_eva.vectorize(np.array([img]))[0]\n",
    "  #data_item = (\n",
    "  #  pred_vec.numpy().astype('float32'),\n",
    "  #  class_name.numpy().decode('utf-8')\n",
    "  #)\n",
    "  #data_image.append(data_item)\n",
    "  #print(f'{idx + 1}/{LEN_TF_RECORD} - {round(((idx + 1) / LEN_TF_RECORD) * 100, 2)}%')\n",
    "\n",
    "pkl.dump(np.array(data_image), open(f'./data/data_image_{all_ds_len}{SUFIX}.pkl', 'wb'))\n",
    "del data_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_image = pkl.load(open(f'./data/data_image_{4007}{SUFIX}.pkl', 'rb'))\n",
    "vector_images = np.array(list(data_image[:, 0])) \n",
    "\n",
    "d = 2048 #Shape of vector result: (2048,)\n",
    "nb = 4007\n",
    "res = faiss.StandardGpuResources()  # use a single GPU\n",
    "\n",
    "# build a flat (CPU) index\n",
    "index_flat = faiss.IndexFlatL2(d)\n",
    "# make it into a gpu index\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index_flat.add(vector_images)\n",
    "print(f'Vectors {gpu_index_flat.ntotal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 # we want to see 4 nearest neighbors\n",
    "idx_search = 670\n",
    "print(f'Target: ', data_image[idx_search][1])\n",
    "\n",
    "dimension_vec, ids_result = gpu_index_flat.search(vector_images[idx_search: idx_search + 1], k) # actual search\n",
    "\n",
    "current_id = 1\n",
    "result = {}\n",
    "for idx, id_v in enumerate(ids_result[0]):\n",
    "  class_name = data_image[id_v][1]\n",
    "  if class_name not in result:\n",
    "    result[class_name] = 0\n",
    "  result[class_name] += 1\n",
    "\n",
    "print(f'\\nResult: {result}')\n",
    "print(f'\\nOnly name: {\", \".join(list(result.keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20 # we want to see 4 nearest neighbors\n",
    "idx_search = 670\n",
    "izumi = load_img(\"./images/unnamed.jpg\", target_size=(224, 224))\n",
    "izumi_2 = load_img(\"./images/izumi____98..png\", target_size=(224, 224))\n",
    "miyamura = load_img(\"./images/miyamura____127..png\", target_size=(224, 224))\n",
    "aika = load_img(\"./images/aika____10..png\", target_size=(224, 224))\n",
    "\n",
    "aika = np.array(aika)\n",
    "izumi = np.array(izumi)\n",
    "miyamura = np.array(miyamura)\n",
    "izumi_2 = np.array(izumi_2)\n",
    "\n",
    "images = preprocess_input(np.array([izumi, izumi_2, miyamura, aika]), mode='tf')\n",
    "\n",
    "result = model_eva.vectorize(images)\n",
    "result = result.numpy().astype('float32')\n",
    "\n",
    "dimension_vec, ids_result = gpu_index_flat.search(result[0:1], k) # actual search\n",
    "\n",
    "current_id = 1\n",
    "result = {}\n",
    "for idx, id_v in enumerate(ids_result[0]):\n",
    "  class_name = data_image[id_v][1]\n",
    "  if class_name not in result:\n",
    "    result[class_name] = 0\n",
    "  result[class_name] += 1\n",
    "\n",
    "print(f'\\nResult: {result}')\n",
    "print(f'\\nOnly name: {\", \".join(list(result.keys()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalaute time TF vs Numba vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "results = model_16.predict(images)\n",
    "iterations = 1000\n",
    "\n",
    "cpu_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  cpu_r.append(cosine_similarity_cpum(tg_vector, results))\n",
    "  #for reuslt in results:\n",
    "  #  cpu_r.append(cosine_similarity_cpu(reuslt, reuslt))\n",
    "end = time.time()\n",
    "print(f'Time to compute on CPU: {end - start}')\n",
    "\n",
    "numba_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  numba_r.append(fast_cosine_gufunc(results, tg_vector))\n",
    "end = time.time()\n",
    "print(f'Time to compute on Numba: {end - start}')\n",
    "\n",
    "images_gpu = [tf.convert_to_tensor(result) for result in results]\n",
    "tf_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = images_gpu[0]\n",
    "  tf_r.append(cosine_similarity_tfm(tg_vector, results))\n",
    "  #for reuslt in images_gpu:\n",
    "  #  tf_r.append(cosine_similarity_tf(result, images_gpu[0]))\n",
    "end = time.time()\n",
    "print(f'Time to compute on TF: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeClassifier(\n",
    "  num_classes=len(class_array),\n",
    "  input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "  type_extractor='vgg',\n",
    "  units=UNITS,\n",
    "  inner_layers=1\n",
    ")\n",
    "model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "PATH_BEST = './models/vgg_16class_1024_units_aqr.h5'\n",
    "model.load_weights(PATH_BEST)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vector and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "izumi = load_img(\"./images/izumi____110..png\", target_size=(224, 224))\n",
    "izumi_2 = load_img(\"./images/izumi____98..png\", target_size=(224, 224))\n",
    "miyamura = load_img(\"./images/miyamura____127..png\", target_size=(224, 224))\n",
    "aika = load_img(\"./images/aika____10..png\", target_size=(224, 224))\n",
    "aika_ext = load_img(\"./images/Aika_Teen.webp\", target_size=(224, 224))\n",
    "\n",
    "aika = np.array(aika)\n",
    "izumi = np.array(izumi)\n",
    "miyamura = np.array(miyamura)\n",
    "izumi_2 = np.array(izumi_2)\n",
    "aika_ext = np.array(aika_ext)\n",
    "\n",
    "images = preprocess_input(np.array([izumi, izumi_2, miyamura, aika, aika_ext]), mode='tf')\n",
    "\n",
    "izumi_v, izumi_2_v, miyamura_v, aika_v, aika_ext_v= model_eva.vectorize(images)\n",
    "print(f'Shape of vectors: {izumi_v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_numba(np.array(aika_ext_v), np.array(aika_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_numba(np.array(db1_v), np.array(db2_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg = pd.read_csv('./data/run-scalars_best_32_class_1024_train-tag-epoch_accuracy.csv')\n",
    "df_res = pd.read_csv('./data/run-scalars_besT_32_class_1024_resnet_train-tag-epoch_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg.columns = ['Wall time', 'Step', 'Accuracy']\n",
    "df_res.columns = ['Wall time', 'Step', 'Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plor line with legent\n",
    "#smooth line 0.6\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x='Step', y='Accuracy', data=df_vgg, label='VGG', ci=None)\n",
    "sns.lineplot(x='Step', y='Accuracy', data=df_res, label='ResNet', ci=None)\n",
    "plt.legend()\n",
    "#font szie 18 x y\n",
    "plt.xlabel('Step', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "#font legend size 18\n",
    "plt.legend(fontsize=16)\n",
    "#Save\n",
    "plt.savefig('./images/accuracy_plot.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATCH TEST\n",
    "generate_patch_layer = GeneratePatch(patch_size=PATCH_SIZE)\n",
    "\n",
    "image = load_img('./images/db1.jpg', target_size=(224, 224))\n",
    "image = np.array(image)\n",
    "image = preprocess_input(image, mode='tf')\n",
    "\n",
    "patches = generate_patch_layer(np.array([image]))\n",
    "print ('patch per image and patches shape: ', patches.shape[1], '\\n', patches.shape)\n",
    "\n",
    "render_image_and_patches(np.array([image]), patches, PATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54fc5817759acf8623396abca65217b207b5805927b1176ef1418a5fdb9e7b65"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tfAnime')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
