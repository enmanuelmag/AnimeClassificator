{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import i2v\n",
    "import time\n",
    "import json\n",
    "import numba\n",
    "import hashlib\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from numba import vectorize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk_metrics\n",
    "from alive_progress import alive_bar\n",
    "from notifier import Notifier, notify\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import pickle as pkl\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "SEED = 42\n",
    "SIZE_IMG = 224 #224#224#416\n",
    "UNITS = 1024 #2048 1024 128 256 512-seq\n",
    "MAX_CLASS = 16 #32 16 8\n",
    "\n",
    "DATASET_PATH = './data/animes'\n",
    "DATASET_FACES_PATH = './data/faces'\n",
    "CLASS_ARRAY_PATH = f'./data/class_array_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_VEC_PATH = f'./data/class_array_vec_{MAX_CLASS}.pkl'\n",
    "\n",
    "DATASET_JSON_PATH = './data/anime_data.json'\n",
    "\n",
    "AMOUNT_TABLE_PATH = './data/anime_amount.pkl'\n",
    "AMOUNT_FACES_TABLE_PATH = './data/faces_amount.pkl'\n",
    "\n",
    "DATASET_JSON_RANK = './data/anime_rank.json'\n",
    "\n",
    "TFRECORD_PATH = f'./data/anime_data_{MAX_CLASS}.tfrecord'\n",
    "TFRECORD_FACES_PATH = './data/anime_faces_data_{MAX_CLASS}.tfrecord'\n",
    "\n",
    "TG_ID = \"293701727\"\n",
    "TG_TOKEN = \"1878628343:AAEFVRsqDz63ycmaLOFS7gvsG969wdAsJ0w\"\n",
    "WEBHOOK_URL = \"https://discord.com/api/webhooks/796406472459288616/PAkiGGwqe0_PwtBxXYQvOzbk78B4RQP6VWRkvpBtw6Av0sc_mDa3saaIlwVPFjOIeIbt\"\n",
    "\n",
    "if False:\n",
    "  DATASET_PATH = DATASET_FACES_PATH\n",
    "  TFRECORD_PATH = TFRECORD_FACES_PATH\n",
    "  AMOUNT_TABLE_PATH = AMOUNT_FACES_TABLE_PATH\n",
    "\n",
    "#seed random seed to 42 for reproducibility\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_image(url, anime_name, idx):\n",
    "  #download image from url\n",
    "  file_path = f'./data/animes/{anime_name}____{idx}.jpg' \n",
    "  if os.path.exists(file_path):\n",
    "    return\n",
    "\n",
    "  img_data = requests.get(url).content\n",
    "  with open(file_path, 'wb') as handler:\n",
    "    handler.write(img_data)\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Anime images',\n",
    "  msg='Finished downloading anime images'\n",
    ")\n",
    "def get_images(data):\n",
    "  #with alive_bar(len(data)) as bar:\n",
    "  for idx_a, anime_name in enumerate(data):\n",
    "    urls = data[anime_name]\n",
    "    for idx, url in enumerate(urls):\n",
    "      if idx >= 400:\n",
    "        break\n",
    "      name_clean = re.sub(r'_+', r'_', re.sub(r'[\\W\\s]', r'_', anime_name))\n",
    "      try:\n",
    "        dowload_image(url['image'], name_clean, idx)\n",
    "      except Exception as e:\n",
    "        print(f'Error on download image {idx + 1} of {anime_name}')\n",
    "        pass\n",
    "    #bar()\n",
    "    print(f'Progress: {idx_a + 1}/{len(data)} - {round((idx_a + 1)/len(data)*100, 2)}%')\n",
    "\n",
    "def get_classes_anime(path):\n",
    "  classes = set()\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    classes.add(class_name)\n",
    "  return list(classes)\n",
    "\n",
    "def wait_for_it(driver, xpath, timeout=3):\n",
    "  try:\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "def iter_post(driver):\n",
    "  anime_data = []\n",
    "\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = True\n",
    "\n",
    "  while next_button is not None:\n",
    "    if len(anime_data) > 400:\n",
    "      break\n",
    "    ul_element = wait_for_it(driver, '//ul[@id=\"post-list-posts\"]')\n",
    "    if ul_element is None:\n",
    "      next_button = wait_for_it(driver, xpath_next)\n",
    "      if next_button is not None:\n",
    "        next_button.click()\n",
    "        time.sleep(1)\n",
    "      continue\n",
    "    for i, li_element in enumerate(ul_element.find_elements(By.TAG_NAME, 'li')):\n",
    "      a_video = li_element.find_element(By.XPATH, './a').get_attribute('href')\n",
    "      a_image = li_element.find_element(By.XPATH, './div/a/img').get_attribute('src')\n",
    "      anime_data.append({\n",
    "        'video': a_video,\n",
    "        'image': a_image\n",
    "      })\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "    if next_button is not None:\n",
    "      next_button.click()\n",
    "      time.sleep(rd.randint(1, 2))\n",
    "  return anime_data\n",
    "\n",
    "def get_images_links(url, driver, anime_name):\n",
    "  url_search = url + anime_name\n",
    "  driver.get(url_search)\n",
    "  return iter_post(driver)\n",
    "\n",
    "def get_names(driver):\n",
    "  names = []\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = wait_for_it(driver, xpath_next)\n",
    "  \n",
    "  while next_button is not None:\n",
    "    for tr_element in driver.find_elements(By.XPATH, '//table[@class=\"highlightable\"]/tbody/tr'):\n",
    "      try:\n",
    "        amount_post = tr_element.find_element(By.XPATH, './td[1]').text\n",
    "        amount_post = int(amount_post)\n",
    "        if amount_post >= 10:\n",
    "          a_name = tr_element.find_element(By.XPATH, './td[2]/a[2]' ).text\n",
    "          names.append(a_name)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    next_button.click()\n",
    "    time.sleep(rd.randint(1, 2))\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "  return names\n",
    "\n",
    "def get_score(anime_name, driver):\n",
    "  url_search = f'https://myanimelist.net/anime.php?cat=anime&q={anime_name}'\n",
    "  driver.get(url_search)\n",
    "  score = 0\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    score += 1\n",
    "  return score\n",
    "\n",
    "def relevant_anime(anime_name, df_anime, amount_table, threshold=350, rank=True):\n",
    "  \n",
    "  if amount_table.get(anime_name, 0) <= threshold:\n",
    "    return False\n",
    "\n",
    "  if not rank:\n",
    "    return True\n",
    "\n",
    "  anime_name = re.sub(r'_', r' ', anime_name)\n",
    "  df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "\n",
    "  if df_result.empty:\n",
    "    anime_name = ' '.join(anime_name.split(' ')[:3])\n",
    "    df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "  return not df_result.empty\n",
    "\n",
    "def amount_anime_table(datapath):\n",
    "  dic = {}\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    dic[class_name] = dic.get(class_name, 0) + 1\n",
    "  return dic\n",
    "\n",
    "def detect(filename, cascade_file):\n",
    "  if not os.path.isfile(cascade_file):\n",
    "    raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "  cascade = cv2.CascadeClassifier(cascade_file)\n",
    "  image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray = cv2.equalizeHist(gray)\n",
    "  \n",
    "  faces = cascade.detectMultiScale(gray,\n",
    "    scaleFactor = 1.1,\n",
    "    minNeighbors = 5,\n",
    "    minSize = (24, 24)\n",
    "  )\n",
    "\n",
    "  new_images = []\n",
    "  #create a copy of the original but cropped faces\n",
    "  for (x, y, w, h) in faces:\n",
    "    new_images.append(image[y:y+h, x:x+w])\n",
    "\n",
    "  return new_images\n",
    "\n",
    "def extract_faces(datapath):\n",
    "  faces_amount = 0\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    new_images = []\n",
    "    try:\n",
    "      new_images = detect(datapath + '/' + filename, './data/haar/lbpcascade_animeface.xml')\n",
    "    except:\n",
    "      pass\n",
    "    if len(new_images) > 0:\n",
    "      for idx, img in enumerate(new_images):\n",
    "        new_face_name = f'./data/faces/{class_name}____{idx}.jpg'\n",
    "        try:\n",
    "          if not os.path.exists(new_face_name):\n",
    "            cv2.imwrite(new_face_name, img)\n",
    "            faces_amount += 1\n",
    "        except:\n",
    "          pass\n",
    "  \n",
    "  print(f'Faces amount: {faces_amount}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get anime images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data = json.load(open(DATASET_JSON_PATH))\n",
    "get_images(anime_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract only faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_faces(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the amount of images per anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = amount_anime_table(DATASET_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_anime_rank.pkl')\n",
    "\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "\n",
    "class_array = set()\n",
    "for anime_name in all_class_array:\n",
    "  if relevant_anime(anime_name, df, amount_table, threshold=100, rank=True):\n",
    "    class_array.add((anime_name, amount_table[anime_name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)}')\n",
    "del all_class_array\n",
    "\n",
    "class_array = [x[0] for x in class_array]\n",
    "pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for faces\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "\n",
    "sort_table = sorted(all_class_array, key=lambda x: amount_table[x], reverse=True)\n",
    "sort_table = sort_table[:256]\n",
    "class_array = set()\n",
    "\n",
    "for name in all_class_array:\n",
    "  if name in sort_table:\n",
    "    class_array.add(name)\n",
    "\n",
    "class_array = list(class_array)\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(class_name):\n",
    "  return class_array.index(class_name)\n",
    "\n",
    "def build_example(path_file, class_name):\n",
    "  img_array = open(path_file, 'rb').read()\n",
    "  \n",
    "  #img = load_img(path_file, target_size=(SIZE_IMG, SIZE_IMG))\n",
    "  #img_array = np.array(img)\n",
    "  #img_array = preprocess_input(img_array, mode='tf')\n",
    "  #key = hashlib.sha256(img_array).hexdigest()\n",
    "  example = tf.train.Example(\n",
    "    features=tf.train.Features(feature={\n",
    "    #'key': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf-8')])),\n",
    "    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array])),\n",
    "    #'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array.tobytes()])),\n",
    "    'class_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[get_class_id(class_name)])),\n",
    "    'class_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[class_name.encode('utf-8')])),\n",
    "    'filepath': tf.train.Feature(bytes_list=tf.train.BytesList(value=[path_file.encode('utf-8')]))\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "def create_tfrecord(data_path, withe_list, path_tfrecord):\n",
    "  files = os.listdir(data_path)\n",
    "  writer = tf.io.TFRecordWriter(path_tfrecord)\n",
    "  \n",
    "  print('Started creating tfrecord')\n",
    "  for idx, filename in enumerate(files):\n",
    "    class_name, _ = filename.split('____')\n",
    "  \n",
    "    if class_name in withe_list:\n",
    "      path_file = os.path.join(data_path, filename)\n",
    "      tf_example = build_example(path_file, class_name)\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "  print('Finished creating tfrecord')\n",
    "  writer.close()\n",
    "\n",
    "def parse_tfrecord(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  #class_id = tf.sparse.to_dense(x['class_id'], default_value=-1)\n",
    "  class_id = x['class_id']\n",
    "  if class_id is None:\n",
    "    class_id = -1\n",
    "\n",
    "  labels = tf.cast(class_id, tf.int64)\n",
    "  y_train = labels\n",
    "  #y_train = tf.stack([ labels ], axis=1)\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord(x, size))\n",
    "\n",
    "IMAGE_FEATURE_MAP = {\n",
    "  'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  'class_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "if False:\n",
    "  class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "  if os.path.exists(TFRECORD_PATH):\n",
    "    os.remove(TFRECORD_PATH)\n",
    "  create_tfrecord(DATASET_PATH, class_array, TFRECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, input_shape, units, type_extractor = 'vgg') -> tf.keras.Model:\n",
    "  if type_extractor == 'vgg':\n",
    "    feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'inception':\n",
    "    feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'resnet':\n",
    "    feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  else:\n",
    "    raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(tf.keras.layers.Input(input_shape, name='input'))\n",
    "  model.add(feature_extractor)\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  #new\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "  return model\n",
    "\n",
    "class AnimeClassifier(tf.keras.Model):\n",
    "  def __init__(self, num_classes, input_shape, units=1024, inner_layers=12, type_extractor='vgg'):\n",
    "    assert type_extractor in ['vgg', 'inception', 'resnet']\n",
    "    assert inner_layers >= 1\n",
    "    assert num_classes >= 2\n",
    "    assert len(input_shape) == 3\n",
    "    assert units >= 64\n",
    "\n",
    "    super(AnimeClassifier, self).__init__(name='AnimeClassifier')\n",
    "\n",
    "    self.units = units\n",
    "    self.in_layer = tf.keras.layers.Input(input_shape, name='input')\n",
    "\n",
    "    if type_extractor == 'vgg':\n",
    "      feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=self.in_layer)\n",
    "    elif type_extractor == 'inception':\n",
    "      feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif type_extractor == 'resnet':\n",
    "      feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "      raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.global_average_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    self.hidden_mlp = []\n",
    "    for i in range(inner_layers):\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
    "\n",
    "    self.out_layer = tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax)\n",
    "\n",
    "  def call(self, inputs, training=None, mask=None):\n",
    "    x = self.feature_extractor(inputs, training=training)\n",
    "    x = self.global_average_pooling(x)\n",
    "    x = self.flatten(x, training=training)\n",
    "    for layer in self.hidden_mlp:\n",
    "      x = layer(x, training=training)\n",
    "    return self.out_layer(x, training=training)\n",
    "\n",
    "  def predict_classes(self, x):\n",
    "    return tf.argmax(self(x), axis=1)\n",
    "\n",
    "  def vectorize(self, x, flatten=True):\n",
    "    x = self.feature_extractor(x)\n",
    "    x = self.global_average_pooling(x)\n",
    "    if flatten:\n",
    "      return self.flatten(x)\n",
    "    return x\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Train model',\n",
    "  msg='Training has finished'\n",
    ")\n",
    "def train(model, train_ds, val_ds, units, epochs=15, mode='fit', type_model='vgg', save_weights_only=False):\n",
    "  logdir = \"logs/scalars/\" + time.strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "  #logdir = \"logs/scalars/\" + \"test_replicated_seed_5\"\n",
    "  if mode == 'eager_tf':\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "      for batch, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(images, training=True)\n",
    "          regularization_loss = tf.reduce_sum(model.losses)\n",
    "          pred_loss = []\n",
    "          for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "          total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"{}_train_{}, {}, {}\".format(\n",
    "          epoch, batch, total_loss.numpy(),\n",
    "          list(map(lambda x: np.sum(x.numpy()), pred_loss))\n",
    "        ))\n",
    "        avg_loss.update_state(total_loss)\n",
    "  elif mode == 'fit':\n",
    "    callbacks = [\n",
    "      ReduceLROnPlateau(verbose=1),\n",
    "      #EarlyStopping(patience=8, verbose=1),\n",
    "      ModelCheckpoint(\n",
    "        f'checkpoints/{type_model}_{units}_units_aqr_{MAX_CLASS}.h5', \n",
    "        verbose=1,\n",
    "        monitor='accuracy',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=save_weights_only,\n",
    "      ),\n",
    "      TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_data=val_ds\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Total Training Time: {end_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_record = load_tfrecord_dataset(TFRECORD_PATH, SIZE_IMG) #TFRECORD_PATH\n",
    "all_ds_len = sum(1 for _ in tf_record)\n",
    "print(f'Total number of images: {all_ds_len}')\n",
    "\n",
    "#len_mini = all_ds_len\n",
    "#mini_tf_record = tf_record.take(len_mini)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_valid = int(all_ds_len * 0.1)\n",
    "n_test = all_ds_len - n_train - n_valid\n",
    "\n",
    "tf_record = tf_record.shuffle(n_train + n_valid + n_test, seed=SEED)\n",
    "train_ds = tf_record.take(n_train)\n",
    "valid_ds = tf_record.skip(n_train).take(n_valid)\n",
    "test_ds = tf_record.skip(n_train + n_valid).take(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Acc** |  **LR** | **Epochs** | **Batch** | **Units** | **Layers** | **Class** | Passed |\n",
    "|:-------:|:-------:|:----------:|:---------:|:---------:|:----------:|:---------:|:--------:|\n",
    "|    0.975 | 0.00001 |        300 |        32 |      1024 |          1 |         8 | - |\n",
    "|    0.925 | 0.000025|        300 |        32 |      1024 |          1 |        16 | x |\n",
    "|    0.903 | 0.000025|        300 |        32 |      1024 |          1 |        32 | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000025, clipnorm=1.0) #0.000025 0.00001\n",
    "#0.00001 - 300 epochs - 32 batch - 1024 units (1 layers) - 8 class - best\n",
    "#0.00001 - 150 epochs - 32 - batch - 512 units (12 layers) - 8 class - prev\n",
    "\n",
    "model = None\n",
    "vanilla_model = False\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "\n",
    "\n",
    "if vanilla_model:\n",
    "  model = create_model(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor='vgg',\n",
    "    units=UNITS\n",
    "  )\n",
    "else:\n",
    "  model = AnimeClassifier(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor='vgg',\n",
    "    units=UNITS,\n",
    "    inner_layers=1\n",
    "  )\n",
    "  model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "train(\n",
    "  model=model,\n",
    "  epochs=300,\n",
    "  units=UNITS,\n",
    "  val_ds=valid_ds.batch(32),\n",
    "  train_ds=train_ds.batch(32),\n",
    "  save_weights_only=False if vanilla_model else True,\n",
    "  mode='fit', type_model='vgg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 32 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 32 classes\n",
    "class_array_32 = pkl.load(open( f'./data/class_array_32.pkl', 'rb'))\n",
    "parmas_32_classes = {\n",
    "  'num_classes':  len(class_array_32),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_32 = AnimeClassifier(**parmas_32_classes)\n",
    "model_32.build(input_shape=(None, *parmas_32_classes['input_shape']))\n",
    "PATH_BEST_32_CLASSES = './models/vgg_32class_1024_units_aqr.h5'\n",
    "model_32.load_weights(PATH_BEST_32_CLASSES)\n",
    "\n",
    "TFRECORD_PATH_32 = './data/anime_data_32.tfrecord'\n",
    "tf_record_32 = load_tfrecord_dataset(TFRECORD_PATH_32, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_32)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_32 = tf_record_32.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_32 = tf_record_32.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_32 = tf_record_32.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_32 = []\n",
    "all_labels_32 = []\n",
    "for images, label in tf_record_32:\n",
    "  preds = model_32.predict(images)\n",
    "  all_labels_32.extend(label)\n",
    "  all_preds_32.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix= tf.math.confusion_matrix(\n",
    "  all_labels_32,\n",
    "  all_preds_32,\n",
    "  num_classes=parmas_32_classes['num_classes']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Confusion matrix - 32 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 16 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 16 classes\n",
    "class_array_16 = pkl.load(open( f'./data/class_array_16.pkl', 'rb'))\n",
    "parmas_16_classes = {\n",
    "  'num_classes': len(class_array_16),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_16 = AnimeClassifier(**parmas_16_classes)\n",
    "model_16.build(input_shape=(None, *parmas_16_classes['input_shape']))\n",
    "#PATH_BEST_16_CLASSES = './models/vgg_16class_1024_units_aqr.h5'\n",
    "PATH_BEST_16_CLASSES = './models/vgg_16class1024_units_aqr_2.h5'\n",
    "model_16.load_weights(PATH_BEST_16_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_PATH_16 = './data/anime_data_16.tfrecord'\n",
    "tf_record_16 = load_tfrecord_dataset(TFRECORD_PATH_16, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_16)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_16 = tf_record_16.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_16 = tf_record_16.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_16 = tf_record_16.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_16 = []\n",
    "all_labels_16 = []\n",
    "for images, label in tf_record_16:\n",
    "  preds = model_16.predict(images)\n",
    "  all_labels_16.extend(label)\n",
    "  all_preds_16.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "  all_labels_16,\n",
    "  all_preds_16,\n",
    "  num_classes=parmas_16_classes['num_classes']\n",
    ")\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('Confusion matrix - 16 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_16,\n",
    "  yticklabels=class_array_16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 8 classes\n",
    "class_array_8 = pkl.load(open( f'./data/class_array_8.pkl', 'rb'))\n",
    "parmas_8_classes = {\n",
    "  'num_classes': len(class_array_8),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_8 = AnimeClassifier(**parmas_8_classes)\n",
    "model_8.build(input_shape=(None, *parmas_8_classes['input_shape']))\n",
    "PATH_BEST_8_CLASSES = './models/vgg_8class_1024_units.h5'\n",
    "model_8.load_weights(PATH_BEST_8_CLASSES)\n",
    "\n",
    "TFRECORD_PATH_8 = './data/anime_data_8.tfrecord'\n",
    "tf_record_8 = load_tfrecord_dataset(TFRECORD_PATH_8, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_8)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_8 = tf_record_8.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_8 = tf_record_8.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_8 = tf_record_8.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_8 = []\n",
    "all_labels_8 = []\n",
    "for images, label in tf_record_8:\n",
    "  preds = model_8.predict(images)\n",
    "  all_labels_8.extend(label)\n",
    "  all_preds_8.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "  all_labels_8,\n",
    "  all_preds_8,\n",
    "  num_classes=parmas_8_classes['num_classes']\n",
    ")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('Confusion matrix - 8 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_cpu(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def cosine_similarity_cpum(u, v):\n",
    "  u_dot_v = np.sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = np.sqrt(np.sum(u*u))\n",
    "  mod_v = np.sqrt(np.sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tf(a, b):\n",
    "  return tf.tensordot(a, b, axes=1) / (tf.norm(a) * tf.norm(b))\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tfm(u, v):\n",
    "  u_dot_v = tf.reduce_sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = tf.sqrt(tf.reduce_sum(u*u))\n",
    "  mod_v = tf.sqrt(tf.reduce_sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "\n",
    "@numba.guvectorize([\"void(float64[:], float64[:], float64[:])\"], \"(n),(n)->()\", target='parallel', fastmath =True)\n",
    "def fast_cosine_gufunc(u, v, result):\n",
    "    m = u.shape[0]\n",
    "    udotv = 0\n",
    "    u_norm = 0\n",
    "    v_norm = 0\n",
    "    for i in range(m):\n",
    "        if (np.isnan(u[i])) or (np.isnan(v[i])):\n",
    "            continue\n",
    "\n",
    "        udotv += u[i] * v[i]\n",
    "        u_norm += u[i] * u[i]\n",
    "        v_norm += v[i] * v[i]\n",
    "\n",
    "    u_norm = np.sqrt(u_norm)\n",
    "    v_norm = np.sqrt(v_norm)\n",
    "\n",
    "    if (u_norm == 0) or (v_norm == 0):\n",
    "        ratio = 1.0\n",
    "    else:\n",
    "        ratio = udotv / (u_norm * v_norm)\n",
    "    result[:] = ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "results = model_16.predict(images)\n",
    "iterations = 1000\n",
    "\n",
    "cpu_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  cpu_r.append(cosine_similarity_cpum(tg_vector, results))\n",
    "  #for reuslt in results:\n",
    "  #  cpu_r.append(cosine_similarity_cpu(reuslt, reuslt))\n",
    "end = time.time()\n",
    "print(f'Time to compute on CPU: {end - start}')\n",
    "\n",
    "numba_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  numba_r.append(fast_cosine_gufunc(results, tg_vector))\n",
    "end = time.time()\n",
    "print(f'Time to compute on Numba: {end - start}')\n",
    "\n",
    "images_gpu = [tf.convert_to_tensor(result) for result in results]\n",
    "tf_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = images_gpu[0]\n",
    "  tf_r.append(cosine_similarity_tfm(tg_vector, results))\n",
    "  #for reuslt in images_gpu:\n",
    "  #  tf_r.append(cosine_similarity_tf(result, images_gpu[0]))\n",
    "end = time.time()\n",
    "print(f'Time to compute on TF: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build vectorize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images or test: 325\n"
     ]
    }
   ],
   "source": [
    "def parse_tfrecord_vec(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'class_name': tf.io.FixedLenFeature([], tf.string),\n",
    "  })\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  y_train = x['class_name']\n",
    "  if y_train is None:\n",
    "    y_train = ''\n",
    "\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset_vec(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord_vec(x, size))\n",
    "\n",
    "\n",
    "TFRECORD_PATH_VEC = './data/anime_data_16.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "\n",
    "n_train = int(all_ds_len * 0.95)\n",
    "n_test = int(all_ds_len * 0.05)\n",
    "\n",
    "tf_record_vec = tf_record_vec.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_vec = tf_record_vec.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = list(itertools.combinations(tf_record_vec, 2))\n",
    "def parse_record_vec(combination):\n",
    "  item_1, item_2 = combination\n",
    "  img_1, label_1 = item_1\n",
    "  img_2, label_2 = item_2\n",
    "  return (img_1, img_2, label_1 == label_2)\n",
    "\n",
    "all_combinations = list(map(parse_record_vec, all_combinations))\n",
    "rd.shuffle(all_combinations)\n",
    "del tf_record_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2048 - 0.05%\n",
      "2/2048 - 0.1%\n",
      "3/2048 - 0.15%\n",
      "4/2048 - 0.2%\n",
      "5/2048 - 0.24%\n",
      "6/2048 - 0.29%\n",
      "7/2048 - 0.34%\n",
      "8/2048 - 0.39%\n",
      "9/2048 - 0.44%\n",
      "10/2048 - 0.49%\n",
      "11/2048 - 0.54%\n",
      "12/2048 - 0.59%\n",
      "13/2048 - 0.63%\n",
      "14/2048 - 0.68%\n",
      "15/2048 - 0.73%\n",
      "16/2048 - 0.78%\n",
      "17/2048 - 0.83%\n",
      "18/2048 - 0.88%\n",
      "19/2048 - 0.93%\n",
      "20/2048 - 0.98%\n",
      "21/2048 - 1.03%\n",
      "22/2048 - 1.07%\n",
      "23/2048 - 1.12%\n",
      "24/2048 - 1.17%\n",
      "25/2048 - 1.22%\n",
      "26/2048 - 1.27%\n",
      "27/2048 - 1.32%\n",
      "28/2048 - 1.37%\n",
      "29/2048 - 1.42%\n",
      "30/2048 - 1.46%\n",
      "31/2048 - 1.51%\n",
      "32/2048 - 1.56%\n",
      "33/2048 - 1.61%\n",
      "34/2048 - 1.66%\n",
      "35/2048 - 1.71%\n",
      "36/2048 - 1.76%\n",
      "37/2048 - 1.81%\n",
      "38/2048 - 1.86%\n",
      "39/2048 - 1.9%\n",
      "40/2048 - 1.95%\n",
      "41/2048 - 2.0%\n",
      "42/2048 - 2.05%\n",
      "43/2048 - 2.1%\n",
      "44/2048 - 2.15%\n",
      "45/2048 - 2.2%\n",
      "46/2048 - 2.25%\n",
      "47/2048 - 2.29%\n",
      "48/2048 - 2.34%\n",
      "49/2048 - 2.39%\n",
      "50/2048 - 2.44%\n",
      "51/2048 - 2.49%\n",
      "52/2048 - 2.54%\n",
      "53/2048 - 2.59%\n",
      "54/2048 - 2.64%\n",
      "55/2048 - 2.69%\n",
      "56/2048 - 2.73%\n",
      "57/2048 - 2.78%\n",
      "58/2048 - 2.83%\n",
      "59/2048 - 2.88%\n",
      "60/2048 - 2.93%\n",
      "61/2048 - 2.98%\n",
      "62/2048 - 3.03%\n",
      "63/2048 - 3.08%\n",
      "64/2048 - 3.12%\n",
      "65/2048 - 3.17%\n",
      "66/2048 - 3.22%\n",
      "67/2048 - 3.27%\n",
      "68/2048 - 3.32%\n",
      "69/2048 - 3.37%\n",
      "70/2048 - 3.42%\n",
      "71/2048 - 3.47%\n",
      "72/2048 - 3.52%\n",
      "73/2048 - 3.56%\n",
      "74/2048 - 3.61%\n",
      "75/2048 - 3.66%\n",
      "76/2048 - 3.71%\n",
      "77/2048 - 3.76%\n",
      "78/2048 - 3.81%\n",
      "79/2048 - 3.86%\n",
      "80/2048 - 3.91%\n",
      "81/2048 - 3.96%\n",
      "82/2048 - 4.0%\n",
      "83/2048 - 4.05%\n",
      "84/2048 - 4.1%\n",
      "85/2048 - 4.15%\n",
      "86/2048 - 4.2%\n",
      "87/2048 - 4.25%\n",
      "88/2048 - 4.3%\n",
      "89/2048 - 4.35%\n",
      "90/2048 - 4.39%\n",
      "91/2048 - 4.44%\n",
      "92/2048 - 4.49%\n",
      "93/2048 - 4.54%\n",
      "94/2048 - 4.59%\n",
      "95/2048 - 4.64%\n",
      "96/2048 - 4.69%\n",
      "97/2048 - 4.74%\n",
      "98/2048 - 4.79%\n",
      "99/2048 - 4.83%\n",
      "100/2048 - 4.88%\n",
      "101/2048 - 4.93%\n",
      "102/2048 - 4.98%\n",
      "103/2048 - 5.03%\n",
      "104/2048 - 5.08%\n",
      "105/2048 - 5.13%\n",
      "106/2048 - 5.18%\n",
      "107/2048 - 5.22%\n",
      "108/2048 - 5.27%\n",
      "109/2048 - 5.32%\n",
      "110/2048 - 5.37%\n",
      "111/2048 - 5.42%\n",
      "112/2048 - 5.47%\n",
      "113/2048 - 5.52%\n",
      "114/2048 - 5.57%\n",
      "115/2048 - 5.62%\n",
      "116/2048 - 5.66%\n",
      "117/2048 - 5.71%\n",
      "118/2048 - 5.76%\n",
      "119/2048 - 5.81%\n",
      "120/2048 - 5.86%\n",
      "121/2048 - 5.91%\n",
      "122/2048 - 5.96%\n",
      "123/2048 - 6.01%\n",
      "124/2048 - 6.05%\n",
      "125/2048 - 6.1%\n",
      "126/2048 - 6.15%\n",
      "127/2048 - 6.2%\n",
      "128/2048 - 6.25%\n",
      "129/2048 - 6.3%\n",
      "130/2048 - 6.35%\n",
      "131/2048 - 6.4%\n",
      "132/2048 - 6.45%\n",
      "133/2048 - 6.49%\n",
      "134/2048 - 6.54%\n",
      "135/2048 - 6.59%\n",
      "136/2048 - 6.64%\n",
      "137/2048 - 6.69%\n",
      "138/2048 - 6.74%\n",
      "139/2048 - 6.79%\n",
      "140/2048 - 6.84%\n",
      "141/2048 - 6.88%\n",
      "142/2048 - 6.93%\n",
      "143/2048 - 6.98%\n",
      "144/2048 - 7.03%\n",
      "145/2048 - 7.08%\n",
      "146/2048 - 7.13%\n",
      "147/2048 - 7.18%\n",
      "148/2048 - 7.23%\n",
      "149/2048 - 7.28%\n",
      "150/2048 - 7.32%\n",
      "151/2048 - 7.37%\n",
      "152/2048 - 7.42%\n",
      "153/2048 - 7.47%\n",
      "154/2048 - 7.52%\n",
      "155/2048 - 7.57%\n",
      "156/2048 - 7.62%\n",
      "157/2048 - 7.67%\n",
      "158/2048 - 7.71%\n",
      "159/2048 - 7.76%\n",
      "160/2048 - 7.81%\n",
      "161/2048 - 7.86%\n",
      "162/2048 - 7.91%\n",
      "163/2048 - 7.96%\n",
      "164/2048 - 8.01%\n",
      "165/2048 - 8.06%\n",
      "166/2048 - 8.11%\n",
      "167/2048 - 8.15%\n",
      "168/2048 - 8.2%\n",
      "169/2048 - 8.25%\n",
      "170/2048 - 8.3%\n",
      "171/2048 - 8.35%\n",
      "172/2048 - 8.4%\n",
      "173/2048 - 8.45%\n",
      "174/2048 - 8.5%\n",
      "175/2048 - 8.54%\n",
      "176/2048 - 8.59%\n",
      "177/2048 - 8.64%\n",
      "178/2048 - 8.69%\n",
      "179/2048 - 8.74%\n",
      "180/2048 - 8.79%\n",
      "181/2048 - 8.84%\n",
      "182/2048 - 8.89%\n",
      "183/2048 - 8.94%\n",
      "184/2048 - 8.98%\n",
      "185/2048 - 9.03%\n",
      "186/2048 - 9.08%\n",
      "187/2048 - 9.13%\n",
      "188/2048 - 9.18%\n",
      "189/2048 - 9.23%\n",
      "190/2048 - 9.28%\n",
      "191/2048 - 9.33%\n",
      "192/2048 - 9.38%\n",
      "193/2048 - 9.42%\n",
      "194/2048 - 9.47%\n",
      "195/2048 - 9.52%\n",
      "196/2048 - 9.57%\n",
      "197/2048 - 9.62%\n",
      "198/2048 - 9.67%\n",
      "199/2048 - 9.72%\n",
      "200/2048 - 9.77%\n",
      "201/2048 - 9.81%\n",
      "202/2048 - 9.86%\n",
      "203/2048 - 9.91%\n",
      "204/2048 - 9.96%\n",
      "205/2048 - 10.01%\n",
      "206/2048 - 10.06%\n",
      "207/2048 - 10.11%\n",
      "208/2048 - 10.16%\n",
      "209/2048 - 10.21%\n",
      "210/2048 - 10.25%\n",
      "211/2048 - 10.3%\n",
      "212/2048 - 10.35%\n",
      "213/2048 - 10.4%\n",
      "214/2048 - 10.45%\n",
      "215/2048 - 10.5%\n",
      "216/2048 - 10.55%\n",
      "217/2048 - 10.6%\n",
      "218/2048 - 10.64%\n",
      "219/2048 - 10.69%\n",
      "220/2048 - 10.74%\n",
      "221/2048 - 10.79%\n",
      "222/2048 - 10.84%\n",
      "223/2048 - 10.89%\n",
      "224/2048 - 10.94%\n",
      "225/2048 - 10.99%\n",
      "226/2048 - 11.04%\n",
      "227/2048 - 11.08%\n",
      "228/2048 - 11.13%\n",
      "229/2048 - 11.18%\n",
      "230/2048 - 11.23%\n",
      "231/2048 - 11.28%\n",
      "232/2048 - 11.33%\n",
      "233/2048 - 11.38%\n",
      "234/2048 - 11.43%\n",
      "235/2048 - 11.47%\n",
      "236/2048 - 11.52%\n",
      "237/2048 - 11.57%\n",
      "238/2048 - 11.62%\n",
      "239/2048 - 11.67%\n",
      "240/2048 - 11.72%\n",
      "241/2048 - 11.77%\n",
      "242/2048 - 11.82%\n",
      "243/2048 - 11.87%\n",
      "244/2048 - 11.91%\n",
      "245/2048 - 11.96%\n",
      "246/2048 - 12.01%\n",
      "247/2048 - 12.06%\n",
      "248/2048 - 12.11%\n",
      "249/2048 - 12.16%\n",
      "250/2048 - 12.21%\n",
      "251/2048 - 12.26%\n",
      "252/2048 - 12.3%\n",
      "253/2048 - 12.35%\n",
      "254/2048 - 12.4%\n",
      "255/2048 - 12.45%\n",
      "256/2048 - 12.5%\n",
      "257/2048 - 12.55%\n",
      "258/2048 - 12.6%\n",
      "259/2048 - 12.65%\n",
      "260/2048 - 12.7%\n",
      "261/2048 - 12.74%\n",
      "262/2048 - 12.79%\n",
      "263/2048 - 12.84%\n",
      "264/2048 - 12.89%\n",
      "265/2048 - 12.94%\n",
      "266/2048 - 12.99%\n",
      "267/2048 - 13.04%\n",
      "268/2048 - 13.09%\n",
      "269/2048 - 13.13%\n",
      "270/2048 - 13.18%\n",
      "271/2048 - 13.23%\n",
      "272/2048 - 13.28%\n",
      "273/2048 - 13.33%\n",
      "274/2048 - 13.38%\n",
      "275/2048 - 13.43%\n",
      "276/2048 - 13.48%\n",
      "277/2048 - 13.53%\n",
      "278/2048 - 13.57%\n",
      "279/2048 - 13.62%\n",
      "280/2048 - 13.67%\n",
      "281/2048 - 13.72%\n",
      "282/2048 - 13.77%\n",
      "283/2048 - 13.82%\n",
      "284/2048 - 13.87%\n",
      "285/2048 - 13.92%\n",
      "286/2048 - 13.96%\n",
      "287/2048 - 14.01%\n",
      "288/2048 - 14.06%\n",
      "289/2048 - 14.11%\n",
      "290/2048 - 14.16%\n",
      "291/2048 - 14.21%\n",
      "292/2048 - 14.26%\n",
      "293/2048 - 14.31%\n",
      "294/2048 - 14.36%\n",
      "295/2048 - 14.4%\n",
      "296/2048 - 14.45%\n",
      "297/2048 - 14.5%\n",
      "298/2048 - 14.55%\n",
      "299/2048 - 14.6%\n",
      "300/2048 - 14.65%\n",
      "301/2048 - 14.7%\n",
      "302/2048 - 14.75%\n",
      "303/2048 - 14.79%\n",
      "304/2048 - 14.84%\n",
      "305/2048 - 14.89%\n",
      "306/2048 - 14.94%\n",
      "307/2048 - 14.99%\n",
      "308/2048 - 15.04%\n",
      "309/2048 - 15.09%\n",
      "310/2048 - 15.14%\n",
      "311/2048 - 15.19%\n",
      "312/2048 - 15.23%\n",
      "313/2048 - 15.28%\n",
      "314/2048 - 15.33%\n",
      "315/2048 - 15.38%\n",
      "316/2048 - 15.43%\n",
      "317/2048 - 15.48%\n",
      "318/2048 - 15.53%\n",
      "319/2048 - 15.58%\n",
      "320/2048 - 15.62%\n",
      "321/2048 - 15.67%\n",
      "322/2048 - 15.72%\n",
      "323/2048 - 15.77%\n",
      "324/2048 - 15.82%\n",
      "325/2048 - 15.87%\n",
      "326/2048 - 15.92%\n",
      "327/2048 - 15.97%\n",
      "328/2048 - 16.02%\n",
      "329/2048 - 16.06%\n",
      "330/2048 - 16.11%\n",
      "331/2048 - 16.16%\n",
      "332/2048 - 16.21%\n",
      "333/2048 - 16.26%\n",
      "334/2048 - 16.31%\n",
      "335/2048 - 16.36%\n",
      "336/2048 - 16.41%\n",
      "337/2048 - 16.46%\n",
      "338/2048 - 16.5%\n",
      "339/2048 - 16.55%\n",
      "340/2048 - 16.6%\n",
      "341/2048 - 16.65%\n",
      "342/2048 - 16.7%\n",
      "343/2048 - 16.75%\n",
      "344/2048 - 16.8%\n",
      "345/2048 - 16.85%\n",
      "346/2048 - 16.89%\n",
      "347/2048 - 16.94%\n",
      "348/2048 - 16.99%\n",
      "349/2048 - 17.04%\n",
      "350/2048 - 17.09%\n",
      "351/2048 - 17.14%\n",
      "352/2048 - 17.19%\n",
      "353/2048 - 17.24%\n",
      "354/2048 - 17.29%\n",
      "355/2048 - 17.33%\n",
      "356/2048 - 17.38%\n",
      "357/2048 - 17.43%\n",
      "358/2048 - 17.48%\n",
      "359/2048 - 17.53%\n",
      "360/2048 - 17.58%\n",
      "361/2048 - 17.63%\n",
      "362/2048 - 17.68%\n",
      "363/2048 - 17.72%\n",
      "364/2048 - 17.77%\n",
      "365/2048 - 17.82%\n",
      "366/2048 - 17.87%\n",
      "367/2048 - 17.92%\n",
      "368/2048 - 17.97%\n",
      "369/2048 - 18.02%\n",
      "370/2048 - 18.07%\n",
      "371/2048 - 18.12%\n",
      "372/2048 - 18.16%\n",
      "373/2048 - 18.21%\n",
      "374/2048 - 18.26%\n",
      "375/2048 - 18.31%\n",
      "376/2048 - 18.36%\n",
      "377/2048 - 18.41%\n",
      "378/2048 - 18.46%\n",
      "379/2048 - 18.51%\n",
      "380/2048 - 18.55%\n",
      "381/2048 - 18.6%\n",
      "382/2048 - 18.65%\n",
      "383/2048 - 18.7%\n",
      "384/2048 - 18.75%\n",
      "385/2048 - 18.8%\n",
      "386/2048 - 18.85%\n",
      "387/2048 - 18.9%\n",
      "388/2048 - 18.95%\n",
      "389/2048 - 18.99%\n",
      "390/2048 - 19.04%\n",
      "391/2048 - 19.09%\n",
      "392/2048 - 19.14%\n",
      "393/2048 - 19.19%\n",
      "394/2048 - 19.24%\n",
      "395/2048 - 19.29%\n",
      "396/2048 - 19.34%\n",
      "397/2048 - 19.38%\n",
      "398/2048 - 19.43%\n",
      "399/2048 - 19.48%\n",
      "400/2048 - 19.53%\n",
      "401/2048 - 19.58%\n",
      "402/2048 - 19.63%\n",
      "403/2048 - 19.68%\n",
      "404/2048 - 19.73%\n",
      "405/2048 - 19.78%\n",
      "406/2048 - 19.82%\n",
      "407/2048 - 19.87%\n",
      "408/2048 - 19.92%\n",
      "409/2048 - 19.97%\n",
      "410/2048 - 20.02%\n",
      "411/2048 - 20.07%\n",
      "412/2048 - 20.12%\n",
      "413/2048 - 20.17%\n",
      "414/2048 - 20.21%\n",
      "415/2048 - 20.26%\n",
      "416/2048 - 20.31%\n",
      "417/2048 - 20.36%\n",
      "418/2048 - 20.41%\n",
      "419/2048 - 20.46%\n",
      "420/2048 - 20.51%\n",
      "421/2048 - 20.56%\n",
      "422/2048 - 20.61%\n",
      "423/2048 - 20.65%\n",
      "424/2048 - 20.7%\n",
      "425/2048 - 20.75%\n",
      "426/2048 - 20.8%\n",
      "427/2048 - 20.85%\n",
      "428/2048 - 20.9%\n",
      "429/2048 - 20.95%\n",
      "430/2048 - 21.0%\n",
      "431/2048 - 21.04%\n",
      "432/2048 - 21.09%\n",
      "433/2048 - 21.14%\n",
      "434/2048 - 21.19%\n",
      "435/2048 - 21.24%\n",
      "436/2048 - 21.29%\n",
      "437/2048 - 21.34%\n",
      "438/2048 - 21.39%\n",
      "439/2048 - 21.44%\n",
      "440/2048 - 21.48%\n",
      "441/2048 - 21.53%\n",
      "442/2048 - 21.58%\n",
      "443/2048 - 21.63%\n",
      "444/2048 - 21.68%\n",
      "445/2048 - 21.73%\n",
      "446/2048 - 21.78%\n",
      "447/2048 - 21.83%\n",
      "448/2048 - 21.88%\n",
      "449/2048 - 21.92%\n",
      "450/2048 - 21.97%\n",
      "451/2048 - 22.02%\n",
      "452/2048 - 22.07%\n",
      "453/2048 - 22.12%\n",
      "454/2048 - 22.17%\n",
      "455/2048 - 22.22%\n",
      "456/2048 - 22.27%\n",
      "457/2048 - 22.31%\n",
      "458/2048 - 22.36%\n",
      "459/2048 - 22.41%\n",
      "460/2048 - 22.46%\n",
      "461/2048 - 22.51%\n",
      "462/2048 - 22.56%\n",
      "463/2048 - 22.61%\n",
      "464/2048 - 22.66%\n",
      "465/2048 - 22.71%\n",
      "466/2048 - 22.75%\n",
      "467/2048 - 22.8%\n",
      "468/2048 - 22.85%\n",
      "469/2048 - 22.9%\n",
      "470/2048 - 22.95%\n",
      "471/2048 - 23.0%\n",
      "472/2048 - 23.05%\n",
      "473/2048 - 23.1%\n",
      "474/2048 - 23.14%\n",
      "475/2048 - 23.19%\n",
      "476/2048 - 23.24%\n",
      "477/2048 - 23.29%\n",
      "478/2048 - 23.34%\n",
      "479/2048 - 23.39%\n",
      "480/2048 - 23.44%\n",
      "481/2048 - 23.49%\n",
      "482/2048 - 23.54%\n",
      "483/2048 - 23.58%\n",
      "484/2048 - 23.63%\n",
      "485/2048 - 23.68%\n",
      "486/2048 - 23.73%\n",
      "487/2048 - 23.78%\n",
      "488/2048 - 23.83%\n",
      "489/2048 - 23.88%\n",
      "490/2048 - 23.93%\n",
      "491/2048 - 23.97%\n",
      "492/2048 - 24.02%\n",
      "493/2048 - 24.07%\n",
      "494/2048 - 24.12%\n",
      "495/2048 - 24.17%\n",
      "496/2048 - 24.22%\n",
      "497/2048 - 24.27%\n",
      "498/2048 - 24.32%\n",
      "499/2048 - 24.37%\n",
      "500/2048 - 24.41%\n",
      "501/2048 - 24.46%\n",
      "502/2048 - 24.51%\n",
      "503/2048 - 24.56%\n",
      "504/2048 - 24.61%\n",
      "505/2048 - 24.66%\n",
      "506/2048 - 24.71%\n",
      "507/2048 - 24.76%\n",
      "508/2048 - 24.8%\n",
      "509/2048 - 24.85%\n",
      "510/2048 - 24.9%\n",
      "511/2048 - 24.95%\n",
      "512/2048 - 25.0%\n",
      "513/2048 - 25.05%\n",
      "514/2048 - 25.1%\n",
      "515/2048 - 25.15%\n",
      "516/2048 - 25.2%\n",
      "517/2048 - 25.24%\n",
      "518/2048 - 25.29%\n",
      "519/2048 - 25.34%\n",
      "520/2048 - 25.39%\n",
      "521/2048 - 25.44%\n",
      "522/2048 - 25.49%\n",
      "523/2048 - 25.54%\n",
      "524/2048 - 25.59%\n",
      "525/2048 - 25.63%\n",
      "526/2048 - 25.68%\n",
      "527/2048 - 25.73%\n",
      "528/2048 - 25.78%\n",
      "529/2048 - 25.83%\n",
      "530/2048 - 25.88%\n",
      "531/2048 - 25.93%\n",
      "532/2048 - 25.98%\n",
      "533/2048 - 26.03%\n",
      "534/2048 - 26.07%\n",
      "535/2048 - 26.12%\n",
      "536/2048 - 26.17%\n",
      "537/2048 - 26.22%\n",
      "538/2048 - 26.27%\n",
      "539/2048 - 26.32%\n",
      "540/2048 - 26.37%\n",
      "541/2048 - 26.42%\n",
      "542/2048 - 26.46%\n",
      "543/2048 - 26.51%\n",
      "544/2048 - 26.56%\n",
      "545/2048 - 26.61%\n",
      "546/2048 - 26.66%\n",
      "547/2048 - 26.71%\n",
      "548/2048 - 26.76%\n",
      "549/2048 - 26.81%\n",
      "550/2048 - 26.86%\n",
      "551/2048 - 26.9%\n",
      "552/2048 - 26.95%\n",
      "553/2048 - 27.0%\n",
      "554/2048 - 27.05%\n",
      "555/2048 - 27.1%\n",
      "556/2048 - 27.15%\n",
      "557/2048 - 27.2%\n",
      "558/2048 - 27.25%\n",
      "559/2048 - 27.29%\n",
      "560/2048 - 27.34%\n",
      "561/2048 - 27.39%\n",
      "562/2048 - 27.44%\n",
      "563/2048 - 27.49%\n",
      "564/2048 - 27.54%\n",
      "565/2048 - 27.59%\n",
      "566/2048 - 27.64%\n",
      "567/2048 - 27.69%\n",
      "568/2048 - 27.73%\n",
      "569/2048 - 27.78%\n",
      "570/2048 - 27.83%\n",
      "571/2048 - 27.88%\n",
      "572/2048 - 27.93%\n",
      "573/2048 - 27.98%\n",
      "574/2048 - 28.03%\n",
      "575/2048 - 28.08%\n",
      "576/2048 - 28.12%\n",
      "577/2048 - 28.17%\n",
      "578/2048 - 28.22%\n",
      "579/2048 - 28.27%\n",
      "580/2048 - 28.32%\n",
      "581/2048 - 28.37%\n",
      "582/2048 - 28.42%\n",
      "583/2048 - 28.47%\n",
      "584/2048 - 28.52%\n",
      "585/2048 - 28.56%\n",
      "586/2048 - 28.61%\n",
      "587/2048 - 28.66%\n",
      "588/2048 - 28.71%\n",
      "589/2048 - 28.76%\n",
      "590/2048 - 28.81%\n",
      "591/2048 - 28.86%\n",
      "592/2048 - 28.91%\n",
      "593/2048 - 28.96%\n",
      "594/2048 - 29.0%\n",
      "595/2048 - 29.05%\n",
      "596/2048 - 29.1%\n",
      "597/2048 - 29.15%\n",
      "598/2048 - 29.2%\n",
      "599/2048 - 29.25%\n",
      "600/2048 - 29.3%\n",
      "601/2048 - 29.35%\n",
      "602/2048 - 29.39%\n",
      "603/2048 - 29.44%\n",
      "604/2048 - 29.49%\n",
      "605/2048 - 29.54%\n",
      "606/2048 - 29.59%\n",
      "607/2048 - 29.64%\n",
      "608/2048 - 29.69%\n",
      "609/2048 - 29.74%\n",
      "610/2048 - 29.79%\n",
      "611/2048 - 29.83%\n",
      "612/2048 - 29.88%\n",
      "613/2048 - 29.93%\n",
      "614/2048 - 29.98%\n",
      "615/2048 - 30.03%\n",
      "616/2048 - 30.08%\n",
      "617/2048 - 30.13%\n",
      "618/2048 - 30.18%\n",
      "619/2048 - 30.22%\n",
      "620/2048 - 30.27%\n",
      "621/2048 - 30.32%\n",
      "622/2048 - 30.37%\n",
      "623/2048 - 30.42%\n",
      "624/2048 - 30.47%\n",
      "625/2048 - 30.52%\n",
      "626/2048 - 30.57%\n",
      "627/2048 - 30.62%\n",
      "628/2048 - 30.66%\n",
      "629/2048 - 30.71%\n",
      "630/2048 - 30.76%\n",
      "631/2048 - 30.81%\n",
      "632/2048 - 30.86%\n",
      "633/2048 - 30.91%\n",
      "634/2048 - 30.96%\n",
      "635/2048 - 31.01%\n",
      "636/2048 - 31.05%\n",
      "637/2048 - 31.1%\n",
      "638/2048 - 31.15%\n",
      "639/2048 - 31.2%\n",
      "640/2048 - 31.25%\n",
      "641/2048 - 31.3%\n",
      "642/2048 - 31.35%\n",
      "643/2048 - 31.4%\n",
      "644/2048 - 31.45%\n",
      "645/2048 - 31.49%\n",
      "646/2048 - 31.54%\n",
      "647/2048 - 31.59%\n",
      "648/2048 - 31.64%\n",
      "649/2048 - 31.69%\n",
      "650/2048 - 31.74%\n",
      "651/2048 - 31.79%\n",
      "652/2048 - 31.84%\n",
      "653/2048 - 31.88%\n",
      "654/2048 - 31.93%\n",
      "655/2048 - 31.98%\n",
      "656/2048 - 32.03%\n",
      "657/2048 - 32.08%\n",
      "658/2048 - 32.13%\n",
      "659/2048 - 32.18%\n",
      "660/2048 - 32.23%\n",
      "661/2048 - 32.28%\n",
      "662/2048 - 32.32%\n",
      "663/2048 - 32.37%\n",
      "664/2048 - 32.42%\n",
      "665/2048 - 32.47%\n",
      "666/2048 - 32.52%\n",
      "667/2048 - 32.57%\n",
      "668/2048 - 32.62%\n",
      "669/2048 - 32.67%\n",
      "670/2048 - 32.71%\n",
      "671/2048 - 32.76%\n",
      "672/2048 - 32.81%\n",
      "673/2048 - 32.86%\n",
      "674/2048 - 32.91%\n",
      "675/2048 - 32.96%\n",
      "676/2048 - 33.01%\n",
      "677/2048 - 33.06%\n",
      "678/2048 - 33.11%\n",
      "679/2048 - 33.15%\n",
      "680/2048 - 33.2%\n",
      "681/2048 - 33.25%\n",
      "682/2048 - 33.3%\n",
      "683/2048 - 33.35%\n",
      "684/2048 - 33.4%\n",
      "685/2048 - 33.45%\n",
      "686/2048 - 33.5%\n",
      "687/2048 - 33.54%\n",
      "688/2048 - 33.59%\n",
      "689/2048 - 33.64%\n",
      "690/2048 - 33.69%\n",
      "691/2048 - 33.74%\n",
      "692/2048 - 33.79%\n",
      "693/2048 - 33.84%\n",
      "694/2048 - 33.89%\n",
      "695/2048 - 33.94%\n",
      "696/2048 - 33.98%\n",
      "697/2048 - 34.03%\n",
      "698/2048 - 34.08%\n",
      "699/2048 - 34.13%\n",
      "700/2048 - 34.18%\n",
      "701/2048 - 34.23%\n",
      "702/2048 - 34.28%\n",
      "703/2048 - 34.33%\n",
      "704/2048 - 34.38%\n",
      "705/2048 - 34.42%\n",
      "706/2048 - 34.47%\n",
      "707/2048 - 34.52%\n",
      "708/2048 - 34.57%\n",
      "709/2048 - 34.62%\n",
      "710/2048 - 34.67%\n",
      "711/2048 - 34.72%\n",
      "712/2048 - 34.77%\n",
      "713/2048 - 34.81%\n",
      "714/2048 - 34.86%\n",
      "715/2048 - 34.91%\n",
      "716/2048 - 34.96%\n",
      "717/2048 - 35.01%\n",
      "718/2048 - 35.06%\n",
      "719/2048 - 35.11%\n",
      "720/2048 - 35.16%\n",
      "721/2048 - 35.21%\n",
      "722/2048 - 35.25%\n",
      "723/2048 - 35.3%\n",
      "724/2048 - 35.35%\n",
      "725/2048 - 35.4%\n",
      "726/2048 - 35.45%\n",
      "727/2048 - 35.5%\n",
      "728/2048 - 35.55%\n",
      "729/2048 - 35.6%\n",
      "730/2048 - 35.64%\n",
      "731/2048 - 35.69%\n",
      "732/2048 - 35.74%\n",
      "733/2048 - 35.79%\n",
      "734/2048 - 35.84%\n",
      "735/2048 - 35.89%\n",
      "736/2048 - 35.94%\n",
      "737/2048 - 35.99%\n",
      "738/2048 - 36.04%\n",
      "739/2048 - 36.08%\n",
      "740/2048 - 36.13%\n",
      "741/2048 - 36.18%\n",
      "742/2048 - 36.23%\n",
      "743/2048 - 36.28%\n",
      "744/2048 - 36.33%\n",
      "745/2048 - 36.38%\n",
      "746/2048 - 36.43%\n",
      "747/2048 - 36.47%\n",
      "748/2048 - 36.52%\n",
      "749/2048 - 36.57%\n",
      "750/2048 - 36.62%\n",
      "751/2048 - 36.67%\n",
      "752/2048 - 36.72%\n",
      "753/2048 - 36.77%\n",
      "754/2048 - 36.82%\n",
      "755/2048 - 36.87%\n",
      "756/2048 - 36.91%\n",
      "757/2048 - 36.96%\n",
      "758/2048 - 37.01%\n",
      "759/2048 - 37.06%\n",
      "760/2048 - 37.11%\n",
      "761/2048 - 37.16%\n",
      "762/2048 - 37.21%\n",
      "763/2048 - 37.26%\n",
      "764/2048 - 37.3%\n",
      "765/2048 - 37.35%\n",
      "766/2048 - 37.4%\n",
      "767/2048 - 37.45%\n",
      "768/2048 - 37.5%\n",
      "769/2048 - 37.55%\n",
      "770/2048 - 37.6%\n",
      "771/2048 - 37.65%\n",
      "772/2048 - 37.7%\n",
      "773/2048 - 37.74%\n",
      "774/2048 - 37.79%\n",
      "775/2048 - 37.84%\n",
      "776/2048 - 37.89%\n",
      "777/2048 - 37.94%\n",
      "778/2048 - 37.99%\n",
      "779/2048 - 38.04%\n",
      "780/2048 - 38.09%\n",
      "781/2048 - 38.13%\n",
      "782/2048 - 38.18%\n",
      "783/2048 - 38.23%\n",
      "784/2048 - 38.28%\n",
      "785/2048 - 38.33%\n",
      "786/2048 - 38.38%\n",
      "787/2048 - 38.43%\n",
      "788/2048 - 38.48%\n",
      "789/2048 - 38.53%\n",
      "790/2048 - 38.57%\n",
      "791/2048 - 38.62%\n",
      "792/2048 - 38.67%\n",
      "793/2048 - 38.72%\n",
      "794/2048 - 38.77%\n",
      "795/2048 - 38.82%\n",
      "796/2048 - 38.87%\n",
      "797/2048 - 38.92%\n",
      "798/2048 - 38.96%\n",
      "799/2048 - 39.01%\n",
      "800/2048 - 39.06%\n",
      "801/2048 - 39.11%\n",
      "802/2048 - 39.16%\n",
      "803/2048 - 39.21%\n",
      "804/2048 - 39.26%\n",
      "805/2048 - 39.31%\n",
      "806/2048 - 39.36%\n",
      "807/2048 - 39.4%\n",
      "808/2048 - 39.45%\n",
      "809/2048 - 39.5%\n",
      "810/2048 - 39.55%\n",
      "811/2048 - 39.6%\n",
      "812/2048 - 39.65%\n",
      "813/2048 - 39.7%\n",
      "814/2048 - 39.75%\n",
      "815/2048 - 39.79%\n",
      "816/2048 - 39.84%\n",
      "817/2048 - 39.89%\n",
      "818/2048 - 39.94%\n",
      "819/2048 - 39.99%\n",
      "820/2048 - 40.04%\n",
      "821/2048 - 40.09%\n",
      "822/2048 - 40.14%\n",
      "823/2048 - 40.19%\n",
      "824/2048 - 40.23%\n",
      "825/2048 - 40.28%\n",
      "826/2048 - 40.33%\n",
      "827/2048 - 40.38%\n",
      "828/2048 - 40.43%\n",
      "829/2048 - 40.48%\n",
      "830/2048 - 40.53%\n",
      "831/2048 - 40.58%\n",
      "832/2048 - 40.62%\n",
      "833/2048 - 40.67%\n",
      "834/2048 - 40.72%\n",
      "835/2048 - 40.77%\n",
      "836/2048 - 40.82%\n",
      "837/2048 - 40.87%\n",
      "838/2048 - 40.92%\n",
      "839/2048 - 40.97%\n",
      "840/2048 - 41.02%\n",
      "841/2048 - 41.06%\n",
      "842/2048 - 41.11%\n",
      "843/2048 - 41.16%\n",
      "844/2048 - 41.21%\n",
      "845/2048 - 41.26%\n",
      "846/2048 - 41.31%\n",
      "847/2048 - 41.36%\n",
      "848/2048 - 41.41%\n",
      "849/2048 - 41.46%\n",
      "850/2048 - 41.5%\n",
      "851/2048 - 41.55%\n",
      "852/2048 - 41.6%\n",
      "853/2048 - 41.65%\n",
      "854/2048 - 41.7%\n",
      "855/2048 - 41.75%\n",
      "856/2048 - 41.8%\n",
      "857/2048 - 41.85%\n",
      "858/2048 - 41.89%\n",
      "859/2048 - 41.94%\n",
      "860/2048 - 41.99%\n",
      "861/2048 - 42.04%\n",
      "862/2048 - 42.09%\n",
      "863/2048 - 42.14%\n",
      "864/2048 - 42.19%\n",
      "865/2048 - 42.24%\n",
      "866/2048 - 42.29%\n",
      "867/2048 - 42.33%\n",
      "868/2048 - 42.38%\n",
      "869/2048 - 42.43%\n",
      "870/2048 - 42.48%\n",
      "871/2048 - 42.53%\n",
      "872/2048 - 42.58%\n",
      "873/2048 - 42.63%\n",
      "874/2048 - 42.68%\n",
      "875/2048 - 42.72%\n",
      "876/2048 - 42.77%\n",
      "877/2048 - 42.82%\n",
      "878/2048 - 42.87%\n",
      "879/2048 - 42.92%\n",
      "880/2048 - 42.97%\n",
      "881/2048 - 43.02%\n",
      "882/2048 - 43.07%\n",
      "883/2048 - 43.12%\n",
      "884/2048 - 43.16%\n",
      "885/2048 - 43.21%\n",
      "886/2048 - 43.26%\n",
      "887/2048 - 43.31%\n",
      "888/2048 - 43.36%\n",
      "889/2048 - 43.41%\n",
      "890/2048 - 43.46%\n",
      "891/2048 - 43.51%\n",
      "892/2048 - 43.55%\n",
      "893/2048 - 43.6%\n",
      "894/2048 - 43.65%\n",
      "895/2048 - 43.7%\n",
      "896/2048 - 43.75%\n",
      "897/2048 - 43.8%\n",
      "898/2048 - 43.85%\n",
      "899/2048 - 43.9%\n",
      "900/2048 - 43.95%\n",
      "901/2048 - 43.99%\n",
      "902/2048 - 44.04%\n",
      "903/2048 - 44.09%\n",
      "904/2048 - 44.14%\n",
      "905/2048 - 44.19%\n",
      "906/2048 - 44.24%\n",
      "907/2048 - 44.29%\n",
      "908/2048 - 44.34%\n",
      "909/2048 - 44.38%\n",
      "910/2048 - 44.43%\n",
      "911/2048 - 44.48%\n",
      "912/2048 - 44.53%\n",
      "913/2048 - 44.58%\n",
      "914/2048 - 44.63%\n",
      "915/2048 - 44.68%\n",
      "916/2048 - 44.73%\n",
      "917/2048 - 44.78%\n",
      "918/2048 - 44.82%\n",
      "919/2048 - 44.87%\n",
      "920/2048 - 44.92%\n",
      "921/2048 - 44.97%\n",
      "922/2048 - 45.02%\n",
      "923/2048 - 45.07%\n",
      "924/2048 - 45.12%\n",
      "925/2048 - 45.17%\n",
      "926/2048 - 45.21%\n",
      "927/2048 - 45.26%\n",
      "928/2048 - 45.31%\n",
      "929/2048 - 45.36%\n",
      "930/2048 - 45.41%\n",
      "931/2048 - 45.46%\n",
      "932/2048 - 45.51%\n",
      "933/2048 - 45.56%\n",
      "934/2048 - 45.61%\n",
      "935/2048 - 45.65%\n",
      "936/2048 - 45.7%\n",
      "937/2048 - 45.75%\n",
      "938/2048 - 45.8%\n",
      "939/2048 - 45.85%\n",
      "940/2048 - 45.9%\n",
      "941/2048 - 45.95%\n",
      "942/2048 - 46.0%\n",
      "943/2048 - 46.04%\n",
      "944/2048 - 46.09%\n",
      "945/2048 - 46.14%\n",
      "946/2048 - 46.19%\n",
      "947/2048 - 46.24%\n",
      "948/2048 - 46.29%\n",
      "949/2048 - 46.34%\n",
      "950/2048 - 46.39%\n",
      "951/2048 - 46.44%\n",
      "952/2048 - 46.48%\n",
      "953/2048 - 46.53%\n",
      "954/2048 - 46.58%\n",
      "955/2048 - 46.63%\n",
      "956/2048 - 46.68%\n",
      "957/2048 - 46.73%\n",
      "958/2048 - 46.78%\n",
      "959/2048 - 46.83%\n",
      "960/2048 - 46.88%\n",
      "961/2048 - 46.92%\n",
      "962/2048 - 46.97%\n",
      "963/2048 - 47.02%\n",
      "964/2048 - 47.07%\n",
      "965/2048 - 47.12%\n",
      "966/2048 - 47.17%\n",
      "967/2048 - 47.22%\n",
      "968/2048 - 47.27%\n",
      "969/2048 - 47.31%\n",
      "970/2048 - 47.36%\n",
      "971/2048 - 47.41%\n",
      "972/2048 - 47.46%\n",
      "973/2048 - 47.51%\n",
      "974/2048 - 47.56%\n",
      "975/2048 - 47.61%\n",
      "976/2048 - 47.66%\n",
      "977/2048 - 47.71%\n",
      "978/2048 - 47.75%\n",
      "979/2048 - 47.8%\n",
      "980/2048 - 47.85%\n",
      "981/2048 - 47.9%\n",
      "982/2048 - 47.95%\n",
      "983/2048 - 48.0%\n",
      "984/2048 - 48.05%\n",
      "985/2048 - 48.1%\n",
      "986/2048 - 48.14%\n",
      "987/2048 - 48.19%\n",
      "988/2048 - 48.24%\n",
      "989/2048 - 48.29%\n",
      "990/2048 - 48.34%\n",
      "991/2048 - 48.39%\n",
      "992/2048 - 48.44%\n",
      "993/2048 - 48.49%\n",
      "994/2048 - 48.54%\n",
      "995/2048 - 48.58%\n",
      "996/2048 - 48.63%\n",
      "997/2048 - 48.68%\n",
      "998/2048 - 48.73%\n",
      "999/2048 - 48.78%\n",
      "1000/2048 - 48.83%\n",
      "1001/2048 - 48.88%\n",
      "1002/2048 - 48.93%\n",
      "1003/2048 - 48.97%\n",
      "1004/2048 - 49.02%\n",
      "1005/2048 - 49.07%\n",
      "1006/2048 - 49.12%\n",
      "1007/2048 - 49.17%\n",
      "1008/2048 - 49.22%\n",
      "1009/2048 - 49.27%\n",
      "1010/2048 - 49.32%\n",
      "1011/2048 - 49.37%\n",
      "1012/2048 - 49.41%\n",
      "1013/2048 - 49.46%\n",
      "1014/2048 - 49.51%\n",
      "1015/2048 - 49.56%\n",
      "1016/2048 - 49.61%\n",
      "1017/2048 - 49.66%\n",
      "1018/2048 - 49.71%\n",
      "1019/2048 - 49.76%\n",
      "1020/2048 - 49.8%\n",
      "1021/2048 - 49.85%\n",
      "1022/2048 - 49.9%\n",
      "1023/2048 - 49.95%\n",
      "1024/2048 - 50.0%\n",
      "1025/2048 - 50.05%\n",
      "1026/2048 - 50.1%\n",
      "1027/2048 - 50.15%\n",
      "1028/2048 - 50.2%\n",
      "1029/2048 - 50.24%\n",
      "1030/2048 - 50.29%\n",
      "1031/2048 - 50.34%\n",
      "1032/2048 - 50.39%\n",
      "1033/2048 - 50.44%\n",
      "1034/2048 - 50.49%\n",
      "1035/2048 - 50.54%\n",
      "1036/2048 - 50.59%\n",
      "1037/2048 - 50.63%\n",
      "1038/2048 - 50.68%\n",
      "1039/2048 - 50.73%\n",
      "1040/2048 - 50.78%\n",
      "1041/2048 - 50.83%\n",
      "1042/2048 - 50.88%\n",
      "1043/2048 - 50.93%\n",
      "1044/2048 - 50.98%\n",
      "1045/2048 - 51.03%\n",
      "1046/2048 - 51.07%\n",
      "1047/2048 - 51.12%\n",
      "1048/2048 - 51.17%\n",
      "1049/2048 - 51.22%\n",
      "1050/2048 - 51.27%\n",
      "1051/2048 - 51.32%\n",
      "1052/2048 - 51.37%\n",
      "1053/2048 - 51.42%\n",
      "1054/2048 - 51.46%\n",
      "1055/2048 - 51.51%\n",
      "1056/2048 - 51.56%\n",
      "1057/2048 - 51.61%\n",
      "1058/2048 - 51.66%\n",
      "1059/2048 - 51.71%\n",
      "1060/2048 - 51.76%\n",
      "1061/2048 - 51.81%\n",
      "1062/2048 - 51.86%\n",
      "1063/2048 - 51.9%\n",
      "1064/2048 - 51.95%\n",
      "1065/2048 - 52.0%\n",
      "1066/2048 - 52.05%\n",
      "1067/2048 - 52.1%\n",
      "1068/2048 - 52.15%\n",
      "1069/2048 - 52.2%\n",
      "1070/2048 - 52.25%\n",
      "1071/2048 - 52.29%\n",
      "1072/2048 - 52.34%\n",
      "1073/2048 - 52.39%\n",
      "1074/2048 - 52.44%\n",
      "1075/2048 - 52.49%\n",
      "1076/2048 - 52.54%\n",
      "1077/2048 - 52.59%\n",
      "1078/2048 - 52.64%\n",
      "1079/2048 - 52.69%\n",
      "1080/2048 - 52.73%\n",
      "1081/2048 - 52.78%\n",
      "1082/2048 - 52.83%\n",
      "1083/2048 - 52.88%\n",
      "1084/2048 - 52.93%\n",
      "1085/2048 - 52.98%\n",
      "1086/2048 - 53.03%\n",
      "1087/2048 - 53.08%\n",
      "1088/2048 - 53.12%\n",
      "1089/2048 - 53.17%\n",
      "1090/2048 - 53.22%\n",
      "1091/2048 - 53.27%\n",
      "1092/2048 - 53.32%\n",
      "1093/2048 - 53.37%\n",
      "1094/2048 - 53.42%\n",
      "1095/2048 - 53.47%\n",
      "1096/2048 - 53.52%\n",
      "1097/2048 - 53.56%\n",
      "1098/2048 - 53.61%\n",
      "1099/2048 - 53.66%\n",
      "1100/2048 - 53.71%\n",
      "1101/2048 - 53.76%\n",
      "1102/2048 - 53.81%\n",
      "1103/2048 - 53.86%\n",
      "1104/2048 - 53.91%\n",
      "1105/2048 - 53.96%\n",
      "1106/2048 - 54.0%\n",
      "1107/2048 - 54.05%\n",
      "1108/2048 - 54.1%\n",
      "1109/2048 - 54.15%\n",
      "1110/2048 - 54.2%\n",
      "1111/2048 - 54.25%\n",
      "1112/2048 - 54.3%\n",
      "1113/2048 - 54.35%\n",
      "1114/2048 - 54.39%\n",
      "1115/2048 - 54.44%\n",
      "1116/2048 - 54.49%\n",
      "1117/2048 - 54.54%\n",
      "1118/2048 - 54.59%\n",
      "1119/2048 - 54.64%\n",
      "1120/2048 - 54.69%\n",
      "1121/2048 - 54.74%\n",
      "1122/2048 - 54.79%\n",
      "1123/2048 - 54.83%\n",
      "1124/2048 - 54.88%\n",
      "1125/2048 - 54.93%\n",
      "1126/2048 - 54.98%\n",
      "1127/2048 - 55.03%\n",
      "1128/2048 - 55.08%\n",
      "1129/2048 - 55.13%\n",
      "1130/2048 - 55.18%\n",
      "1131/2048 - 55.22%\n",
      "1132/2048 - 55.27%\n",
      "1133/2048 - 55.32%\n",
      "1134/2048 - 55.37%\n",
      "1135/2048 - 55.42%\n",
      "1136/2048 - 55.47%\n",
      "1137/2048 - 55.52%\n",
      "1138/2048 - 55.57%\n",
      "1139/2048 - 55.62%\n",
      "1140/2048 - 55.66%\n",
      "1141/2048 - 55.71%\n",
      "1142/2048 - 55.76%\n",
      "1143/2048 - 55.81%\n",
      "1144/2048 - 55.86%\n",
      "1145/2048 - 55.91%\n",
      "1146/2048 - 55.96%\n",
      "1147/2048 - 56.01%\n",
      "1148/2048 - 56.05%\n",
      "1149/2048 - 56.1%\n",
      "1150/2048 - 56.15%\n",
      "1151/2048 - 56.2%\n",
      "1152/2048 - 56.25%\n",
      "1153/2048 - 56.3%\n",
      "1154/2048 - 56.35%\n",
      "1155/2048 - 56.4%\n",
      "1156/2048 - 56.45%\n",
      "1157/2048 - 56.49%\n",
      "1158/2048 - 56.54%\n",
      "1159/2048 - 56.59%\n",
      "1160/2048 - 56.64%\n",
      "1161/2048 - 56.69%\n",
      "1162/2048 - 56.74%\n",
      "1163/2048 - 56.79%\n",
      "1164/2048 - 56.84%\n",
      "1165/2048 - 56.88%\n",
      "1166/2048 - 56.93%\n",
      "1167/2048 - 56.98%\n",
      "1168/2048 - 57.03%\n",
      "1169/2048 - 57.08%\n",
      "1170/2048 - 57.13%\n",
      "1171/2048 - 57.18%\n",
      "1172/2048 - 57.23%\n",
      "1173/2048 - 57.28%\n",
      "1174/2048 - 57.32%\n",
      "1175/2048 - 57.37%\n",
      "1176/2048 - 57.42%\n",
      "1177/2048 - 57.47%\n",
      "1178/2048 - 57.52%\n",
      "1179/2048 - 57.57%\n",
      "1180/2048 - 57.62%\n",
      "1181/2048 - 57.67%\n",
      "1182/2048 - 57.71%\n",
      "1183/2048 - 57.76%\n",
      "1184/2048 - 57.81%\n",
      "1185/2048 - 57.86%\n",
      "1186/2048 - 57.91%\n",
      "1187/2048 - 57.96%\n",
      "1188/2048 - 58.01%\n",
      "1189/2048 - 58.06%\n",
      "1190/2048 - 58.11%\n",
      "1191/2048 - 58.15%\n",
      "1192/2048 - 58.2%\n",
      "1193/2048 - 58.25%\n",
      "1194/2048 - 58.3%\n",
      "1195/2048 - 58.35%\n",
      "1196/2048 - 58.4%\n",
      "1197/2048 - 58.45%\n",
      "1198/2048 - 58.5%\n",
      "1199/2048 - 58.54%\n",
      "1200/2048 - 58.59%\n",
      "1201/2048 - 58.64%\n",
      "1202/2048 - 58.69%\n",
      "1203/2048 - 58.74%\n",
      "1204/2048 - 58.79%\n",
      "1205/2048 - 58.84%\n",
      "1206/2048 - 58.89%\n",
      "1207/2048 - 58.94%\n",
      "1208/2048 - 58.98%\n",
      "1209/2048 - 59.03%\n",
      "1210/2048 - 59.08%\n",
      "1211/2048 - 59.13%\n",
      "1212/2048 - 59.18%\n",
      "1213/2048 - 59.23%\n",
      "1214/2048 - 59.28%\n",
      "1215/2048 - 59.33%\n",
      "1216/2048 - 59.38%\n",
      "1217/2048 - 59.42%\n",
      "1218/2048 - 59.47%\n",
      "1219/2048 - 59.52%\n",
      "1220/2048 - 59.57%\n",
      "1221/2048 - 59.62%\n",
      "1222/2048 - 59.67%\n",
      "1223/2048 - 59.72%\n",
      "1224/2048 - 59.77%\n",
      "1225/2048 - 59.81%\n",
      "1226/2048 - 59.86%\n",
      "1227/2048 - 59.91%\n",
      "1228/2048 - 59.96%\n",
      "1229/2048 - 60.01%\n",
      "1230/2048 - 60.06%\n",
      "1231/2048 - 60.11%\n",
      "1232/2048 - 60.16%\n",
      "1233/2048 - 60.21%\n",
      "1234/2048 - 60.25%\n",
      "1235/2048 - 60.3%\n",
      "1236/2048 - 60.35%\n",
      "1237/2048 - 60.4%\n",
      "1238/2048 - 60.45%\n",
      "1239/2048 - 60.5%\n",
      "1240/2048 - 60.55%\n",
      "1241/2048 - 60.6%\n",
      "1242/2048 - 60.64%\n",
      "1243/2048 - 60.69%\n",
      "1244/2048 - 60.74%\n",
      "1245/2048 - 60.79%\n",
      "1246/2048 - 60.84%\n",
      "1247/2048 - 60.89%\n",
      "1248/2048 - 60.94%\n",
      "1249/2048 - 60.99%\n",
      "1250/2048 - 61.04%\n",
      "1251/2048 - 61.08%\n",
      "1252/2048 - 61.13%\n",
      "1253/2048 - 61.18%\n",
      "1254/2048 - 61.23%\n",
      "1255/2048 - 61.28%\n",
      "1256/2048 - 61.33%\n",
      "1257/2048 - 61.38%\n",
      "1258/2048 - 61.43%\n",
      "1259/2048 - 61.47%\n",
      "1260/2048 - 61.52%\n",
      "1261/2048 - 61.57%\n",
      "1262/2048 - 61.62%\n",
      "1263/2048 - 61.67%\n",
      "1264/2048 - 61.72%\n",
      "1265/2048 - 61.77%\n",
      "1266/2048 - 61.82%\n",
      "1267/2048 - 61.87%\n",
      "1268/2048 - 61.91%\n",
      "1269/2048 - 61.96%\n",
      "1270/2048 - 62.01%\n",
      "1271/2048 - 62.06%\n",
      "1272/2048 - 62.11%\n",
      "1273/2048 - 62.16%\n",
      "1274/2048 - 62.21%\n",
      "1275/2048 - 62.26%\n",
      "1276/2048 - 62.3%\n",
      "1277/2048 - 62.35%\n",
      "1278/2048 - 62.4%\n",
      "1279/2048 - 62.45%\n",
      "1280/2048 - 62.5%\n",
      "1281/2048 - 62.55%\n",
      "1282/2048 - 62.6%\n",
      "1283/2048 - 62.65%\n",
      "1284/2048 - 62.7%\n",
      "1285/2048 - 62.74%\n",
      "1286/2048 - 62.79%\n",
      "1287/2048 - 62.84%\n",
      "1288/2048 - 62.89%\n",
      "1289/2048 - 62.94%\n",
      "1290/2048 - 62.99%\n",
      "1291/2048 - 63.04%\n",
      "1292/2048 - 63.09%\n",
      "1293/2048 - 63.13%\n",
      "1294/2048 - 63.18%\n",
      "1295/2048 - 63.23%\n",
      "1296/2048 - 63.28%\n",
      "1297/2048 - 63.33%\n",
      "1298/2048 - 63.38%\n",
      "1299/2048 - 63.43%\n",
      "1300/2048 - 63.48%\n",
      "1301/2048 - 63.53%\n",
      "1302/2048 - 63.57%\n",
      "1303/2048 - 63.62%\n",
      "1304/2048 - 63.67%\n",
      "1305/2048 - 63.72%\n",
      "1306/2048 - 63.77%\n",
      "1307/2048 - 63.82%\n",
      "1308/2048 - 63.87%\n",
      "1309/2048 - 63.92%\n",
      "1310/2048 - 63.96%\n",
      "1311/2048 - 64.01%\n",
      "1312/2048 - 64.06%\n",
      "1313/2048 - 64.11%\n",
      "1314/2048 - 64.16%\n",
      "1315/2048 - 64.21%\n",
      "1316/2048 - 64.26%\n",
      "1317/2048 - 64.31%\n",
      "1318/2048 - 64.36%\n",
      "1319/2048 - 64.4%\n",
      "1320/2048 - 64.45%\n",
      "1321/2048 - 64.5%\n",
      "1322/2048 - 64.55%\n",
      "1323/2048 - 64.6%\n",
      "1324/2048 - 64.65%\n",
      "1325/2048 - 64.7%\n",
      "1326/2048 - 64.75%\n",
      "1327/2048 - 64.79%\n",
      "1328/2048 - 64.84%\n",
      "1329/2048 - 64.89%\n",
      "1330/2048 - 64.94%\n",
      "1331/2048 - 64.99%\n",
      "1332/2048 - 65.04%\n",
      "1333/2048 - 65.09%\n",
      "1334/2048 - 65.14%\n",
      "1335/2048 - 65.19%\n",
      "1336/2048 - 65.23%\n",
      "1337/2048 - 65.28%\n",
      "1338/2048 - 65.33%\n",
      "1339/2048 - 65.38%\n",
      "1340/2048 - 65.43%\n",
      "1341/2048 - 65.48%\n",
      "1342/2048 - 65.53%\n",
      "1343/2048 - 65.58%\n",
      "1344/2048 - 65.62%\n",
      "1345/2048 - 65.67%\n",
      "1346/2048 - 65.72%\n",
      "1347/2048 - 65.77%\n",
      "1348/2048 - 65.82%\n",
      "1349/2048 - 65.87%\n",
      "1350/2048 - 65.92%\n",
      "1351/2048 - 65.97%\n",
      "1352/2048 - 66.02%\n",
      "1353/2048 - 66.06%\n",
      "1354/2048 - 66.11%\n",
      "1355/2048 - 66.16%\n",
      "1356/2048 - 66.21%\n",
      "1357/2048 - 66.26%\n",
      "1358/2048 - 66.31%\n",
      "1359/2048 - 66.36%\n",
      "1360/2048 - 66.41%\n",
      "1361/2048 - 66.46%\n",
      "1362/2048 - 66.5%\n",
      "1363/2048 - 66.55%\n",
      "1364/2048 - 66.6%\n",
      "1365/2048 - 66.65%\n",
      "1366/2048 - 66.7%\n",
      "1367/2048 - 66.75%\n",
      "1368/2048 - 66.8%\n",
      "1369/2048 - 66.85%\n",
      "1370/2048 - 66.89%\n",
      "1371/2048 - 66.94%\n",
      "1372/2048 - 66.99%\n",
      "1373/2048 - 67.04%\n",
      "1374/2048 - 67.09%\n",
      "1375/2048 - 67.14%\n",
      "1376/2048 - 67.19%\n",
      "1377/2048 - 67.24%\n",
      "1378/2048 - 67.29%\n",
      "1379/2048 - 67.33%\n",
      "1380/2048 - 67.38%\n",
      "1381/2048 - 67.43%\n",
      "1382/2048 - 67.48%\n",
      "1383/2048 - 67.53%\n",
      "1384/2048 - 67.58%\n",
      "1385/2048 - 67.63%\n",
      "1386/2048 - 67.68%\n",
      "1387/2048 - 67.72%\n",
      "1388/2048 - 67.77%\n",
      "1389/2048 - 67.82%\n",
      "1390/2048 - 67.87%\n",
      "1391/2048 - 67.92%\n",
      "1392/2048 - 67.97%\n",
      "1393/2048 - 68.02%\n",
      "1394/2048 - 68.07%\n",
      "1395/2048 - 68.12%\n",
      "1396/2048 - 68.16%\n",
      "1397/2048 - 68.21%\n",
      "1398/2048 - 68.26%\n",
      "1399/2048 - 68.31%\n",
      "1400/2048 - 68.36%\n",
      "1401/2048 - 68.41%\n",
      "1402/2048 - 68.46%\n",
      "1403/2048 - 68.51%\n",
      "1404/2048 - 68.55%\n",
      "1405/2048 - 68.6%\n",
      "1406/2048 - 68.65%\n",
      "1407/2048 - 68.7%\n",
      "1408/2048 - 68.75%\n",
      "1409/2048 - 68.8%\n",
      "1410/2048 - 68.85%\n",
      "1411/2048 - 68.9%\n",
      "1412/2048 - 68.95%\n",
      "1413/2048 - 68.99%\n",
      "1414/2048 - 69.04%\n",
      "1415/2048 - 69.09%\n",
      "1416/2048 - 69.14%\n",
      "1417/2048 - 69.19%\n",
      "1418/2048 - 69.24%\n",
      "1419/2048 - 69.29%\n",
      "1420/2048 - 69.34%\n",
      "1421/2048 - 69.38%\n",
      "1422/2048 - 69.43%\n",
      "1423/2048 - 69.48%\n",
      "1424/2048 - 69.53%\n",
      "1425/2048 - 69.58%\n",
      "1426/2048 - 69.63%\n",
      "1427/2048 - 69.68%\n",
      "1428/2048 - 69.73%\n",
      "1429/2048 - 69.78%\n",
      "1430/2048 - 69.82%\n",
      "1431/2048 - 69.87%\n",
      "1432/2048 - 69.92%\n",
      "1433/2048 - 69.97%\n",
      "1434/2048 - 70.02%\n",
      "1435/2048 - 70.07%\n",
      "1436/2048 - 70.12%\n",
      "1437/2048 - 70.17%\n",
      "1438/2048 - 70.21%\n",
      "1439/2048 - 70.26%\n",
      "1440/2048 - 70.31%\n",
      "1441/2048 - 70.36%\n",
      "1442/2048 - 70.41%\n",
      "1443/2048 - 70.46%\n",
      "1444/2048 - 70.51%\n",
      "1445/2048 - 70.56%\n",
      "1446/2048 - 70.61%\n",
      "1447/2048 - 70.65%\n",
      "1448/2048 - 70.7%\n",
      "1449/2048 - 70.75%\n",
      "1450/2048 - 70.8%\n",
      "1451/2048 - 70.85%\n",
      "1452/2048 - 70.9%\n",
      "1453/2048 - 70.95%\n",
      "1454/2048 - 71.0%\n",
      "1455/2048 - 71.04%\n",
      "1456/2048 - 71.09%\n",
      "1457/2048 - 71.14%\n",
      "1458/2048 - 71.19%\n",
      "1459/2048 - 71.24%\n",
      "1460/2048 - 71.29%\n",
      "1461/2048 - 71.34%\n",
      "1462/2048 - 71.39%\n",
      "1463/2048 - 71.44%\n",
      "1464/2048 - 71.48%\n",
      "1465/2048 - 71.53%\n",
      "1466/2048 - 71.58%\n",
      "1467/2048 - 71.63%\n",
      "1468/2048 - 71.68%\n",
      "1469/2048 - 71.73%\n",
      "1470/2048 - 71.78%\n",
      "1471/2048 - 71.83%\n",
      "1472/2048 - 71.88%\n",
      "1473/2048 - 71.92%\n",
      "1474/2048 - 71.97%\n",
      "1475/2048 - 72.02%\n",
      "1476/2048 - 72.07%\n",
      "1477/2048 - 72.12%\n",
      "1478/2048 - 72.17%\n",
      "1479/2048 - 72.22%\n",
      "1480/2048 - 72.27%\n",
      "1481/2048 - 72.31%\n",
      "1482/2048 - 72.36%\n",
      "1483/2048 - 72.41%\n",
      "1484/2048 - 72.46%\n",
      "1485/2048 - 72.51%\n",
      "1486/2048 - 72.56%\n",
      "1487/2048 - 72.61%\n",
      "1488/2048 - 72.66%\n",
      "1489/2048 - 72.71%\n",
      "1490/2048 - 72.75%\n",
      "1491/2048 - 72.8%\n",
      "1492/2048 - 72.85%\n",
      "1493/2048 - 72.9%\n",
      "1494/2048 - 72.95%\n",
      "1495/2048 - 73.0%\n",
      "1496/2048 - 73.05%\n",
      "1497/2048 - 73.1%\n",
      "1498/2048 - 73.14%\n",
      "1499/2048 - 73.19%\n",
      "1500/2048 - 73.24%\n",
      "1501/2048 - 73.29%\n",
      "1502/2048 - 73.34%\n",
      "1503/2048 - 73.39%\n",
      "1504/2048 - 73.44%\n",
      "1505/2048 - 73.49%\n",
      "1506/2048 - 73.54%\n",
      "1507/2048 - 73.58%\n",
      "1508/2048 - 73.63%\n",
      "1509/2048 - 73.68%\n",
      "1510/2048 - 73.73%\n",
      "1511/2048 - 73.78%\n",
      "1512/2048 - 73.83%\n",
      "1513/2048 - 73.88%\n",
      "1514/2048 - 73.93%\n",
      "1515/2048 - 73.97%\n",
      "1516/2048 - 74.02%\n",
      "1517/2048 - 74.07%\n",
      "1518/2048 - 74.12%\n",
      "1519/2048 - 74.17%\n",
      "1520/2048 - 74.22%\n",
      "1521/2048 - 74.27%\n",
      "1522/2048 - 74.32%\n",
      "1523/2048 - 74.37%\n",
      "1524/2048 - 74.41%\n",
      "1525/2048 - 74.46%\n",
      "1526/2048 - 74.51%\n",
      "1527/2048 - 74.56%\n",
      "1528/2048 - 74.61%\n",
      "1529/2048 - 74.66%\n",
      "1530/2048 - 74.71%\n",
      "1531/2048 - 74.76%\n",
      "1532/2048 - 74.8%\n",
      "1533/2048 - 74.85%\n",
      "1534/2048 - 74.9%\n",
      "1535/2048 - 74.95%\n",
      "1536/2048 - 75.0%\n",
      "1537/2048 - 75.05%\n",
      "1538/2048 - 75.1%\n",
      "1539/2048 - 75.15%\n",
      "1540/2048 - 75.2%\n",
      "1541/2048 - 75.24%\n",
      "1542/2048 - 75.29%\n",
      "1543/2048 - 75.34%\n",
      "1544/2048 - 75.39%\n",
      "1545/2048 - 75.44%\n",
      "1546/2048 - 75.49%\n",
      "1547/2048 - 75.54%\n",
      "1548/2048 - 75.59%\n",
      "1549/2048 - 75.63%\n",
      "1550/2048 - 75.68%\n",
      "1551/2048 - 75.73%\n",
      "1552/2048 - 75.78%\n",
      "1553/2048 - 75.83%\n",
      "1554/2048 - 75.88%\n",
      "1555/2048 - 75.93%\n",
      "1556/2048 - 75.98%\n",
      "1557/2048 - 76.03%\n",
      "1558/2048 - 76.07%\n",
      "1559/2048 - 76.12%\n",
      "1560/2048 - 76.17%\n",
      "1561/2048 - 76.22%\n",
      "1562/2048 - 76.27%\n",
      "1563/2048 - 76.32%\n",
      "1564/2048 - 76.37%\n",
      "1565/2048 - 76.42%\n",
      "1566/2048 - 76.46%\n",
      "1567/2048 - 76.51%\n",
      "1568/2048 - 76.56%\n",
      "1569/2048 - 76.61%\n",
      "1570/2048 - 76.66%\n",
      "1571/2048 - 76.71%\n",
      "1572/2048 - 76.76%\n",
      "1573/2048 - 76.81%\n",
      "1574/2048 - 76.86%\n",
      "1575/2048 - 76.9%\n",
      "1576/2048 - 76.95%\n",
      "1577/2048 - 77.0%\n",
      "1578/2048 - 77.05%\n",
      "1579/2048 - 77.1%\n",
      "1580/2048 - 77.15%\n",
      "1581/2048 - 77.2%\n",
      "1582/2048 - 77.25%\n",
      "1583/2048 - 77.29%\n",
      "1584/2048 - 77.34%\n",
      "1585/2048 - 77.39%\n",
      "1586/2048 - 77.44%\n",
      "1587/2048 - 77.49%\n",
      "1588/2048 - 77.54%\n",
      "1589/2048 - 77.59%\n",
      "1590/2048 - 77.64%\n",
      "1591/2048 - 77.69%\n",
      "1592/2048 - 77.73%\n",
      "1593/2048 - 77.78%\n",
      "1594/2048 - 77.83%\n",
      "1595/2048 - 77.88%\n",
      "1596/2048 - 77.93%\n",
      "1597/2048 - 77.98%\n",
      "1598/2048 - 78.03%\n",
      "1599/2048 - 78.08%\n",
      "1600/2048 - 78.12%\n",
      "1601/2048 - 78.17%\n",
      "1602/2048 - 78.22%\n",
      "1603/2048 - 78.27%\n",
      "1604/2048 - 78.32%\n",
      "1605/2048 - 78.37%\n",
      "1606/2048 - 78.42%\n",
      "1607/2048 - 78.47%\n",
      "1608/2048 - 78.52%\n",
      "1609/2048 - 78.56%\n",
      "1610/2048 - 78.61%\n",
      "1611/2048 - 78.66%\n",
      "1612/2048 - 78.71%\n",
      "1613/2048 - 78.76%\n",
      "1614/2048 - 78.81%\n",
      "1615/2048 - 78.86%\n",
      "1616/2048 - 78.91%\n",
      "1617/2048 - 78.96%\n",
      "1618/2048 - 79.0%\n",
      "1619/2048 - 79.05%\n",
      "1620/2048 - 79.1%\n",
      "1621/2048 - 79.15%\n",
      "1622/2048 - 79.2%\n",
      "1623/2048 - 79.25%\n",
      "1624/2048 - 79.3%\n",
      "1625/2048 - 79.35%\n",
      "1626/2048 - 79.39%\n",
      "1627/2048 - 79.44%\n",
      "1628/2048 - 79.49%\n",
      "1629/2048 - 79.54%\n",
      "1630/2048 - 79.59%\n",
      "1631/2048 - 79.64%\n",
      "1632/2048 - 79.69%\n",
      "1633/2048 - 79.74%\n",
      "1634/2048 - 79.79%\n",
      "1635/2048 - 79.83%\n",
      "1636/2048 - 79.88%\n",
      "1637/2048 - 79.93%\n",
      "1638/2048 - 79.98%\n",
      "1639/2048 - 80.03%\n",
      "1640/2048 - 80.08%\n",
      "1641/2048 - 80.13%\n",
      "1642/2048 - 80.18%\n",
      "1643/2048 - 80.22%\n",
      "1644/2048 - 80.27%\n",
      "1645/2048 - 80.32%\n",
      "1646/2048 - 80.37%\n",
      "1647/2048 - 80.42%\n",
      "1648/2048 - 80.47%\n",
      "1649/2048 - 80.52%\n",
      "1650/2048 - 80.57%\n",
      "1651/2048 - 80.62%\n",
      "1652/2048 - 80.66%\n",
      "1653/2048 - 80.71%\n",
      "1654/2048 - 80.76%\n",
      "1655/2048 - 80.81%\n",
      "1656/2048 - 80.86%\n",
      "1657/2048 - 80.91%\n",
      "1658/2048 - 80.96%\n",
      "1659/2048 - 81.01%\n",
      "1660/2048 - 81.05%\n",
      "1661/2048 - 81.1%\n",
      "1662/2048 - 81.15%\n",
      "1663/2048 - 81.2%\n",
      "1664/2048 - 81.25%\n",
      "1665/2048 - 81.3%\n",
      "1666/2048 - 81.35%\n",
      "1667/2048 - 81.4%\n",
      "1668/2048 - 81.45%\n",
      "1669/2048 - 81.49%\n",
      "1670/2048 - 81.54%\n",
      "1671/2048 - 81.59%\n",
      "1672/2048 - 81.64%\n",
      "1673/2048 - 81.69%\n",
      "1674/2048 - 81.74%\n",
      "1675/2048 - 81.79%\n",
      "1676/2048 - 81.84%\n",
      "1677/2048 - 81.88%\n",
      "1678/2048 - 81.93%\n",
      "1679/2048 - 81.98%\n",
      "1680/2048 - 82.03%\n",
      "1681/2048 - 82.08%\n",
      "1682/2048 - 82.13%\n",
      "1683/2048 - 82.18%\n",
      "1684/2048 - 82.23%\n",
      "1685/2048 - 82.28%\n",
      "1686/2048 - 82.32%\n",
      "1687/2048 - 82.37%\n",
      "1688/2048 - 82.42%\n",
      "1689/2048 - 82.47%\n",
      "1690/2048 - 82.52%\n",
      "1691/2048 - 82.57%\n",
      "1692/2048 - 82.62%\n",
      "1693/2048 - 82.67%\n",
      "1694/2048 - 82.71%\n",
      "1695/2048 - 82.76%\n",
      "1696/2048 - 82.81%\n",
      "1697/2048 - 82.86%\n",
      "1698/2048 - 82.91%\n",
      "1699/2048 - 82.96%\n",
      "1700/2048 - 83.01%\n",
      "1701/2048 - 83.06%\n",
      "1702/2048 - 83.11%\n",
      "1703/2048 - 83.15%\n",
      "1704/2048 - 83.2%\n",
      "1705/2048 - 83.25%\n",
      "1706/2048 - 83.3%\n",
      "1707/2048 - 83.35%\n",
      "1708/2048 - 83.4%\n",
      "1709/2048 - 83.45%\n",
      "1710/2048 - 83.5%\n",
      "1711/2048 - 83.54%\n",
      "1712/2048 - 83.59%\n",
      "1713/2048 - 83.64%\n",
      "1714/2048 - 83.69%\n",
      "1715/2048 - 83.74%\n",
      "1716/2048 - 83.79%\n",
      "1717/2048 - 83.84%\n",
      "1718/2048 - 83.89%\n",
      "1719/2048 - 83.94%\n",
      "1720/2048 - 83.98%\n",
      "1721/2048 - 84.03%\n",
      "1722/2048 - 84.08%\n",
      "1723/2048 - 84.13%\n",
      "1724/2048 - 84.18%\n",
      "1725/2048 - 84.23%\n",
      "1726/2048 - 84.28%\n",
      "1727/2048 - 84.33%\n",
      "1728/2048 - 84.38%\n",
      "1729/2048 - 84.42%\n",
      "1730/2048 - 84.47%\n",
      "1731/2048 - 84.52%\n",
      "1732/2048 - 84.57%\n",
      "1733/2048 - 84.62%\n",
      "1734/2048 - 84.67%\n",
      "1735/2048 - 84.72%\n",
      "1736/2048 - 84.77%\n",
      "1737/2048 - 84.81%\n",
      "1738/2048 - 84.86%\n",
      "1739/2048 - 84.91%\n",
      "1740/2048 - 84.96%\n",
      "1741/2048 - 85.01%\n",
      "1742/2048 - 85.06%\n",
      "1743/2048 - 85.11%\n",
      "1744/2048 - 85.16%\n",
      "1745/2048 - 85.21%\n",
      "1746/2048 - 85.25%\n",
      "1747/2048 - 85.3%\n",
      "1748/2048 - 85.35%\n",
      "1749/2048 - 85.4%\n",
      "1750/2048 - 85.45%\n",
      "1751/2048 - 85.5%\n",
      "1752/2048 - 85.55%\n",
      "1753/2048 - 85.6%\n",
      "1754/2048 - 85.64%\n",
      "1755/2048 - 85.69%\n",
      "1756/2048 - 85.74%\n",
      "1757/2048 - 85.79%\n",
      "1758/2048 - 85.84%\n",
      "1759/2048 - 85.89%\n",
      "1760/2048 - 85.94%\n",
      "1761/2048 - 85.99%\n",
      "1762/2048 - 86.04%\n",
      "1763/2048 - 86.08%\n",
      "1764/2048 - 86.13%\n",
      "1765/2048 - 86.18%\n",
      "1766/2048 - 86.23%\n",
      "1767/2048 - 86.28%\n",
      "1768/2048 - 86.33%\n",
      "1769/2048 - 86.38%\n",
      "1770/2048 - 86.43%\n",
      "1771/2048 - 86.47%\n",
      "1772/2048 - 86.52%\n",
      "1773/2048 - 86.57%\n",
      "1774/2048 - 86.62%\n",
      "1775/2048 - 86.67%\n",
      "1776/2048 - 86.72%\n",
      "1777/2048 - 86.77%\n",
      "1778/2048 - 86.82%\n",
      "1779/2048 - 86.87%\n",
      "1780/2048 - 86.91%\n",
      "1781/2048 - 86.96%\n",
      "1782/2048 - 87.01%\n",
      "1783/2048 - 87.06%\n",
      "1784/2048 - 87.11%\n",
      "1785/2048 - 87.16%\n",
      "1786/2048 - 87.21%\n",
      "1787/2048 - 87.26%\n",
      "1788/2048 - 87.3%\n",
      "1789/2048 - 87.35%\n",
      "1790/2048 - 87.4%\n",
      "1791/2048 - 87.45%\n",
      "1792/2048 - 87.5%\n",
      "1793/2048 - 87.55%\n",
      "1794/2048 - 87.6%\n",
      "1795/2048 - 87.65%\n",
      "1796/2048 - 87.7%\n",
      "1797/2048 - 87.74%\n",
      "1798/2048 - 87.79%\n",
      "1799/2048 - 87.84%\n",
      "1800/2048 - 87.89%\n",
      "1801/2048 - 87.94%\n",
      "1802/2048 - 87.99%\n",
      "1803/2048 - 88.04%\n",
      "1804/2048 - 88.09%\n",
      "1805/2048 - 88.13%\n",
      "1806/2048 - 88.18%\n",
      "1807/2048 - 88.23%\n",
      "1808/2048 - 88.28%\n",
      "1809/2048 - 88.33%\n",
      "1810/2048 - 88.38%\n",
      "1811/2048 - 88.43%\n",
      "1812/2048 - 88.48%\n",
      "1813/2048 - 88.53%\n",
      "1814/2048 - 88.57%\n",
      "1815/2048 - 88.62%\n",
      "1816/2048 - 88.67%\n",
      "1817/2048 - 88.72%\n",
      "1818/2048 - 88.77%\n",
      "1819/2048 - 88.82%\n",
      "1820/2048 - 88.87%\n",
      "1821/2048 - 88.92%\n",
      "1822/2048 - 88.96%\n",
      "1823/2048 - 89.01%\n",
      "1824/2048 - 89.06%\n",
      "1825/2048 - 89.11%\n",
      "1826/2048 - 89.16%\n",
      "1827/2048 - 89.21%\n",
      "1828/2048 - 89.26%\n",
      "1829/2048 - 89.31%\n",
      "1830/2048 - 89.36%\n",
      "1831/2048 - 89.4%\n",
      "1832/2048 - 89.45%\n",
      "1833/2048 - 89.5%\n",
      "1834/2048 - 89.55%\n",
      "1835/2048 - 89.6%\n",
      "1836/2048 - 89.65%\n",
      "1837/2048 - 89.7%\n",
      "1838/2048 - 89.75%\n",
      "1839/2048 - 89.79%\n",
      "1840/2048 - 89.84%\n",
      "1841/2048 - 89.89%\n",
      "1842/2048 - 89.94%\n",
      "1843/2048 - 89.99%\n",
      "1844/2048 - 90.04%\n",
      "1845/2048 - 90.09%\n",
      "1846/2048 - 90.14%\n",
      "1847/2048 - 90.19%\n",
      "1848/2048 - 90.23%\n",
      "1849/2048 - 90.28%\n",
      "1850/2048 - 90.33%\n",
      "1851/2048 - 90.38%\n",
      "1852/2048 - 90.43%\n",
      "1853/2048 - 90.48%\n",
      "1854/2048 - 90.53%\n",
      "1855/2048 - 90.58%\n",
      "1856/2048 - 90.62%\n",
      "1857/2048 - 90.67%\n",
      "1858/2048 - 90.72%\n",
      "1859/2048 - 90.77%\n",
      "1860/2048 - 90.82%\n",
      "1861/2048 - 90.87%\n",
      "1862/2048 - 90.92%\n",
      "1863/2048 - 90.97%\n",
      "1864/2048 - 91.02%\n",
      "1865/2048 - 91.06%\n",
      "1866/2048 - 91.11%\n",
      "1867/2048 - 91.16%\n",
      "1868/2048 - 91.21%\n",
      "1869/2048 - 91.26%\n",
      "1870/2048 - 91.31%\n",
      "1871/2048 - 91.36%\n",
      "1872/2048 - 91.41%\n",
      "1873/2048 - 91.46%\n",
      "1874/2048 - 91.5%\n",
      "1875/2048 - 91.55%\n",
      "1876/2048 - 91.6%\n",
      "1877/2048 - 91.65%\n",
      "1878/2048 - 91.7%\n",
      "1879/2048 - 91.75%\n",
      "1880/2048 - 91.8%\n",
      "1881/2048 - 91.85%\n",
      "1882/2048 - 91.89%\n",
      "1883/2048 - 91.94%\n",
      "1884/2048 - 91.99%\n",
      "1885/2048 - 92.04%\n",
      "1886/2048 - 92.09%\n",
      "1887/2048 - 92.14%\n",
      "1888/2048 - 92.19%\n",
      "1889/2048 - 92.24%\n",
      "1890/2048 - 92.29%\n",
      "1891/2048 - 92.33%\n",
      "1892/2048 - 92.38%\n",
      "1893/2048 - 92.43%\n",
      "1894/2048 - 92.48%\n",
      "1895/2048 - 92.53%\n",
      "1896/2048 - 92.58%\n",
      "1897/2048 - 92.63%\n",
      "1898/2048 - 92.68%\n",
      "1899/2048 - 92.72%\n",
      "1900/2048 - 92.77%\n",
      "1901/2048 - 92.82%\n",
      "1902/2048 - 92.87%\n",
      "1903/2048 - 92.92%\n",
      "1904/2048 - 92.97%\n",
      "1905/2048 - 93.02%\n",
      "1906/2048 - 93.07%\n",
      "1907/2048 - 93.12%\n",
      "1908/2048 - 93.16%\n",
      "1909/2048 - 93.21%\n",
      "1910/2048 - 93.26%\n",
      "1911/2048 - 93.31%\n",
      "1912/2048 - 93.36%\n",
      "1913/2048 - 93.41%\n",
      "1914/2048 - 93.46%\n",
      "1915/2048 - 93.51%\n",
      "1916/2048 - 93.55%\n",
      "1917/2048 - 93.6%\n",
      "1918/2048 - 93.65%\n",
      "1919/2048 - 93.7%\n",
      "1920/2048 - 93.75%\n",
      "1921/2048 - 93.8%\n",
      "1922/2048 - 93.85%\n",
      "1923/2048 - 93.9%\n",
      "1924/2048 - 93.95%\n",
      "1925/2048 - 93.99%\n",
      "1926/2048 - 94.04%\n",
      "1927/2048 - 94.09%\n",
      "1928/2048 - 94.14%\n",
      "1929/2048 - 94.19%\n",
      "1930/2048 - 94.24%\n",
      "1931/2048 - 94.29%\n",
      "1932/2048 - 94.34%\n",
      "1933/2048 - 94.38%\n",
      "1934/2048 - 94.43%\n",
      "1935/2048 - 94.48%\n",
      "1936/2048 - 94.53%\n",
      "1937/2048 - 94.58%\n",
      "1938/2048 - 94.63%\n",
      "1939/2048 - 94.68%\n",
      "1940/2048 - 94.73%\n",
      "1941/2048 - 94.78%\n",
      "1942/2048 - 94.82%\n",
      "1943/2048 - 94.87%\n",
      "1944/2048 - 94.92%\n",
      "1945/2048 - 94.97%\n",
      "1946/2048 - 95.02%\n",
      "1947/2048 - 95.07%\n",
      "1948/2048 - 95.12%\n",
      "1949/2048 - 95.17%\n",
      "1950/2048 - 95.21%\n",
      "1951/2048 - 95.26%\n",
      "1952/2048 - 95.31%\n",
      "1953/2048 - 95.36%\n",
      "1954/2048 - 95.41%\n",
      "1955/2048 - 95.46%\n",
      "1956/2048 - 95.51%\n",
      "1957/2048 - 95.56%\n",
      "1958/2048 - 95.61%\n",
      "1959/2048 - 95.65%\n",
      "1960/2048 - 95.7%\n",
      "1961/2048 - 95.75%\n",
      "1962/2048 - 95.8%\n",
      "1963/2048 - 95.85%\n",
      "1964/2048 - 95.9%\n",
      "1965/2048 - 95.95%\n",
      "1966/2048 - 96.0%\n",
      "1967/2048 - 96.04%\n",
      "1968/2048 - 96.09%\n",
      "1969/2048 - 96.14%\n",
      "1970/2048 - 96.19%\n",
      "1971/2048 - 96.24%\n",
      "1972/2048 - 96.29%\n",
      "1973/2048 - 96.34%\n",
      "1974/2048 - 96.39%\n",
      "1975/2048 - 96.44%\n",
      "1976/2048 - 96.48%\n",
      "1977/2048 - 96.53%\n",
      "1978/2048 - 96.58%\n",
      "1979/2048 - 96.63%\n",
      "1980/2048 - 96.68%\n",
      "1981/2048 - 96.73%\n",
      "1982/2048 - 96.78%\n",
      "1983/2048 - 96.83%\n",
      "1984/2048 - 96.88%\n",
      "1985/2048 - 96.92%\n",
      "1986/2048 - 96.97%\n",
      "1987/2048 - 97.02%\n",
      "1988/2048 - 97.07%\n",
      "1989/2048 - 97.12%\n",
      "1990/2048 - 97.17%\n",
      "1991/2048 - 97.22%\n",
      "1992/2048 - 97.27%\n",
      "1993/2048 - 97.31%\n",
      "1994/2048 - 97.36%\n",
      "1995/2048 - 97.41%\n",
      "1996/2048 - 97.46%\n",
      "1997/2048 - 97.51%\n",
      "1998/2048 - 97.56%\n",
      "1999/2048 - 97.61%\n",
      "2000/2048 - 97.66%\n",
      "2001/2048 - 97.71%\n",
      "2002/2048 - 97.75%\n",
      "2003/2048 - 97.8%\n",
      "2004/2048 - 97.85%\n",
      "2005/2048 - 97.9%\n",
      "2006/2048 - 97.95%\n",
      "2007/2048 - 98.0%\n",
      "2008/2048 - 98.05%\n",
      "2009/2048 - 98.1%\n",
      "2010/2048 - 98.14%\n",
      "2011/2048 - 98.19%\n",
      "2012/2048 - 98.24%\n",
      "2013/2048 - 98.29%\n",
      "2014/2048 - 98.34%\n",
      "2015/2048 - 98.39%\n",
      "2016/2048 - 98.44%\n",
      "2017/2048 - 98.49%\n",
      "2018/2048 - 98.54%\n",
      "2019/2048 - 98.58%\n",
      "2020/2048 - 98.63%\n",
      "2021/2048 - 98.68%\n",
      "2022/2048 - 98.73%\n",
      "2023/2048 - 98.78%\n",
      "2024/2048 - 98.83%\n",
      "2025/2048 - 98.88%\n",
      "2026/2048 - 98.93%\n",
      "2027/2048 - 98.97%\n",
      "2028/2048 - 99.02%\n",
      "2029/2048 - 99.07%\n",
      "2030/2048 - 99.12%\n",
      "2031/2048 - 99.17%\n",
      "2032/2048 - 99.22%\n",
      "2033/2048 - 99.27%\n",
      "2034/2048 - 99.32%\n",
      "2035/2048 - 99.37%\n",
      "2036/2048 - 99.41%\n",
      "2037/2048 - 99.46%\n",
      "2038/2048 - 99.51%\n",
      "2039/2048 - 99.56%\n",
      "2040/2048 - 99.61%\n",
      "2041/2048 - 99.66%\n",
      "2042/2048 - 99.71%\n",
      "2043/2048 - 99.76%\n",
      "2044/2048 - 99.8%\n",
      "2045/2048 - 99.85%\n",
      "2046/2048 - 99.9%\n",
      "2047/2048 - 99.95%\n",
      "2048/2048 - 100.0%\n"
     ]
    }
   ],
   "source": [
    "def calculate_cosine_similarity(combinations, model):\n",
    "  result = []\n",
    "  for idx, item in enumerate(combinations):\n",
    "    img_1, img_2, label = item\n",
    "    pred_1, pred_2 = model.vectorize(np.array([img_1, img_2]))\n",
    "    cos_sim = cosine_similarity_cpu(pred_1, pred_2)\n",
    "    result.append((cos_sim, label))\n",
    "    print(f'{idx + 1}/{len(combinations)} - {round(((idx + 1) / len(combinations)) * 100, 2)}%')\n",
    "  return result\n",
    "\n",
    "MAX_VEC_LEN = 2048\n",
    "result = calculate_cosine_similarity(all_combinations[:MAX_VEC_LEN], model_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1954\n",
       "False      94\n",
       "Name: correct, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['cos_sim', 'label']\n",
    "df.label = df.label.astype(bool)\n",
    "df['pred_label'] = df.apply(lambda x: True if x.cos_sim > 0.8 else False, axis=1)\n",
    "df['correct'] = df.apply(lambda x: True if x.label == x.pred_label else False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 0.41509433962264153\n"
     ]
    }
   ],
   "source": [
    "# Impact when the model predict to lot False positives\n",
    "precision_result = sk_metrics.precision_score(df.label, df.pred_label)\n",
    "\n",
    "# Impact when the model predict to lot False negatives\n",
    "recall_result = sk_metrics.recall_score(df.label, df.pred_label)\n",
    "\n",
    "# Impact when the model predict to lot False positives and False negatives\n",
    "f1_result = sk_metrics.f1_score(df.label, df.pred_label)\n",
    "\n",
    "#print(f'Precision score: {precision_result}')\n",
    "print(f'Recall score: {recall_result}')\n",
    "#print(f'F1 score: {f1_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeClassifier(\n",
    "  num_classes=len(class_array),\n",
    "  input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "  type_extractor='vgg',\n",
    "  units=UNITS,\n",
    "  inner_layers=1\n",
    ")\n",
    "model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "PATH_BEST = './models/vgg_16class_1024_units_aqr.h5'\n",
    "model.load_weights(PATH_BEST)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vector and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "db1_v, db2_v, nr1_v = model.vectorize(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(db1_v, nr1_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(db1_v, db2_v)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "852bc408046ca7dfc5c8f91ce764d8630d2287ca09c7fe9d1b4d9cd156705bcb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
