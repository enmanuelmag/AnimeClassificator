{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import i2v\n",
    "import time\n",
    "import json\n",
    "import numba\n",
    "import hashlib\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from numba import vectorize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk_metrics\n",
    "from alive_progress import alive_bar\n",
    "from notifier import Notifier, notify\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import pickle as pkl\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "SEED = 42\n",
    "SIZE_IMG = 224 #224#224#416\n",
    "UNITS = 1024 #2048 1024 128 256 512-seq\n",
    "MAX_CLASS = 1024 #1024 32 16 8\n",
    "\n",
    "DATASET_PATH = './data/animes'\n",
    "DATASET_FACES_PATH = './data/faces'\n",
    "CLASS_ARRAY_PATH = f'./data/class_array_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_FACES_PATH = f'./data/class_array_faces_{MAX_CLASS}.pkl'\n",
    "CLASS_ARRAY_VEC_PATH = f'./data/class_array_vec_{MAX_CLASS}.pkl'\n",
    "\n",
    "DATASET_JSON_PATH = './data/anime_data.json'\n",
    "\n",
    "AMOUNT_TABLE_PATH = './data/anime_amount.pkl'\n",
    "AMOUNT_FACES_TABLE_PATH = './data/faces_amount.pkl'\n",
    "\n",
    "DATASET_JSON_RANK = './data/anime_rank.json'\n",
    "\n",
    "TFRECORD_PATH = f'./data/anime_data_{MAX_CLASS}.tfrecord'\n",
    "TFRECORD_FACES_PATH = f'./data/anime_faces_data_{MAX_CLASS}.tfrecord'\n",
    "\n",
    "TG_ID = \"293701727\"\n",
    "TG_TOKEN = \"1878628343:AAEFVRsqDz63ycmaLOFS7gvsG969wdAsJ0w\"\n",
    "WEBHOOK_URL = \"https://discord.com/api/webhooks/796406472459288616/PAkiGGwqe0_PwtBxXYQvOzbk78B4RQP6VWRkvpBtw6Av0sc_mDa3saaIlwVPFjOIeIbt\"\n",
    "\n",
    "if True:\n",
    "  #DATASET_PATH = DATASET_FACES_PATH\n",
    "  CLASS_ARRAY_PATH = CLASS_ARRAY_FACES_PATH\n",
    "  TFRECORD_PATH = TFRECORD_FACES_PATH\n",
    "  AMOUNT_TABLE_PATH = AMOUNT_FACES_TABLE_PATH\n",
    "\n",
    "#seed random seed to 42 for reproducibility\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dowload_image(url, anime_name, idx):\n",
    "  #download image from url\n",
    "  file_path = f'./data/animes/{anime_name}____{idx}.jpg' \n",
    "  if os.path.exists(file_path):\n",
    "    return\n",
    "\n",
    "  img_data = requests.get(url).content\n",
    "  with open(file_path, 'wb') as handler:\n",
    "    handler.write(img_data)\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Anime images',\n",
    "  msg='Finished downloading anime images'\n",
    ")\n",
    "def get_images(data):\n",
    "  #with alive_bar(len(data)) as bar:\n",
    "  for idx_a, anime_name in enumerate(data):\n",
    "    urls = data[anime_name]\n",
    "    for idx, url in enumerate(urls):\n",
    "      if idx >= 400:\n",
    "        break\n",
    "      name_clean = re.sub(r'_+', r'_', re.sub(r'[\\W\\s]', r'_', anime_name))\n",
    "      try:\n",
    "        dowload_image(url['image'], name_clean, idx)\n",
    "      except Exception as e:\n",
    "        print(f'Error on download image {idx + 1} of {anime_name}')\n",
    "        pass\n",
    "    #bar()\n",
    "    print(f'Progress: {idx_a + 1}/{len(data)} - {round((idx_a + 1)/len(data)*100, 2)}%')\n",
    "\n",
    "def get_classes_anime(path):\n",
    "  classes = set()\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    classes.add(class_name)\n",
    "  return list(classes)\n",
    "\n",
    "def wait_for_it(driver, xpath, timeout=3):\n",
    "  try:\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "  except Exception as e:\n",
    "    return None\n",
    "\n",
    "def iter_post(driver):\n",
    "  anime_data = []\n",
    "\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = True\n",
    "\n",
    "  while next_button is not None:\n",
    "    if len(anime_data) > 400:\n",
    "      break\n",
    "    ul_element = wait_for_it(driver, '//ul[@id=\"post-list-posts\"]')\n",
    "    if ul_element is None:\n",
    "      next_button = wait_for_it(driver, xpath_next)\n",
    "      if next_button is not None:\n",
    "        next_button.click()\n",
    "        time.sleep(1)\n",
    "      continue\n",
    "    for i, li_element in enumerate(ul_element.find_elements(By.TAG_NAME, 'li')):\n",
    "      a_video = li_element.find_element(By.XPATH, './a').get_attribute('href')\n",
    "      a_image = li_element.find_element(By.XPATH, './div/a/img').get_attribute('src')\n",
    "      anime_data.append({\n",
    "        'video': a_video,\n",
    "        'image': a_image\n",
    "      })\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "    if next_button is not None:\n",
    "      next_button.click()\n",
    "      time.sleep(rd.randint(1, 2))\n",
    "  return anime_data\n",
    "\n",
    "def get_images_links(url, driver, anime_name):\n",
    "  url_search = url + anime_name\n",
    "  driver.get(url_search)\n",
    "  return iter_post(driver)\n",
    "\n",
    "def get_names(driver):\n",
    "  names = []\n",
    "  xpath_next = '//a[@class=\"next_page\"]'\n",
    "  next_button = wait_for_it(driver, xpath_next)\n",
    "  \n",
    "  while next_button is not None:\n",
    "    for tr_element in driver.find_elements(By.XPATH, '//table[@class=\"highlightable\"]/tbody/tr'):\n",
    "      try:\n",
    "        amount_post = tr_element.find_element(By.XPATH, './td[1]').text\n",
    "        amount_post = int(amount_post)\n",
    "        if amount_post >= 10:\n",
    "          a_name = tr_element.find_element(By.XPATH, './td[2]/a[2]' ).text\n",
    "          names.append(a_name)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    next_button.click()\n",
    "    time.sleep(rd.randint(1, 2))\n",
    "    next_button = wait_for_it(driver, xpath_next)\n",
    "  return names\n",
    "\n",
    "def get_score(anime_name, driver):\n",
    "  url_search = f'https://myanimelist.net/anime.php?cat=anime&q={anime_name}'\n",
    "  driver.get(url_search)\n",
    "  score = 0\n",
    "  for filename in os.listdir(path):\n",
    "    class_name, _ = filename.split('____')\n",
    "    score += 1\n",
    "  return score\n",
    "\n",
    "def relevant_anime(anime_name, df_anime, amount_table, threshold=350, rank=True):\n",
    "  \n",
    "  if amount_table.get(anime_name, 0) <= threshold:\n",
    "    return False\n",
    "\n",
    "  if not rank:\n",
    "    return True\n",
    "\n",
    "  anime_name = re.sub(r'_', r' ', anime_name)\n",
    "  df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "\n",
    "  if df_result.empty:\n",
    "    anime_name = ' '.join(anime_name.split(' ')[:3])\n",
    "    df_result = df_anime[df_anime['name'].str.contains(anime_name)]\n",
    "  return not df_result.empty\n",
    "\n",
    "def amount_anime_table(datapath):\n",
    "  dic = {}\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    dic[class_name] = dic.get(class_name, 0) + 1\n",
    "  return dic\n",
    "\n",
    "def detect(filename, cascade_file):\n",
    "  if not os.path.isfile(cascade_file):\n",
    "    raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "  cascade = cv2.CascadeClassifier(cascade_file)\n",
    "  image = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "  #src = cv2.cuda_GpuMat()\n",
    "  #src.upload(image)\n",
    "\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  gray = cv2.equalizeHist(gray)\n",
    "\n",
    "  faces = cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor = 1.1,\n",
    "    minNeighbors = 5,\n",
    "    minSize = (24, 24)\n",
    "  )\n",
    "\n",
    "  new_images = []\n",
    "  for (x, y, w, h) in faces:\n",
    "    new_images.append(image[y:y+h, x:x+w])\n",
    "  #clahe = cv2.cuda.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "  #dst = clahe.apply(src, cv2.cuda_Stream.Null())\n",
    "  #result = dst.download()\n",
    "  return new_images\n",
    "\n",
    "def extract_faces(datapath):\n",
    "  faces_amount = 0\n",
    "  for filename in os.listdir(datapath):\n",
    "    class_name, _ = filename.split('____')\n",
    "    new_images = []\n",
    "    try:\n",
    "      new_images = detect(datapath + '/' + filename, './data/haar/lbpcascade_animeface.xml')\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      pass\n",
    "    if len(new_images) > 0:\n",
    "      for idx, img in enumerate(new_images):\n",
    "        new_face_name = f'./data/faces/{class_name}____{idx}.jpg'\n",
    "        try:\n",
    "          if not os.path.exists(new_face_name):\n",
    "            cv2.imwrite(new_face_name, img)\n",
    "            faces_amount += 1\n",
    "        except:\n",
    "          pass\n",
    "  print(f'Faces amount: {faces_amount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anime images\n",
    "anime_data = json.load(open(DATASET_JSON_PATH))\n",
    "get_images(anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only faces\n",
    "extract_faces(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of images per anime\n",
    "amount = amount_anime_table(DATASET_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of images per anime\n",
    "# ONLY FACES\n",
    "amount = amount_anime_table(DATASET_FACES_PATH) #DATASET_PATH #DATASET_FACES_PATH\n",
    "pkl.dump(amount, open(AMOUNT_TABLE_PATH, 'wb')) #AMOUNT_TABLE_PATH #AMOUNT_FACES_TABLE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes: 4598 - Filtered 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('./data/df_anime_rank.pkl')\n",
    "\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "all_class_array = get_classes_anime(DATASET_PATH)\n",
    "\n",
    "class_array = set()\n",
    "for anime_name in all_class_array:\n",
    "  if relevant_anime(anime_name, df, amount_table, threshold=100, rank=True):\n",
    "    class_array.add((anime_name, amount_table[anime_name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)}')\n",
    "del all_class_array\n",
    "\n",
    "class_array = [x[0] for x in class_array]\n",
    "pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes: 2749 - Filtered 1024\n"
     ]
    }
   ],
   "source": [
    "#Only for faces\n",
    "all_class_array = get_classes_anime(DATASET_FACES_PATH)\n",
    "amount_table = pkl.load(open(AMOUNT_TABLE_PATH, 'rb'))\n",
    "\n",
    "class_array = set()\n",
    "for name in all_class_array:\n",
    "  class_array.add((name, amount_table[name]))\n",
    "\n",
    "class_array = list(class_array)\n",
    "class_array.sort(key=lambda x: x[1], reverse=True)\n",
    "class_array = class_array[:MAX_CLASS]\n",
    "\n",
    "class_array = list(class_array)\n",
    "print(f'All classes: {len(all_class_array)} - Filtered {len(class_array)}')\n",
    "del all_class_array\n",
    "\n",
    "class_array = [x[0] for x in class_array]\n",
    "pkl.dump(class_array, open(CLASS_ARRAY_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started creating tfrecord\n",
      "Finished creating tfrecord\n"
     ]
    }
   ],
   "source": [
    "def get_class_id(class_name):\n",
    "  return class_array.index(class_name)\n",
    "\n",
    "def build_example(path_file, class_name):\n",
    "  img_array = open(path_file, 'rb').read()\n",
    "  \n",
    "  #img = load_img(path_file, target_size=(SIZE_IMG, SIZE_IMG))\n",
    "  #img_array = np.array(img)\n",
    "  #img_array = preprocess_input(img_array, mode='tf')\n",
    "  #key = hashlib.sha256(img_array).hexdigest()\n",
    "  example = tf.train.Example(\n",
    "    features=tf.train.Features(feature={\n",
    "    #'key': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode('utf-8')])),\n",
    "    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array])),\n",
    "    #'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_array.tobytes()])),\n",
    "    'class_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[get_class_id(class_name)])),\n",
    "    'class_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[class_name.encode('utf-8')])),\n",
    "    'filepath': tf.train.Feature(bytes_list=tf.train.BytesList(value=[path_file.encode('utf-8')]))\n",
    "  }))\n",
    "  return example\n",
    "\n",
    "def create_tfrecord(data_path, withe_list, path_tfrecord):\n",
    "  files = os.listdir(data_path)\n",
    "  writer = tf.io.TFRecordWriter(path_tfrecord)\n",
    "  \n",
    "  print('Started creating tfrecord')\n",
    "  for idx, filename in enumerate(files):\n",
    "    class_name, _ = filename.split('____')\n",
    "  \n",
    "    if class_name in withe_list:\n",
    "      path_file = os.path.join(data_path, filename)\n",
    "      tf_example = build_example(path_file, class_name)\n",
    "      writer.write(tf_example.SerializeToString())\n",
    "  print('Finished creating tfrecord')\n",
    "  writer.close()\n",
    "\n",
    "def parse_tfrecord(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  #class_id = tf.sparse.to_dense(x['class_id'], default_value=-1)\n",
    "  class_id = x['class_id']\n",
    "  if class_id is None:\n",
    "    class_id = -1\n",
    "\n",
    "  labels = tf.cast(class_id, tf.int64)\n",
    "  y_train = labels\n",
    "  #y_train = tf.stack([ labels ], axis=1)\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord(x, size))\n",
    "\n",
    "def create_model(num_classes, input_shape, units, type_extractor = 'vgg') -> tf.keras.Model:\n",
    "  if type_extractor == 'vgg':\n",
    "    feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'inception':\n",
    "    feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  elif type_extractor == 'resnet':\n",
    "    feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "  else:\n",
    "    raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(tf.keras.layers.Input(input_shape, name='input'))\n",
    "  model.add(feature_extractor)\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  #new\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "  return model\n",
    "\n",
    "class AnimeClassifier(tf.keras.Model):\n",
    "  def __init__(self, num_classes, input_shape, units=1024, inner_layers=12, type_extractor='vgg'):\n",
    "    assert type_extractor in ['vgg', 'inception', 'resnet']\n",
    "    assert inner_layers >= 1\n",
    "    assert num_classes >= 2\n",
    "    assert len(input_shape) == 3\n",
    "    assert units >= 64\n",
    "\n",
    "    super(AnimeClassifier, self).__init__(name='AnimeClassifier')\n",
    "\n",
    "    self.units = units\n",
    "    self.in_layer = tf.keras.layers.Input(input_shape, name='input')\n",
    "\n",
    "    if type_extractor == 'vgg':\n",
    "      feature_extractor = VGG19(weights='imagenet', include_top=False, input_shape=input_shape, input_tensor=self.in_layer)\n",
    "    elif type_extractor == 'inception':\n",
    "      feature_extractor = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif type_extractor == 'resnet':\n",
    "      feature_extractor = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "      raise ValueError('type_extractor must be vgg, inception or resnet')\n",
    "\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.global_average_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    self.hidden_mlp = []\n",
    "    for i in range(inner_layers):\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dense(units,activation=tf.nn.relu))\n",
    "      self.hidden_mlp.append(tf.keras.layers.Dropout(0.5, seed=SEED))\n",
    "\n",
    "    self.out_layer = tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax)\n",
    "\n",
    "  def call(self, inputs, training=None, mask=None):\n",
    "    x = self.feature_extractor(inputs, training=training)\n",
    "    x = self.global_average_pooling(x)\n",
    "    x = self.flatten(x, training=training)\n",
    "    for layer in self.hidden_mlp:\n",
    "      x = layer(x, training=training)\n",
    "    return self.out_layer(x, training=training)\n",
    "\n",
    "  def predict_classes(self, x):\n",
    "    return tf.argmax(self(x), axis=1)\n",
    "\n",
    "  def vectorize(self, x, flatten=True):\n",
    "    x = self.feature_extractor(x)\n",
    "    x = self.global_average_pooling(x)\n",
    "    if flatten:\n",
    "      return self.flatten(x)\n",
    "    return x\n",
    "\n",
    "@notify(\n",
    "  chat_id=TG_ID,\n",
    "  api_token=TG_TOKEN,\n",
    "  title='Train model',\n",
    "  msg='Training has finished'\n",
    ")\n",
    "def train(model, train_ds, val_ds, units, epochs=15, mode='fit', type_model='vgg', save_weights_only=False):\n",
    "  logdir = \"logs/scalars/\" + time.strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "  #logdir = \"logs/scalars/\" + \"test_replicated_seed_5\"\n",
    "  if mode == 'eager_tf':\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "      for batch, (images, labels) in enumerate(train_ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(images, training=True)\n",
    "          regularization_loss = tf.reduce_sum(model.losses)\n",
    "          pred_loss = []\n",
    "          for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "          total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(\"{}_train_{}, {}, {}\".format(\n",
    "          epoch, batch, total_loss.numpy(),\n",
    "          list(map(lambda x: np.sum(x.numpy()), pred_loss))\n",
    "        ))\n",
    "        avg_loss.update_state(total_loss)\n",
    "  elif mode == 'fit':\n",
    "    callbacks = [\n",
    "      ReduceLROnPlateau(verbose=1),\n",
    "      #EarlyStopping(patience=8, verbose=1),\n",
    "      ModelCheckpoint(\n",
    "        f'checkpoints/{type_model}_{units}_units_faces_{MAX_CLASS}.h5', \n",
    "        verbose=1,\n",
    "        monitor='accuracy',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=save_weights_only,\n",
    "      ),\n",
    "      TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(\n",
    "      train_ds,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_data=val_ds\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Total Training Time: {end_time} seconds')\n",
    "\n",
    "IMAGE_FEATURE_MAP = {\n",
    "  'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  'class_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "if True:\n",
    "  class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "  if os.path.exists(TFRECORD_PATH):\n",
    "    os.remove(TFRECORD_PATH)\n",
    "  #DATASET_PATH\n",
    "  create_tfrecord(DATASET_FACES_PATH, class_array, TFRECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 2043\n"
     ]
    }
   ],
   "source": [
    "tf_record = load_tfrecord_dataset(TFRECORD_PATH, SIZE_IMG) #TFRECORD_PATH\n",
    "all_ds_len = sum(1 for _ in tf_record)\n",
    "print(f'Total number of images: {all_ds_len}')\n",
    "\n",
    "#len_mini = all_ds_len\n",
    "#mini_tf_record = tf_record.take(len_mini)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_valid = int(all_ds_len * 0.1)\n",
    "n_test = all_ds_len - n_train - n_valid\n",
    "\n",
    "tf_record = tf_record.shuffle(n_train + n_valid + n_test, seed=SEED)\n",
    "train_ds = tf_record.take(n_train)\n",
    "valid_ds = tf_record.skip(n_train).take(n_valid)\n",
    "test_ds = tf_record.skip(n_train + n_valid).take(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Acc** |  **LR** | **Epochs** | **Batch** | **Units** | **Layers** | **Class** | Passed |\n",
    "|:-------:|:-------:|:----------:|:---------:|:---------:|:----------:|:---------:|:--------:|\n",
    "|    0.975 | 0.00001 |        300 |        32 |      1024 |          1 |         8 | - |\n",
    "|    0.925 | 0.000025|        300 |        32 |      1024 |          1 |        16 | x |\n",
    "|    0.903 | 0.000025|        300 |        32 |      1024 |          1 |        32 | - |\n",
    "|    FACES |\n",
    "|     | 0.000025|        300 |        32 |      1024 |          1 |        32 | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.000025, clipnorm=1.0) #0.000025 0.00001\n",
    "#0.00001 - 300 epochs - 32 batch - 1024 units (1 layers) - 8 class - best\n",
    "#0.00001 - 150 epochs - 32 - batch - 512 units (12 layers) - 8 class - prev\n",
    "\n",
    "model = None\n",
    "vanilla_model = False\n",
    "class_array = pkl.load(open(CLASS_ARRAY_PATH, 'rb'))\n",
    "\n",
    "\n",
    "if vanilla_model:\n",
    "  model = create_model(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor='vgg',\n",
    "    units=UNITS\n",
    "  )\n",
    "else:\n",
    "  model = AnimeClassifier(\n",
    "    num_classes=len(class_array),\n",
    "    input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "    type_extractor='vgg',\n",
    "    units=UNITS,\n",
    "    inner_layers=1\n",
    "  )\n",
    "  model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "train(\n",
    "  model=model,\n",
    "  epochs=300,\n",
    "  units=UNITS,\n",
    "  val_ds=valid_ds.batch(32),\n",
    "  train_ds=train_ds.batch(32),\n",
    "  save_weights_only=False if vanilla_model else True,\n",
    "  mode='fit', type_model='vgg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 32 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 32 classes\n",
    "class_array_32 = pkl.load(open( f'./data/class_array_32.pkl', 'rb'))\n",
    "parmas_32_classes = {\n",
    "  'num_classes':  len(class_array_32),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_32 = AnimeClassifier(**parmas_32_classes)\n",
    "model_32.build(input_shape=(None, *parmas_32_classes['input_shape']))\n",
    "PATH_BEST_32_CLASSES = './models/vgg_32class_1024_units_aqr.h5'\n",
    "model_32.load_weights(PATH_BEST_32_CLASSES)\n",
    "\n",
    "TFRECORD_PATH_32 = './data/anime_data_32.tfrecord'\n",
    "tf_record_32 = load_tfrecord_dataset(TFRECORD_PATH_32, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_32)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_32 = tf_record_32.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_32 = tf_record_32.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_32 = tf_record_32.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_32 = []\n",
    "all_labels_32 = []\n",
    "for images, label in tf_record_32:\n",
    "  preds = model_32.predict(images)\n",
    "  all_labels_32.extend(label)\n",
    "  all_preds_32.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix= tf.math.confusion_matrix(\n",
    "  all_labels_32,\n",
    "  all_preds_32,\n",
    "  num_classes=parmas_32_classes['num_classes']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Confusion matrix - 32 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 16 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 16 classes\n",
    "class_array_16 = pkl.load(open( f'./data/class_array_16.pkl', 'rb'))\n",
    "parmas_16_classes = {\n",
    "  'num_classes': len(class_array_16),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_16 = AnimeClassifier(**parmas_16_classes)\n",
    "model_16.build(input_shape=(None, *parmas_16_classes['input_shape']))\n",
    "#PATH_BEST_16_CLASSES = './models/vgg_16class_1024_units_aqr.h5'\n",
    "PATH_BEST_16_CLASSES = './models/vgg_16class1024_units_aqr_2.h5'\n",
    "model_16.load_weights(PATH_BEST_16_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORD_PATH_16 = './data/anime_data_16.tfrecord'\n",
    "tf_record_16 = load_tfrecord_dataset(TFRECORD_PATH_16, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_16)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_16 = tf_record_16.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_16 = tf_record_16.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_16 = tf_record_16.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_16 = []\n",
    "all_labels_16 = []\n",
    "for images, label in tf_record_16:\n",
    "  preds = model_16.predict(images)\n",
    "  all_labels_16.extend(label)\n",
    "  all_preds_16.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "  all_labels_16,\n",
    "  all_preds_16,\n",
    "  num_classes=parmas_16_classes['num_classes']\n",
    ")\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title('Confusion matrix - 16 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_16,\n",
    "  yticklabels=class_array_16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trained with 8 classes\n",
    "class_array_8 = pkl.load(open( f'./data/class_array_8.pkl', 'rb'))\n",
    "parmas_8_classes = {\n",
    "  'num_classes': len(class_array_8),\n",
    "  'input_shape': (SIZE_IMG, SIZE_IMG, 3),\n",
    "  'type_extractor': 'vgg',\n",
    "  'units': 1024,\n",
    "  'inner_layers': 1\n",
    "}\n",
    "model_8 = AnimeClassifier(**parmas_8_classes)\n",
    "model_8.build(input_shape=(None, *parmas_8_classes['input_shape']))\n",
    "PATH_BEST_8_CLASSES = './models/vgg_8class_1024_units.h5'\n",
    "model_8.load_weights(PATH_BEST_8_CLASSES)\n",
    "\n",
    "TFRECORD_PATH_8 = './data/anime_data_8.tfrecord'\n",
    "tf_record_8 = load_tfrecord_dataset(TFRECORD_PATH_8, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_8)\n",
    "\n",
    "n_train = int(all_ds_len * 0.8)\n",
    "n_test = int(all_ds_len * 0.2)\n",
    "\n",
    "tf_record_8 = tf_record_8.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_8 = tf_record_8.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')\n",
    "tf_record_8 = tf_record_8.batch(32)\n",
    "\n",
    "#Evaluate model and create confusion matrix\n",
    "all_preds_8 = []\n",
    "all_labels_8 = []\n",
    "for images, label in tf_record_8:\n",
    "  preds = model_8.predict(images)\n",
    "  all_labels_8.extend(label)\n",
    "  all_preds_8.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(\n",
    "  all_labels_8,\n",
    "  all_preds_8,\n",
    "  num_classes=parmas_8_classes['num_classes']\n",
    ")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('Confusion matrix - 8 classes')\n",
    "sns.heatmap(\n",
    "  confusion_matrix.numpy(),\n",
    "  annot=True,\n",
    "  cmap='Blues',\n",
    "  xticklabels=class_array_8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search vectors similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_cpu(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def cosine_similarity_cpum(u, v):\n",
    "  u_dot_v = np.sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = np.sqrt(np.sum(u*u))\n",
    "  mod_v = np.sqrt(np.sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tf(a, b):\n",
    "  return tf.tensordot(a, b, axes=1) / (tf.norm(a) * tf.norm(b))\n",
    "\n",
    "@tf.function\n",
    "def cosine_similarity_tfm(u, v):\n",
    "  u_dot_v = tf.reduce_sum(u*v,axis = 1)\n",
    "\n",
    "  mod_u = tf.sqrt(tf.reduce_sum(u*u))\n",
    "  mod_v = tf.sqrt(tf.reduce_sum(v*v,axis = 1))\n",
    "  return 1 - u_dot_v/(mod_u*mod_v)\n",
    "\n",
    "\n",
    "@numba.guvectorize([\"void(float64[:], float64[:], float64[:])\"], \"(n),(n)->()\", target='parallel', fastmath =True)\n",
    "def fast_cosine_gufunc(u, v, result):\n",
    "    m = u.shape[0]\n",
    "    udotv = 0\n",
    "    u_norm = 0\n",
    "    v_norm = 0\n",
    "    for i in range(m):\n",
    "        if (np.isnan(u[i])) or (np.isnan(v[i])):\n",
    "            continue\n",
    "\n",
    "        udotv += u[i] * v[i]\n",
    "        u_norm += u[i] * u[i]\n",
    "        v_norm += v[i] * v[i]\n",
    "\n",
    "    u_norm = np.sqrt(u_norm)\n",
    "    v_norm = np.sqrt(v_norm)\n",
    "\n",
    "    if (u_norm == 0) or (v_norm == 0):\n",
    "        ratio = 1.0\n",
    "    else:\n",
    "        ratio = udotv / (u_norm * v_norm)\n",
    "    result[:] = ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "results = model_16.predict(images)\n",
    "iterations = 1000\n",
    "\n",
    "cpu_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  cpu_r.append(cosine_similarity_cpum(tg_vector, results))\n",
    "  #for reuslt in results:\n",
    "  #  cpu_r.append(cosine_similarity_cpu(reuslt, reuslt))\n",
    "end = time.time()\n",
    "print(f'Time to compute on CPU: {end - start}')\n",
    "\n",
    "numba_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = results[0]\n",
    "  numba_r.append(fast_cosine_gufunc(results, tg_vector))\n",
    "end = time.time()\n",
    "print(f'Time to compute on Numba: {end - start}')\n",
    "\n",
    "images_gpu = [tf.convert_to_tensor(result) for result in results]\n",
    "tf_r = []\n",
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "  tg_vector = images_gpu[0]\n",
    "  tf_r.append(cosine_similarity_tfm(tg_vector, results))\n",
    "  #for reuslt in images_gpu:\n",
    "  #  tf_r.append(cosine_similarity_tf(result, images_gpu[0]))\n",
    "end = time.time()\n",
    "print(f'Time to compute on TF: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build and get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_vec(tfrecord, size):\n",
    "  x = tf.io.parse_single_example(tfrecord, {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'class_name': tf.io.FixedLenFeature([], tf.string),\n",
    "  })\n",
    "  x_train = tf.image.decode_jpeg(x['image'], channels=3)\n",
    "  x_train = tf.image.resize(x_train, (size, size))\n",
    "  x_train = preprocess_input(x_train, mode='tf')\n",
    "\n",
    "  y_train = x['class_name']\n",
    "  if y_train is None:\n",
    "    y_train = ''\n",
    "\n",
    "  return x_train, y_train\n",
    "\n",
    "def load_tfrecord_dataset_vec(file_pattern, size):\n",
    "  files = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "  return dataset.map(lambda x: parse_tfrecord_vec(x, size))\n",
    "\n",
    "\n",
    "TFRECORD_PATH_VEC = './data/anime_data_32.tfrecord'\n",
    "tf_record_vec = load_tfrecord_dataset_vec(TFRECORD_PATH_VEC, SIZE_IMG)\n",
    "\n",
    "all_ds_len = sum(1 for _ in tf_record_vec)\n",
    "\n",
    "n_train = int(all_ds_len * 0.95)\n",
    "n_test = int(all_ds_len * 0.05)\n",
    "\n",
    "tf_record_vec = tf_record_vec.shuffle(n_train + n_test, seed=SEED)\n",
    "tf_record_vec = tf_record_vec.skip(n_train).take(n_test)\n",
    "print(f'Total number of images or test: {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = list(itertools.combinations(tf_record_vec, 2))\n",
    "def parse_record_vec(combination):\n",
    "  item_1, item_2 = combination\n",
    "  img_1, label_1 = item_1\n",
    "  img_2, label_2 = item_2\n",
    "  return (img_1, img_2, label_1 == label_2)\n",
    "\n",
    "all_combinations = list(map(parse_record_vec, all_combinations))\n",
    "rd.shuffle(all_combinations)\n",
    "del tf_record_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(combinations, model):\n",
    "  result = []\n",
    "  for idx, item in enumerate(combinations):\n",
    "    img_1, img_2, label = item\n",
    "    pred_1, pred_2 = model.vectorize(np.array([img_1, img_2]))\n",
    "    cos_sim = cosine_similarity_cpu(pred_1, pred_2)\n",
    "    result.append((cos_sim, label))\n",
    "    #print(f'{idx + 1}/{len(combinations)} - {round(((idx + 1) / len(combinations)) * 100, 2)}%')\n",
    "  return result\n",
    "\n",
    "MAX_VEC_LEN = 2048\n",
    "result = calculate_cosine_similarity(all_combinations[:MAX_VEC_LEN], model_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result)\n",
    "df.columns = ['cos_sim', 'label']\n",
    "df.label = df.label.astype(bool)\n",
    "\n",
    "thresholds = [0.65, 0.75, 0.85]\n",
    "\n",
    "for threshold in thresholds:\n",
    "  pred_label = df.apply(lambda x: True if x.cos_sim > threshold else False, axis=1)\n",
    "\n",
    "  # Impact when the model predict to lot False positives\n",
    "  precision_result = sk_metrics.precision_score(df.label, pred_label)\n",
    "  # Impact when the model predict to lot False negatives\n",
    "  recall_result = sk_metrics.recall_score(df.label, pred_label)\n",
    "  # Impact when the model predict to lot False positives and False negatives\n",
    "  f1_result = sk_metrics.f1_score(df.label, pred_label)\n",
    "  \n",
    "  print(f'\\nThreshold: {threshold}')\n",
    "  print(f'Precision score: {precision_result}')\n",
    "  print(f'Recall score: {recall_result}')\n",
    "  print(f'F1 score: {f1_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "\n",
    "| **Class** | **Threshold** | **Precision** | **Recall** | **F1** |\n",
    "|:---------:|---------------|---------------|:----------:|--------|\n",
    "|         8 |          0.65 |         0.277 |      0.804 |  0.407 |\n",
    "|         8 |          0.75 |         0.487 |      0.574 |  0.522 |\n",
    "|         8 |          0.85 |         0.710 |      0.184 |  0.303 |\n",
    "|        16 |          0.65 |         0.272 |      0.798 |  0.403 |\n",
    "|        16 |          0.75 |         0.643 |      0.623 |  0.634 |\n",
    "|        16 |          0.85 |         0.829 |      0.231 |  0.384 |\n",
    "|        32 |          0.65 |         0.071 |      0.707 |  0.129 |\n",
    "|        32 |          0.75 |         0.137 |      0.523 |  0.217 |\n",
    "|        32 |          0.85 |         0.435 |      0.261 |  0.327 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeClassifier(\n",
    "  num_classes=len(class_array),\n",
    "  input_shape=(SIZE_IMG, SIZE_IMG, 3),\n",
    "  type_extractor='vgg',\n",
    "  units=UNITS,\n",
    "  inner_layers=1\n",
    ")\n",
    "model.build(input_shape=(None, SIZE_IMG, SIZE_IMG, 3))\n",
    "PATH_BEST = './models/vgg_16class_1024_units_aqr.h5'\n",
    "model.load_weights(PATH_BEST)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vector and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = load_img(\"./images/db1.jpg\", target_size=(224, 224))\n",
    "db2 = load_img(\"./images/db2.jpeg\", target_size=(224, 224))\n",
    "nr1 = load_img(\"./images/nr1.webp\", target_size=(224, 224))\n",
    "db1 = np.array(db1)\n",
    "db2 = np.array(db2)\n",
    "nr1 = np.array(nr1)\n",
    "\n",
    "images = preprocess_input(np.array([db1, db2, nr1]), mode='tf')\n",
    "\n",
    "db1_v, db2_v, nr1_v = model.vectorize(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(db1_v, nr1_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(db1_v, db2_v)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "852bc408046ca7dfc5c8f91ce764d8630d2287ca09c7fe9d1b4d9cd156705bcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
